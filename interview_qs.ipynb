{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am from Parent\n",
      "I am from Child\n"
     ]
    }
   ],
   "source": [
    "class Parent:\n",
    "    def __init__(self):\n",
    "        self.parent_var = \"I am from Parent\"\n",
    "\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Calls Parent's __init__()\n",
    "        self.child_var = \"I am from Child\"\n",
    "\n",
    "\n",
    "child = Child()\n",
    "print(child.parent_var)  # Output: I am from Parent\n",
    "print(child.child_var)  # Output: I am from Child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, **child classes inherit instance variables initialized in the `__init__()` method or assigned to `self` in the parent class**. However, the behavior depends on whether the child class calls the parent's `__init__()` method explicitly. Let's break this down:\n",
    "\n",
    "### 1. **Variables in `__init__()` of Parent Class**\n",
    "If a variable is initialized in the parent class's `__init__()` method, it belongs to the parent class's instance. For the child class to access those variables, the parent class's `__init__()` must be explicitly invoked, either directly or through `super()`.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "class Parent:\n",
    "    def __init__(self):\n",
    "        self.parent_var = \"I am from Parent\"\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Calls Parent's __init__()\n",
    "        self.child_var = \"I am from Child\"\n",
    "\n",
    "child = Child()\n",
    "print(child.parent_var)  # Output: I am from Parent\n",
    "print(child.child_var)   # Output: I am from Child\n",
    "```\n",
    "\n",
    "If the parent's `__init__()` is not called, the `parent_var` will not exist in the child instance.\n",
    "\n",
    "#### Example without `super()`:\n",
    "```python\n",
    "class Parent:\n",
    "    def __init__(self):\n",
    "        self.parent_var = \"I am from Parent\"\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self):\n",
    "        self.child_var = \"I am from Child\"\n",
    "\n",
    "child = Child()\n",
    "print(child.child_var)  # Output: I am from Child\n",
    "# print(child.parent_var)  # AttributeError: 'Child' object has no attribute 'parent_var'\n",
    "```\n",
    "\n",
    "### 2. **Variables Assigned to `self` Outside `__init__()`**\n",
    "Variables assigned to `self` in methods other than `__init__()` are still instance variables. If a child class calls such a method from the parent class, the variable will be accessible in the child instance.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "class Parent:\n",
    "    def set_var(self):\n",
    "        self.dynamic_var = \"Set outside __init__\"\n",
    "\n",
    "class Child(Parent):\n",
    "    pass\n",
    "\n",
    "child = Child()\n",
    "child.set_var()  # Calls the parent method\n",
    "print(child.dynamic_var)  # Output: Set outside __init__\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "1. **`super()` or explicit call**: To inherit variables from the parent's `__init__()`, the child class must call the parent's `__init__()` explicitly (e.g., `super().__init__()`).\n",
    "2. **Inheritance of methods**: If the child calls methods from the parent that assign instance variables, those variables will be inherited as part of the instance.\n",
    "3. **Separate `__init__()` in child**: If the child defines its own `__init__()` without calling the parent's `__init__()`, it must handle initializing variables from the parent class manually if needed.\n",
    "\n",
    "Let me know if you need further clarification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is data abstraction accomplished?\n",
    "Data abstraction is accomplished with the help of abstract methods or abstract classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data abstraction in object-oriented programming is the process of hiding implementation details and exposing only the essential features or functionalities of an object. This allows users to interact with the object without needing to understand the underlying complexity.\n",
    "\n",
    "In Python, **data abstraction** is achieved using **abstract methods** and **abstract classes**. Here's how they work:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Abstract Classes**\n",
    "An **abstract class** is a class that cannot be instantiated directly. It serves as a blueprint for other classes. Abstract classes are created using the `abc` module (`abc` stands for Abstract Base Classes).\n",
    "\n",
    "#### Key Points:\n",
    "- An abstract class can have both concrete (implemented) methods and abstract methods.\n",
    "- It provides a template for derived (child) classes to follow.\n",
    "- Abstract classes ensure that derived classes implement specific methods.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Animal(ABC):  # Abstract class\n",
    "    @abstractmethod\n",
    "    def make_sound(self):\n",
    "        pass  # Abstract method (must be implemented in child classes)\n",
    "\n",
    "    def sleep(self):\n",
    "        print(\"Sleeping...\")  # Concrete method (optional to override)\n",
    "\n",
    "class Dog(Animal):  # Child class\n",
    "    def make_sound(self):\n",
    "        print(\"Woof!\")\n",
    "\n",
    "class Cat(Animal):  # Another child class\n",
    "    def make_sound(self):\n",
    "        print(\"Meow!\")\n",
    "\n",
    "# Abstract class cannot be instantiated\n",
    "# animal = Animal()  # TypeError: Can't instantiate abstract class\n",
    "\n",
    "dog = Dog()\n",
    "dog.make_sound()  # Output: Woof!\n",
    "dog.sleep()       # Output: Sleeping...\n",
    "\n",
    "cat = Cat()\n",
    "cat.make_sound()  # Output: Meow!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Abstract Methods**\n",
    "An **abstract method** is a method that is declared but not implemented in the abstract class. It is marked with the `@abstractmethod` decorator from the `abc` module.\n",
    "\n",
    "#### Key Points:\n",
    "- Abstract methods must be overridden in any non-abstract subclass.\n",
    "- If a subclass does not implement all abstract methods, it remains abstract and cannot be instantiated.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Shape(ABC):  # Abstract class\n",
    "    @abstractmethod\n",
    "    def area(self):\n",
    "        pass  # Must be implemented by subclasses\n",
    "\n",
    "    @abstractmethod\n",
    "    def perimeter(self):\n",
    "        pass  # Must be implemented by subclasses\n",
    "\n",
    "class Rectangle(Shape):  # Concrete class\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def area(self):\n",
    "        return self.width * self.height\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * (self.width + self.height)\n",
    "\n",
    "# shape = Shape()  # TypeError: Can't instantiate abstract class\n",
    "rect = Rectangle(4, 5)\n",
    "print(rect.area())       # Output: 20\n",
    "print(rect.perimeter())  # Output: 18\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Advantages of Data Abstraction**\n",
    "- **Hides implementation details**: Users interact with high-level interfaces and don't need to know the underlying complexity.\n",
    "- **Enforces design contracts**: Abstract classes ensure that derived classes implement required methods.\n",
    "- **Encourages code reuse**: Common functionality can be defined in the abstract class, reducing duplication in subclasses.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Additional Notes**\n",
    "- Abstract classes are not mandatory for abstraction in Python. Interfaces can also be defined using normal methods or protocols (like in duck typing).\n",
    "- While Python uses `@abstractmethod` for abstraction, other languages (like Java and C++) achieve abstraction through similar constructs, such as interfaces or abstract classes.\n",
    "\n",
    "Let me know if you'd like to explore more examples or details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In C++, **data abstraction** is achieved using **abstract classes** and **pure virtual functions**. These concepts allow you to define a blueprint for derived classes while hiding implementation details from the user.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Abstract Classes in C++**\n",
    "An **abstract class** is a class that has at least one **pure virtual function**. A pure virtual function is a function that is declared in a class but is not implemented in it. Instead, it must be implemented by derived classes.\n",
    "\n",
    "#### Key Points:\n",
    "- A pure virtual function is declared using the `= 0` syntax.\n",
    "- Abstract classes cannot be instantiated directly.\n",
    "- Derived classes must provide implementations for all pure virtual functions, or they will also become abstract classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Example of Data Abstraction in C++**\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Abstract class\n",
    "class Shape {\n",
    "public:\n",
    "    // Pure virtual functions (abstract methods)\n",
    "    virtual void area() const = 0;\n",
    "    virtual void perimeter() const = 0;\n",
    "\n",
    "    // Concrete method\n",
    "    void description() const {\n",
    "        cout << \"This is a shape.\\n\";\n",
    "    }\n",
    "};\n",
    "\n",
    "// Derived class for Rectangle\n",
    "class Rectangle : public Shape {\n",
    "private:\n",
    "    double width, height;\n",
    "\n",
    "public:\n",
    "    Rectangle(double w, double h) : width(w), height(h) {}\n",
    "\n",
    "    void area() const override {\n",
    "        cout << \"Area of Rectangle: \" << width * height << endl;\n",
    "    }\n",
    "\n",
    "    void perimeter() const override {\n",
    "        cout << \"Perimeter of Rectangle: \" << 2 * (width + height) << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "// Derived class for Circle\n",
    "class Circle : public Shape {\n",
    "private:\n",
    "    double radius;\n",
    "\n",
    "public:\n",
    "    Circle(double r) : radius(r) {}\n",
    "\n",
    "    void area() const override {\n",
    "        cout << \"Area of Circle: \" << 3.14159 * radius * radius << endl;\n",
    "    }\n",
    "\n",
    "    void perimeter() const override {\n",
    "        cout << \"Perimeter of Circle: \" << 2 * 3.14159 * radius << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    // Abstract class cannot be instantiated\n",
    "    // Shape shape; // Error: Cannot instantiate abstract class\n",
    "\n",
    "    // Use pointers or references to work with derived classes polymorphically\n",
    "    Shape* rect = new Rectangle(4, 5);\n",
    "    Shape* circle = new Circle(3);\n",
    "\n",
    "    rect->description();  // Concrete method in the base class\n",
    "    rect->area();\n",
    "    rect->perimeter();\n",
    "\n",
    "    circle->description();  // Concrete method in the base class\n",
    "    circle->area();\n",
    "    circle->perimeter();\n",
    "\n",
    "    delete rect;\n",
    "    delete circle;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Output**\n",
    "```\n",
    "This is a shape.\n",
    "Area of Rectangle: 20\n",
    "Perimeter of Rectangle: 18\n",
    "This is a shape.\n",
    "Area of Circle: 28.2735\n",
    "Perimeter of Circle: 18.8496\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Key Concepts in C++ Data Abstraction**\n",
    "\n",
    "1. **Pure Virtual Functions**:\n",
    "   - Declared using `virtual return_type function_name() = 0;`.\n",
    "   - Enforces derived classes to provide their own implementation.\n",
    "\n",
    "2. **Abstract Classes**:\n",
    "   - A class with at least one pure virtual function.\n",
    "   - Cannot be instantiated directly.\n",
    "\n",
    "3. **Polymorphism**:\n",
    "   - Abstract classes and pure virtual functions enable **runtime polymorphism**, where derived class objects can be used through base class pointers or references.\n",
    "\n",
    "4. **Encapsulation**:\n",
    "   - Along with abstraction, encapsulation (using `private`, `protected`, and `public` access specifiers) ensures that internal details of the class are hidden from the outside.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Advantages of Data Abstraction in C++**\n",
    "- **Hides implementation details**: Users interact only with high-level interfaces.\n",
    "- **Promotes modularity**: Allows separate development and maintenance of base and derived classes.\n",
    "- **Ensures extensibility**: New derived classes can be added without changing the base class.\n",
    "\n",
    "---\n",
    "\n",
    "If you have further questions or need clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an abstract class?\n",
    "An abstract class is a special class containing abstract methods. The significance of abstract class is that the abstract methods inside it are not implemented and only declared. So as a result, when a subclass inherits the abstract class and needs to use its abstract methods, they need to define and implement them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. What is meant by Garbage Collection in OOPs world?\n",
    "Object-oriented programming revolves around entities like objects. Each object consumes memory and there can be multiple objects of a class. So if these objects and their memories are not handled properly, then it might lead to certain memory-related errors and the system might fail.\n",
    "\n",
    "Garbage collection refers to this mechanism of handling the memory in the program. Through garbage collection, the unwanted memory is freed up by removing the objects that are no longer needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. What is Compile time Polymorphism and how is it different from Runtime Polymorphism?\n",
    "\n",
    "\n",
    "**Compile-time polymorphism** and **runtime polymorphism** are two types of polymorphism in object-oriented programming. They differ in how and when the method to be executed is determined.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Compile-time Polymorphism**\n",
    "Also known as **static polymorphism** or **early binding**, compile-time polymorphism occurs when the method to be executed is determined at **compile time**.\n",
    "\n",
    "#### Achieved Through:\n",
    "- **Function Overloading**: Defining multiple functions with the same name but different parameter lists.\n",
    "- **Operator Overloading**: Defining custom behavior for operators.\n",
    "\n",
    "#### Key Features:\n",
    "- The compiler decides which method to invoke based on the method signature.\n",
    "- It is faster since the decision is made at compile time.\n",
    "- No runtime overhead.\n",
    "\n",
    "#### Example (C++):\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "class Calculator {\n",
    "public:\n",
    "    // Function Overloading\n",
    "    int add(int a, int b) {\n",
    "        return a + b;\n",
    "    }\n",
    "\n",
    "    double add(double a, double b) {\n",
    "        return a + b;\n",
    "    }\n",
    "\n",
    "    int add(int a, int b, int c) {\n",
    "        return a + b + c;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    Calculator calc;\n",
    "    cout << calc.add(3, 4) << endl;        // Output: 7\n",
    "    cout << calc.add(3.5, 2.5) << endl;    // Output: 6\n",
    "    cout << calc.add(1, 2, 3) << endl;     // Output: 6\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Runtime Polymorphism**\n",
    "Also known as **dynamic polymorphism** or **late binding**, runtime polymorphism occurs when the method to be executed is determined at **runtime**.\n",
    "\n",
    "#### Achieved Through:\n",
    "- **Function Overriding**: Redefining a method in a derived class that exists in the base class.\n",
    "- Requires the use of **virtual functions** in C++ to achieve dynamic binding.\n",
    "\n",
    "#### Key Features:\n",
    "- The decision about which method to invoke is made during program execution.\n",
    "- Enables polymorphism through inheritance.\n",
    "- Involves runtime overhead due to dynamic dispatch (vtable lookup).\n",
    "\n",
    "#### Example (C++):\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "class Animal {\n",
    "public:\n",
    "    virtual void sound() {  // Virtual function\n",
    "        cout << \"Animal makes a sound\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Dog : public Animal {\n",
    "public:\n",
    "    void sound() override {  // Overriding the base class method\n",
    "        cout << \"Dog barks\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Cat : public Animal {\n",
    "public:\n",
    "    void sound() override {\n",
    "        cout << \"Cat meows\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    Animal* animal;\n",
    "    Dog dog;\n",
    "    Cat cat;\n",
    "\n",
    "    animal = &dog;\n",
    "    animal->sound();  // Output: Dog barks\n",
    "\n",
    "    animal = &cat;\n",
    "    animal->sound();  // Output: Cat meows\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Key Differences Between Compile-time and Runtime Polymorphism**\n",
    "\n",
    "| Feature                     | Compile-time Polymorphism               | Runtime Polymorphism                  |\n",
    "|-----------------------------|------------------------------------------|---------------------------------------|\n",
    "| **Binding Time**            | Happens at compile time (early binding).| Happens at runtime (late binding).   |\n",
    "| **Implementation**          | Achieved via function/operator overloading.| Achieved via function overriding.    |\n",
    "| **Speed**                   | Faster, as it is resolved at compile time.| Slower, due to runtime method resolution.|\n",
    "| **Flexibility**             | Less flexible, determined at compile time.| More flexible, allows dynamic behavior.|\n",
    "| **Inheritance**             | Does not require inheritance.           | Requires inheritance and virtual functions.|\n",
    "| **Overhead**                | No runtime overhead.                    | Runtime overhead due to dynamic dispatch. |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Practical Usage**\n",
    "- **Compile-time polymorphism** is useful when the behavior can be determined based on input types or number of arguments (e.g., overloading operators, mathematical functions).\n",
    "- **Runtime polymorphism** is ideal for scenarios where the exact behavior depends on the specific type of the object at runtime (e.g., designing extensible systems with a base class and derived classes).\n",
    "\n",
    "Let me know if you need more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encapsulation** is one of the core principles of object-oriented programming (OOP). It refers to the bundling of **data (variables)** and **methods (functions)** that operate on that data into a single unit, typically a class, while restricting direct access to some components. This helps protect the integrity of the data and ensures that it is accessed and modified only in controlled ways.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features of Encapsulation**\n",
    "\n",
    "1. **Data Hiding**:\n",
    "   - By restricting access to certain parts of an object, encapsulation hides the internal implementation details from the outside world.\n",
    "   - Access modifiers (`private`, `protected`, `public` in C++/Java) are used to control visibility.\n",
    "\n",
    "2. **Controlled Access**:\n",
    "   - Access to the data is provided through **getter** and **setter** methods, allowing controlled read/write operations.\n",
    "\n",
    "3. **Improved Security**:\n",
    "   - Sensitive data is protected from accidental or malicious modification.\n",
    "\n",
    "4. **Modularity**:\n",
    "   - Encapsulation promotes modularity, as changes to the internal implementation do not affect the external interface.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example in C++**\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <string>\n",
    "using namespace std;\n",
    "\n",
    "class BankAccount {\n",
    "private:\n",
    "    // Private data members\n",
    "    string accountHolder;\n",
    "    double balance;\n",
    "\n",
    "public:\n",
    "    // Constructor\n",
    "    BankAccount(string name, double initialBalance) {\n",
    "        accountHolder = name;\n",
    "        balance = initialBalance > 0 ? initialBalance : 0;  // Ensure non-negative balance\n",
    "    }\n",
    "\n",
    "    // Getter for account holder\n",
    "    string getAccountHolder() const {\n",
    "        return accountHolder;\n",
    "    }\n",
    "\n",
    "    // Getter for balance\n",
    "    double getBalance() const {\n",
    "        return balance;\n",
    "    }\n",
    "\n",
    "    // Method to deposit money\n",
    "    void deposit(double amount) {\n",
    "        if (amount > 0) {\n",
    "            balance += amount;\n",
    "            cout << \"Deposited: $\" << amount << endl;\n",
    "        } else {\n",
    "            cout << \"Invalid deposit amount!\" << endl;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Method to withdraw money\n",
    "    void withdraw(double amount) {\n",
    "        if (amount > 0 && amount <= balance) {\n",
    "            balance -= amount;\n",
    "            cout << \"Withdrew: $\" << amount << endl;\n",
    "        } else {\n",
    "            cout << \"Invalid withdrawal amount!\" << endl;\n",
    "        }\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    BankAccount account(\"Zohaib\", 1000);\n",
    "\n",
    "    // Accessing data through methods\n",
    "    cout << \"Account Holder: \" << account.getAccountHolder() << endl;\n",
    "    cout << \"Balance: $\" << account.getBalance() << endl;\n",
    "\n",
    "    // Depositing and withdrawing\n",
    "    account.deposit(500);\n",
    "    account.withdraw(300);\n",
    "\n",
    "    // Direct access to private members is not allowed\n",
    "    // account.balance = 2000; // Error: 'balance' is private\n",
    "\n",
    "    cout << \"Final Balance: $\" << account.getBalance() << endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "\n",
    "```\n",
    "Account Holder: Zohaib\n",
    "Balance: $1000\n",
    "Deposited: $500\n",
    "Withdrew: $300\n",
    "Final Balance: $1200\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Encapsulation**\n",
    "\n",
    "1. **Improved Code Security**:\n",
    "   - Prevents unauthorized access to critical data.\n",
    "   \n",
    "2. **Code Maintainability**:\n",
    "   - Changes in implementation details can be made without affecting external code that uses the class.\n",
    "\n",
    "3. **Reusability**:\n",
    "   - Encapsulation makes it easier to reuse and modify the code.\n",
    "\n",
    "4. **Flexibility**:\n",
    "   - Encapsulation allows controlled access to the data through well-defined interfaces (getters and setters).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts Related to Encapsulation**\n",
    "- **Access Modifiers**:\n",
    "  - **Private**: Accessible only within the class.\n",
    "  - **Protected**: Accessible within the class and derived classes.\n",
    "  - **Public**: Accessible from anywhere.\n",
    "\n",
    "- **Getter and Setter Methods**:\n",
    "  - Provide controlled access to private members.\n",
    "\n",
    "---\n",
    "\n",
    "If you have further questions or need clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Polymorphism: A Technical Explanation**\n",
    "\n",
    "Polymorphism is a fundamental concept in object-oriented programming (OOP) that allows entities such as functions, methods, or objects to take on multiple forms. It provides a unified interface to different underlying types, enabling flexibility and extensibility in software design.\n",
    "\n",
    "The term *polymorphism* comes from the Greek words *poly* (many) and *morph* (form), and in software development, it refers to the ability of a single interface to represent different data types or perform different tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Polymorphism**\n",
    "\n",
    "Polymorphism is typically categorized into two types based on when the method to be executed is determined:\n",
    "\n",
    "#### **1. Compile-time Polymorphism (Static Binding/Early Binding)**\n",
    "- **Definition**: The method to be executed is determined at compile time. This is achieved through method overloading or operator overloading.\n",
    "- **Implementation**: Function or operator overloading allows multiple functions/operators with the same name but different signatures (parameters or types).\n",
    "- **Example**: Overloading a `print()` function to handle integers, strings, or floats.\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "class Printer {\n",
    "public:\n",
    "    void print(int i) {\n",
    "        cout << \"Printing integer: \" << i << endl;\n",
    "    }\n",
    "\n",
    "    void print(double d) {\n",
    "        cout << \"Printing double: \" << d << endl;\n",
    "    }\n",
    "\n",
    "    void print(string s) {\n",
    "        cout << \"Printing string: \" << s << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    Printer p;\n",
    "    p.print(42);        // Calls print(int)\n",
    "    p.print(3.14);      // Calls print(double)\n",
    "    p.print(\"Hello\");   // Calls print(string)\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Runtime Polymorphism (Dynamic Binding/Late Binding)**\n",
    "- **Definition**: The method to be executed is determined at runtime. This is typically achieved through method overriding using inheritance and virtual functions.\n",
    "- **Implementation**: A base class defines a method as `virtual`, allowing derived classes to override its implementation. A pointer or reference to the base class is used to call the method, and the actual method invoked depends on the type of the object at runtime.\n",
    "- **Example**: Overriding a `speak()` method in a base class `Animal` with derived classes `Dog` and `Cat`.\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "class Animal {\n",
    "public:\n",
    "    virtual void speak() {  // Virtual function\n",
    "        cout << \"Animal speaks\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Dog : public Animal {\n",
    "public:\n",
    "    void speak() override {  // Override base class method\n",
    "        cout << \"Dog barks\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Cat : public Animal {\n",
    "public:\n",
    "    void speak() override {\n",
    "        cout << \"Cat meows\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    Animal* animal;\n",
    "\n",
    "    Dog dog;\n",
    "    Cat cat;\n",
    "\n",
    "    animal = &dog;\n",
    "    animal->speak();  // Output: Dog barks (runtime decision)\n",
    "\n",
    "    animal = &cat;\n",
    "    animal->speak();  // Output: Cat meows (runtime decision)\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Polymorphism**\n",
    "\n",
    "1. **Extensibility**:\n",
    "   - New classes can be introduced without modifying existing code, as long as they conform to the interface.\n",
    "\n",
    "2. **Code Reusability**:\n",
    "   - The same interface can handle objects of different types, reducing duplication.\n",
    "\n",
    "3. **Improved Maintainability**:\n",
    "   - Changes to a base class method automatically propagate to derived classes.\n",
    "\n",
    "4. **Dynamic Behavior**:\n",
    "   - The ability to decide at runtime which method to invoke allows for flexible and adaptive code.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Cases**\n",
    "\n",
    "1. **Frameworks and Libraries**:\n",
    "   - APIs often use polymorphism to allow developers to extend functionality without altering the core library.\n",
    "\n",
    "2. **UI Design**:\n",
    "   - A base `Widget` class can define methods like `draw()` and `resize()`, which are overridden by derived classes like `Button`, `TextBox`, and `Slider`.\n",
    "\n",
    "3. **Game Development**:\n",
    "   - A base `GameObject` class with methods like `update()` and `render()` can be overridden by derived classes like `Player`, `Enemy`, and `Projectile`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: Compile-time vs. Runtime Polymorphism**\n",
    "\n",
    "| Feature                     | Compile-time Polymorphism               | Runtime Polymorphism                  |\n",
    "|-----------------------------|------------------------------------------|---------------------------------------|\n",
    "| **Binding Time**            | At compile time (early binding).         | At runtime (late binding).            |\n",
    "| **Mechanism**               | Function/Operator overloading.           | Function overriding with virtual functions. |\n",
    "| **Performance**             | Faster, no runtime overhead.             | Slower due to dynamic dispatch.       |\n",
    "| **Inheritance Required**    | No.                                      | Yes.                                  |\n",
    "| **Flexibility**             | Less flexible, determined at compile time.| More flexible, determined at runtime. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Polymorphism in Design Patterns**\n",
    "Polymorphism is a key enabler for many design patterns:\n",
    "- **Strategy Pattern**: Allows a family of algorithms to be defined and interchanged at runtime.\n",
    "- **Factory Pattern**: Uses polymorphism to return objects of different derived classes.\n",
    "- **Decorator Pattern**: Dynamically adds behavior to objects.\n",
    "\n",
    "---\n",
    "\n",
    "Polymorphism, when used correctly, simplifies complex systems and makes them more adaptable to future changes. However, overuse or misuse can lead to unnecessary complexity and performance overhead, so it’s essential to apply it judiciously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that's a great way to put it! Polymorphism in Object-Oriented Programming (OOP) is about **\"many forms\"** — it allows a single piece of code, method, or object to behave differently depending on the context in which it's used. This is achieved through two primary types of polymorphism:\n",
    "\n",
    "### **1. Compile-time Polymorphism (Static Polymorphism)**\n",
    "\n",
    "- **Definition**: The method to be invoked is determined at **compile time**.\n",
    "- **How It Works**: This type of polymorphism is achieved through **method overloading** (same method name but different parameters) or **operator overloading** (defining how operators like `+`, `-`, etc., work for custom types).\n",
    "- **Examples**: \n",
    "  - **Method Overloading**: Multiple methods with the same name but different parameter types or counts.\n",
    "  - **Operator Overloading**: Overloading operators to define their behavior for user-defined types (e.g., `+` for custom objects).\n",
    "\n",
    "**Example in C++ (Method Overloading)**:\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "class Printer {\n",
    "public:\n",
    "    void print(int i) {\n",
    "        cout << \"Printing integer: \" << i << endl;\n",
    "    }\n",
    "\n",
    "    void print(double d) {\n",
    "        cout << \"Printing double: \" << d << endl;\n",
    "    }\n",
    "\n",
    "    void print(string s) {\n",
    "        cout << \"Printing string: \" << s << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    Printer p;\n",
    "    p.print(42);        // Calls print(int)\n",
    "    p.print(3.14);      // Calls print(double)\n",
    "    p.print(\"Hello\");   // Calls print(string)\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Printing integer: 42\n",
    "Printing double: 3.14\n",
    "Printing string: Hello\n",
    "```\n",
    "\n",
    "### **2. Runtime Polymorphism (Dynamic Polymorphism)**\n",
    "\n",
    "- **Definition**: The method to be invoked is determined at **runtime**, i.e., when the program is running, not when it is compiled.\n",
    "- **How It Works**: This type of polymorphism is achieved through **method overriding**. A method in a base class is declared as `virtual`, and it is overridden in derived classes. The actual method called is determined by the type of the object, not the type of the reference or pointer.\n",
    "- **Examples**:\n",
    "  - **Method Overriding**: A derived class provides a specific implementation of a method that was declared in a base class.\n",
    "  \n",
    "**Example in C++ (Method Overriding)**:\n",
    "```cpp\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "class Animal {\n",
    "public:\n",
    "    virtual void speak() {  // Virtual function for runtime polymorphism\n",
    "        cout << \"Animal speaks\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Dog : public Animal {\n",
    "public:\n",
    "    void speak() override {  // Override base class method\n",
    "        cout << \"Dog barks\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Cat : public Animal {\n",
    "public:\n",
    "    void speak() override {\n",
    "        cout << \"Cat meows\" << endl;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    Animal* animal;\n",
    "\n",
    "    Dog dog;\n",
    "    Cat cat;\n",
    "\n",
    "    animal = &dog;\n",
    "    animal->speak();  // Output: Dog barks (runtime decision)\n",
    "\n",
    "    animal = &cat;\n",
    "    animal->speak();  // Output: Cat meows (runtime decision)\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Dog barks\n",
    "Cat meows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences between Compile-time and Runtime Polymorphism**\n",
    "\n",
    "| Feature                       | **Compile-time Polymorphism**                 | **Runtime Polymorphism**                |\n",
    "|-------------------------------|-----------------------------------------------|-----------------------------------------|\n",
    "| **Binding Time**               | The method is bound at compile time (early binding). | The method is bound at runtime (late binding). |\n",
    "| **Mechanism**                  | Method Overloading, Operator Overloading.      | Method Overriding with Virtual Functions. |\n",
    "| **Performance**                | Faster execution (no overhead).               | Slower execution (due to dynamic dispatch). |\n",
    "| **Inheritance**                | Not required.                                 | Requires inheritance and virtual functions. |\n",
    "| **Flexibility**                | Less flexible, method signatures must be known at compile time. | More flexible, allows dynamic method resolution. |\n",
    "| **Example**                    | Function/Operator overloading.                | Virtual function overriding in derived classes. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Polymorphism Matters**\n",
    "\n",
    "- **Code Reusability**: By using polymorphism, you can design more general, reusable code. The same code can handle different types of objects or data, reducing redundancy.\n",
    "- **Extensibility**: Polymorphism makes it easier to extend and modify the behavior of programs. New derived classes can be added without changing existing code, promoting the **open/closed principle** (open for extension, closed for modification).\n",
    "- **Maintainability**: It helps in maintaining the code since behavior can be updated in derived classes without affecting the base class or other parts of the code.\n",
    "\n",
    "---\n",
    "\n",
    "### **In Summary:**\n",
    "Polymorphism in OOP enables objects or methods to take on multiple forms, making your code more flexible and extensible. It’s classified into:\n",
    "- **Compile-time Polymorphism**: The decision of which method to call is made during compilation (e.g., method overloading).\n",
    "- **Runtime Polymorphism**: The decision of which method to call is made at runtime (e.g., method overriding with virtual functions).\n",
    "\n",
    "If you need further clarification or examples, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much memory does a class occupy?\n",
    "Classes do not consume any memory. They are just a blueprint based on which objects are created. Now when objects are created, they actually initialize the class members and methods and therefore consume memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In C++, **classes** and **structures** are very similar in terms of their functionality, but they have some key differences primarily related to default access control and their intended use. Here's a detailed comparison:\n",
    "\n",
    "### **Key Differences Between a Class and a Structure in C++**\n",
    "\n",
    "| Feature                         | **Class**                                    | **Structure**                              |\n",
    "|----------------------------------|----------------------------------------------|--------------------------------------------|\n",
    "| **Default Access Modifier**     | `private` by default                         | `public` by default                        |\n",
    "| **Intended Use**                 | Used for defining complex data types, often with encapsulation and behavior (methods). | Typically used for grouping data together, with less emphasis on encapsulation and behavior. |\n",
    "| **Access Control**               | Can have private, protected, and public members. | Can have public members, but access control can be explicitly specified (e.g., `private`, `protected`). |\n",
    "| **Member Functions**             | Can have member functions, constructors, destructors, and accessors/mutators. | Can also have member functions, but historically used more for simple data grouping. |\n",
    "| **Inheritance**                  | Supports inheritance, polymorphism, and other OOP features. | Supports inheritance (from C++ onward), but generally used for simpler data types. |\n",
    "| **Memory Layout**                | Same memory layout as `struct`, no difference in terms of memory storage. | Same memory layout as `class`.             |\n",
    "\n",
    "### **1. Default Access Control:**\n",
    "- **Class**: By default, all members (variables and functions) are `private`, meaning they cannot be accessed directly from outside the class.\n",
    "  ```cpp\n",
    "  class MyClass {\n",
    "      int x;  // private by default\n",
    "  };\n",
    "  ```\n",
    "- **Structure**: By default, all members are `public`, meaning they can be accessed directly from outside the structure.\n",
    "  ```cpp\n",
    "  struct MyStruct {\n",
    "      int x;  // public by default\n",
    "  };\n",
    "  ```\n",
    "\n",
    "### **2. Default Use Cases:**\n",
    "- **Class**: Classes are typically used when you want to model more complex behavior, where encapsulation (hiding implementation details) and data manipulation are important. Classes allow for features like **constructors**, **destructors**, **private members**, and **inheritance**.\n",
    "- **Structure**: Structures were originally designed for simpler use cases, where the focus is on grouping related data without the need for complex behavior or encapsulation. Historically, structures were used for \"plain old data\" (POD) types, but with C++'s introduction of object-oriented features, structures can now also support methods, inheritance, and polymorphism.\n",
    "\n",
    "### **3. Member Functions and Access Control:**\n",
    "- **Class**: In classes, you can define methods, and the access to these methods can be controlled using `private`, `protected`, or `public` access specifiers.\n",
    "  ```cpp\n",
    "  class MyClass {\n",
    "  private:\n",
    "      int x;  // private\n",
    "  public:\n",
    "      void setX(int val) { x = val; }  // public method\n",
    "      int getX() { return x; }  // public method\n",
    "  };\n",
    "  ```\n",
    "- **Structure**: In structures, you can also define methods, but traditionally, they were not associated with methods (though this is no longer a restriction in modern C++).\n",
    "  ```cpp\n",
    "  struct MyStruct {\n",
    "      int x;  // public by default\n",
    "      void setX(int val) { x = val; }  // public method\n",
    "      int getX() { return x; }  // public method\n",
    "  };\n",
    "  ```\n",
    "\n",
    "### **4. Inheritance and Polymorphism:**\n",
    "- **Class**: Classes fully support inheritance and polymorphism. You can derive new classes from a base class, and you can override methods, use virtual functions, etc.\n",
    "  ```cpp\n",
    "  class Animal {\n",
    "  public:\n",
    "      virtual void speak() { cout << \"Animal speaks\"; }\n",
    "  };\n",
    "\n",
    "  class Dog : public Animal {\n",
    "  public:\n",
    "      void speak() override { cout << \"Dog barks\"; }\n",
    "  };\n",
    "  ```\n",
    "- **Structure**: In C++ (since C++11), structures also support inheritance and polymorphism, just like classes.\n",
    "  ```cpp\n",
    "  struct Animal {\n",
    "      virtual void speak() { cout << \"Animal speaks\"; }\n",
    "  };\n",
    "\n",
    "  struct Dog : public Animal {\n",
    "      void speak() override { cout << \"Dog barks\"; }\n",
    "  };\n",
    "  ```\n",
    "\n",
    "### **5. Memory Layout:**\n",
    "- **Class**: Classes and structures have the same memory layout. There is no difference in how they store data in memory. Both have members stored contiguously in memory, but the access control (public/private) does not affect memory layout.\n",
    "- **Structure**: Like classes, structures are stored contiguously in memory, but historically structures were designed to be simple data containers without methods or encapsulation.\n",
    "\n",
    "### **6. Constructors and Destructors:**\n",
    "- **Class**: Classes can have constructors and destructors that allow you to initialize and clean up resources when objects are created and destroyed.\n",
    "  ```cpp\n",
    "  class MyClass {\n",
    "  public:\n",
    "      MyClass() { cout << \"Constructor called\"; }\n",
    "      ~MyClass() { cout << \"Destructor called\"; }\n",
    "  };\n",
    "  ```\n",
    "- **Structure**: Structures can also have constructors and destructors in modern C++ (since C++11). Earlier, they were used more for simple data storage without complex initialization logic.\n",
    "  ```cpp\n",
    "  struct MyStruct {\n",
    "      MyStruct() { cout << \"Constructor called\"; }\n",
    "      ~MyStruct() { cout << \"Destructor called\"; }\n",
    "  };\n",
    "  ```\n",
    "\n",
    "### **Summary:**\n",
    "- **Classes** are more versatile and are used to model objects with encapsulation, inheritance, and other OOP features. The members are `private` by default.\n",
    "- **Structures** are used primarily for grouping data, and historically had a simpler use case. The members are `public` by default.\n",
    "\n",
    "In modern C++, the difference between a class and a structure is minimal, as both can support methods, inheritance, and other object-oriented features. The main distinction remains the default access control: `private` for classes and `public` for structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why is dimensionality reduction used in ml?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a key technique in machine learning (ML) and data analysis used to reduce the number of input variables (features) in a dataset while retaining as much of the relevant information as possible. The main reasons for using dimensionality reduction are:\n",
    "\n",
    "### 1. **Reducing Computational Complexity**\n",
    "   - **Less computation**: High-dimensional data often requires a significant amount of computational resources for both training models and making predictions. Reducing the number of features helps speed up the training and inference processes.\n",
    "   - **Memory efficiency**: A reduced number of features means less memory usage, which is crucial when dealing with large datasets, especially in environments with limited resources.\n",
    "\n",
    "### 2. **Mitigating the Curse of Dimensionality**\n",
    "   - As the number of dimensions (features) increases, the volume of the feature space grows exponentially. This makes the data sparse, and models may struggle to generalize well. The \"curse of dimensionality\" refers to the issues caused by high-dimensional spaces, such as overfitting, where the model captures noise instead of the underlying pattern.\n",
    "   - Dimensionality reduction helps mitigate these issues by reducing the number of irrelevant or redundant features.\n",
    "\n",
    "### 3. **Improving Model Performance**\n",
    "   - **Avoid overfitting**: In high-dimensional spaces, models are more likely to overfit because they may learn noise as part of the pattern. By reducing the dimensionality, we reduce the risk of overfitting, leading to better generalization.\n",
    "   - **Improved interpretability**: In many cases, reducing the number of features makes the model easier to interpret. This is especially important in fields like healthcare or finance, where understanding the model’s decisions is critical.\n",
    "\n",
    "### 4. **Visualization and Exploration**\n",
    "   - **Data visualization**: Dimensionality reduction is particularly useful for visualizing high-dimensional data. By projecting the data into 2D or 3D spaces, you can plot the data and gain insights into the underlying patterns. Techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) are commonly used for this purpose.\n",
    "   - **Pattern recognition**: Reducing the number of features can help reveal hidden structures or clusters in the data, making it easier to identify important relationships between features.\n",
    "\n",
    "### 5. **Removing Redundancy and Noise**\n",
    "   - **Feature selection**: Many features in a dataset are often highly correlated or redundant. Dimensionality reduction can identify and eliminate such redundant features, retaining only the most important ones that contribute to the variance in the data.\n",
    "   - **Noise reduction**: By projecting the data into a lower-dimensional space, dimensionality reduction techniques often discard noisy, irrelevant information that does not contribute to the primary patterns in the data.\n",
    "\n",
    "### 6. **Enhancing Performance of Machine Learning Algorithms**\n",
    "   - Some machine learning algorithms perform better with lower-dimensional data because they can be more efficient and avoid the curse of dimensionality. For example, algorithms like k-nearest neighbors (KNN) and clustering algorithms like k-means can become inefficient and inaccurate in high-dimensional spaces.\n",
    "\n",
    "### 7. **Improving Data Preprocessing**\n",
    "   - **Data cleaning**: Dimensionality reduction can help in identifying irrelevant or unimportant features, which can be removed during the preprocessing stage, improving the quality of the dataset.\n",
    "   - **Noise filtering**: In datasets with noisy features, dimensionality reduction helps focus on the more significant features and reduce the influence of noisy data points.\n",
    "\n",
    "### Common Dimensionality Reduction Techniques:\n",
    "1. **Principal Component Analysis (PCA)**: A linear method that projects data onto a smaller number of orthogonal dimensions (principal components) that capture the maximum variance in the data.\n",
    "2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: A non-linear method particularly useful for visualizing high-dimensional data in 2D or 3D spaces.\n",
    "3. **Linear Discriminant Analysis (LDA)**: A technique used for dimensionality reduction that focuses on maximizing the separability between classes.\n",
    "4. **Autoencoders**: Neural network-based methods that learn a compressed representation of the data, often used in unsupervised learning for feature extraction.\n",
    "\n",
    "### Conclusion:\n",
    "Dimensionality reduction is essential in machine learning for simplifying the model, improving performance, reducing computational overhead, and enhancing data visualization. It is especially useful when dealing with high-dimensional datasets, as it helps mitigate problems like overfitting and the curse of dimensionality while preserving important information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, **L1 regularization** can indeed help with dimensionality reduction, particularly in the context of **feature selection**.\n",
    "\n",
    "### How L1 Regularization Helps with Dimensionality Reduction:\n",
    "\n",
    "L1 regularization (also known as **Lasso** in the context of linear models) works by adding a penalty to the model's cost function that is proportional to the absolute values of the model's coefficients. The key property of L1 regularization is that it encourages **sparsity** in the learned coefficients.\n",
    "\n",
    "### Mechanism of L1 Regularization:\n",
    "- **Sparsity**: During training, L1 regularization drives many of the coefficients to **zero**. This results in a model where only a subset of the features have non-zero coefficients, effectively **selecting** the most important features and discarding the irrelevant ones.\n",
    "- **Feature selection**: By setting less important feature coefficients to zero, L1 regularization effectively performs **automatic feature selection**, which can be seen as a form of **dimensionality reduction**. This is especially useful when dealing with datasets that have a large number of features, as it helps focus on the most relevant ones.\n",
    "  \n",
    "### L1 Regularization and Dimensionality Reduction:\n",
    "While **L1 regularization** does not directly reduce the dimensionality in the same way as techniques like **Principal Component Analysis (PCA)**, it can still be considered a form of dimensionality reduction because:\n",
    "1. **Sparse solutions**: L1 regularization leads to sparse solutions where many features have zero weights, thus reducing the effective number of features used by the model.\n",
    "2. **Feature selection**: By forcing some features' coefficients to zero, it can automatically discard irrelevant or redundant features, reducing the feature space and helping with model simplification.\n",
    "3. **Improved interpretability**: Since L1 regularization helps identify the most important features, it can improve the interpretability of the model, as you are left with only the relevant features.\n",
    "\n",
    "### Example of L1 Regularization (Lasso Regression):\n",
    "In a linear regression model, L1 regularization modifies the cost function as follows:\n",
    "\\[\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\theta \\) are the model's parameters (coefficients),\n",
    "- \\( m \\) is the number of training examples,\n",
    "- \\( \\lambda \\) is the regularization parameter,\n",
    "- \\( \\sum_{j=1}^{n} |\\theta_j| \\) is the L1 penalty term that encourages sparsity.\n",
    "\n",
    "The L1 penalty term forces some coefficients \\( \\theta_j \\) to be exactly zero, thereby eliminating the corresponding features from the model.\n",
    "\n",
    "### L1 Regularization vs. Dimensionality Reduction Techniques:\n",
    "- **L1 regularization** is more focused on **feature selection** (i.e., choosing a subset of the most relevant features) rather than projecting data onto a lower-dimensional space.\n",
    "- Techniques like **PCA** perform **linear transformations** of the data, projecting it into a lower-dimensional space where the most significant variance is captured. In contrast, L1 regularization simply selects a subset of the original features based on their importance.\n",
    "\n",
    "### Summary:\n",
    "- **L1 regularization** can help with dimensionality reduction by driving some feature coefficients to zero, effectively selecting a subset of the most relevant features.\n",
    "- It does **feature selection**, not dimensionality reduction in the strict sense (like PCA or t-SNE), but the result is a model with fewer features, which can lead to reduced complexity and improved performance.\n",
    "- It is particularly useful when you want to perform feature selection while training a model, and it's especially effective in situations with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes, QLoRA (Quantized Low-Rank Adaptation) is essentially a variation of LoRA (Low-Rank Adaptation) with quantization applied. In QLoRA, the model weights are quantized to reduce memory and computational requirements while still leveraging the benefits of LoRA. \n",
    "\n",
    "LoRA itself is a technique that adapts pre-trained models by injecting low-rank trainable matrices into the model, allowing for efficient fine-tuning with fewer parameters. By adding quantization, QLoRA further compresses the model, which is particularly useful for large language models, enabling more efficient deployment and reducing the hardware requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a comparison between **LoRA (Low-Rank Adaptation)** and **QLoRA (Quantized Low-Rank Adaptation)**:\n",
    "\n",
    "### 1. **Concept**:\n",
    "   - **LoRA**: It introduces trainable low-rank matrices to a pre-trained model to fine-tune it with fewer parameters. Instead of updating the entire model's weights, LoRA modifies only the low-rank components, which is memory-efficient.\n",
    "   - **QLoRA**: This is essentially LoRA with **quantization**. It not only uses low-rank adaptation but also quantizes the model weights (or the low-rank matrices) to further reduce the memory footprint and computational cost.\n",
    "\n",
    "### 2. **Efficiency**:\n",
    "   - **LoRA**: Reduces the number of parameters that need to be updated during fine-tuning by injecting low-rank matrices, making it more efficient than standard fine-tuning methods.\n",
    "   - **QLoRA**: Further enhances efficiency by applying quantization to the weights. This reduces the model size even more, allowing it to fit on devices with limited memory and compute resources while retaining performance.\n",
    "\n",
    "### 3. **Memory Usage**:\n",
    "   - **LoRA**: Reduces memory usage by storing only the low-rank matrices, which are smaller than the full set of model weights.\n",
    "   - **QLoRA**: In addition to using low-rank matrices, quantization compresses these matrices further by reducing the precision of the weights (e.g., using 8-bit integers instead of 32-bit floating-point values).\n",
    "\n",
    "### 4. **Model Size**:\n",
    "   - **LoRA**: The model size is reduced compared to traditional fine-tuning, but not as much as QLoRA.\n",
    "   - **QLoRA**: The model size is significantly smaller than LoRA, thanks to quantization, making it more suitable for resource-constrained environments (like edge devices or low-memory GPUs).\n",
    "\n",
    "### 5. **Training Speed**:\n",
    "   - **LoRA**: Training is faster compared to traditional fine-tuning since fewer parameters are being updated.\n",
    "   - **QLoRA**: Training can be even faster in some cases due to reduced memory bandwidth requirements and the efficient use of quantized weights.\n",
    "\n",
    "### 6. **Performance**:\n",
    "   - **LoRA**: While it significantly reduces the number of parameters and computational requirements, it still performs well in many fine-tuning tasks, though it may not always reach the same performance as fully fine-tuned models.\n",
    "   - **QLoRA**: The quantization step may cause some loss in precision, potentially impacting performance slightly. However, it still provides competitive performance with much smaller model sizes and can be a good trade-off when memory and compute are critical constraints.\n",
    "\n",
    "### 7. **Use Cases**:\n",
    "   - **LoRA**: Well-suited for fine-tuning large models on tasks where you want to save memory but still need high-quality performance, such as in NLP or computer vision.\n",
    "   - **QLoRA**: Ideal for deploying models in resource-constrained environments where both memory and computation are limited, such as on mobile devices or low-power edge devices.\n",
    "\n",
    "### Summary:\n",
    "- **LoRA** focuses on reducing the number of trainable parameters using low-rank matrices.\n",
    "- **QLoRA** extends LoRA by applying quantization, which reduces memory usage even further and is beneficial for environments with stricter hardware limitations.\n",
    "\n",
    "QLoRA is generally more memory-efficient than LoRA but might sacrifice some performance due to the quantization step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **vanishing gradient** and **exploding gradient** problems are two common issues encountered during the training of deep neural networks, especially when using gradient-based optimization algorithms like **Stochastic Gradient Descent (SGD)**. Both issues arise due to the behavior of gradients during backpropagation, but they manifest in different ways and have different impacts on training.\n",
    "\n",
    "### 1. **Vanishing Gradient Problem**\n",
    "\n",
    "#### Definition:\n",
    "The **vanishing gradient problem** occurs when the gradients (i.e., the partial derivatives of the loss function with respect to the model parameters) become **extremely small** as they are propagated backward through the network during training. This causes the weights in the earlier layers of the network to update very slowly, or not at all, effectively **stalling** the training process.\n",
    "\n",
    "#### Cause:\n",
    "- In deep neural networks, gradients are calculated via the **chain rule** during backpropagation. When gradients are propagated back through layers, they can be repeatedly multiplied by small numbers (for example, the derivatives of activation functions like **sigmoid** or **tanh**, which have small gradients for large positive or negative inputs).\n",
    "- If the weights or activation functions cause the gradient to shrink, the gradients of the earlier layers (closer to the input) become **smaller and smaller** as the network depth increases.\n",
    "  \n",
    "#### Impact:\n",
    "- The **earlier layers** of the network (closer to the input) receive very small gradients, making their weights update very slowly or not at all. This can result in poor learning, especially for deep networks, because the model fails to capture complex patterns in the data.\n",
    "- **Training stagnation**: The model may fail to improve or converge to a good solution because of the insufficient updates to the weights.\n",
    "\n",
    "#### Common Scenarios:\n",
    "- **Activation functions like sigmoid and tanh**: These activation functions squash the input into a small range, and their gradients are small for large positive or negative inputs, contributing to the vanishing gradient problem.\n",
    "- **Deep networks**: The problem becomes more pronounced as the depth of the network increases, because gradients can diminish as they are propagated backward through many layers.\n",
    "\n",
    "#### Solutions:\n",
    "- **ReLU (Rectified Linear Unit)** activation function: ReLU helps mitigate the vanishing gradient problem because its gradient is either 0 (for negative inputs) or 1 (for positive inputs), which is less likely to cause gradients to shrink.\n",
    "- **He initialization**: This initialization method helps maintain the scale of gradients by ensuring that the variance of the weights is appropriately scaled.\n",
    "- **Batch normalization**: Normalizing the inputs to each layer helps reduce the effect of vanishing gradients by ensuring that activations are on a consistent scale.\n",
    "- **Skip connections** (Residual Networks): Networks like **ResNets** use skip connections to allow gradients to flow more easily across layers, bypassing certain layers if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Exploding Gradient Problem**\n",
    "\n",
    "#### Definition:\n",
    "The **exploding gradient problem** occurs when the gradients become **very large** during backpropagation, causing the weights to update by extremely large amounts. This can lead to **instability** in the model, where the model’s weights diverge, making it impossible to find a good solution.\n",
    "\n",
    "#### Cause:\n",
    "- Similar to the vanishing gradient problem, the exploding gradient problem arises due to the **chain rule** in backpropagation. However, in this case, the gradients get multiplied by **large values** (e.g., large weights or large derivatives of the activation functions), causing them to grow exponentially as they are propagated back through the network.\n",
    "- If the weights are not properly initialized or if the learning rate is too high, the gradients can blow up and lead to extremely large updates to the weights.\n",
    "\n",
    "#### Impact:\n",
    "- The **weights of the network** can become extremely large, which leads to instability in the training process.\n",
    "- The model may **diverge** rather than converge to a good solution. In the worst case, the training process can fail completely, with the model’s weights growing to infinity.\n",
    "\n",
    "#### Common Scenarios:\n",
    "- **Deep networks with poor initialization**: If the network’s weights are initialized too large, gradients can accumulate and explode as they propagate back through the layers.\n",
    "- **High learning rates**: If the learning rate is too high, even small gradients can lead to large updates to the weights, causing instability.\n",
    "- **Activation functions with large derivatives**: Some activation functions can have very large gradients for certain inputs, which can exacerbate the exploding gradient problem.\n",
    "\n",
    "#### Solutions:\n",
    "- **Gradient clipping**: This technique involves capping the gradients during backpropagation to a predefined maximum value, preventing them from becoming too large.\n",
    "- **Weight regularization**: Techniques like **L2 regularization** can help prevent the weights from becoming too large, reducing the chances of exploding gradients.\n",
    "- **Proper initialization**: Using weight initialization techniques like **Xavier** or **He initialization** helps prevent the gradients from exploding or vanishing.\n",
    "- **Lower learning rate**: Using a smaller learning rate can help reduce the magnitude of weight updates and prevent gradients from growing too large.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Differences:\n",
    "\n",
    "| Feature                  | Vanishing Gradient Problem                          | Exploding Gradient Problem                           |\n",
    "|--------------------------|------------------------------------------------------|------------------------------------------------------|\n",
    "| **Cause**                 | Gradients become very small during backpropagation, leading to small weight updates. | Gradients become very large, causing large weight updates. |\n",
    "| **Impact**                | Causes slow or no learning in the earlier layers, preventing the model from converging. | Causes model instability, where weights can grow uncontrollably and the model fails to converge. |\n",
    "| **Common Activation Functions** | Sigmoid, tanh, and other saturating activation functions. | Poor weight initialization, large learning rates, or activation functions with large gradients. |\n",
    "| **Solution**              | Use ReLU or variants, batch normalization, skip connections, He initialization. | Gradient clipping, proper weight initialization, lower learning rates, and regularization. |\n",
    "\n",
    "Both vanishing and exploding gradients are critical challenges when training deep neural networks, and understanding how to mitigate them is crucial for building effective models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Masked Attention** vs **Bidirectional Attention**:\n",
    "\n",
    "Both masked attention and bidirectional attention are mechanisms used in attention-based models like transformers, but they are applied in different contexts and serve different purposes. Here's a breakdown of the two:\n",
    "\n",
    "### 1. **Masked Attention**:\n",
    "   - **Purpose**: Masked attention is primarily used to **prevent information leakage** from future tokens during training or inference. It is typically used in **autoregressive models** like GPT (Generative Pre-trained Transformer) where the model is trained to predict the next token in a sequence.\n",
    "   - **How it works**: \n",
    "     - In masked attention, during the self-attention process, a **mask** is applied to prevent the model from attending to future tokens. This means that when the model is processing token `i`, it can only attend to tokens from `1` to `i` (or the current token and previous tokens) and not to any future tokens.\n",
    "     - The mask is usually applied to the attention weights, setting the weights of the future tokens to negative infinity or zero, ensuring that they don't contribute to the current token's attention.\n",
    "   - **Use case**: This is crucial for tasks where you need to predict the next token in a sequence or generate text in a left-to-right fashion, as in autoregressive models like GPT.\n",
    "\n",
    "   **Example**: In GPT, when predicting the 5th word, the model is only allowed to attend to words 1 to 4 and not 6 to the end of the sequence.\n",
    "\n",
    "   - **Pros**:\n",
    "     - Prevents \"cheating\" by ensuring the model only has access to past context when predicting the next token.\n",
    "     - Essential for autoregressive generation.\n",
    "   - **Cons**:\n",
    "     - Limits the model's ability to fully understand the context from the entire sequence during training.\n",
    "\n",
    "### 2. **Bidirectional Attention**:\n",
    "   - **Purpose**: Bidirectional attention allows the model to **attend to all tokens** in the sequence (both past and future) simultaneously. It is typically used in **non-autoregressive models** like BERT (Bidirectional Encoder Representations from Transformers).\n",
    "   - **How it works**: \n",
    "     - In bidirectional attention, during self-attention, the model can attend to **all tokens** in the sequence (both left and right context). This allows the model to fully capture the context for each token, leading to a richer understanding of the sequence.\n",
    "     - There is no masking, and the model can attend to the entire sequence in both directions, making it \"bidirectional.\"\n",
    "   - **Use case**: This is useful for tasks where you need to understand the full context of the sequence at once, such as in **masked language modeling (MLM)** tasks, question answering, or sentence classification.\n",
    "\n",
    "   **Example**: In BERT, when processing the word \"apple\" in the sentence \"I ate an apple,\" the model can attend to both \"I ate\" and \"an apple,\" understanding the context in both directions.\n",
    "\n",
    "   - **Pros**:\n",
    "     - Provides a full understanding of the sequence by attending to both past and future tokens.\n",
    "     - Improves performance on tasks that require a holistic understanding of the entire sequence.\n",
    "   - **Cons**:\n",
    "     - Not suitable for autoregressive generation tasks, as the model has access to future tokens during training.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| **Feature**               | **Masked Attention**                                     | **Bidirectional Attention**                              |\n",
    "|---------------------------|----------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Context**                | Only attends to past tokens (unidirectional).            | Attends to both past and future tokens (bidirectional).   |\n",
    "| **Use Case**               | Autoregressive tasks (e.g., text generation, GPT).       | Tasks requiring full context understanding (e.g., BERT). |\n",
    "| **Training Objective**     | Predict the next token given the previous tokens.       | Predict missing tokens using full sequence context.      |\n",
    "| **Masking**                | Future tokens are masked (not attended to).             | No masking; all tokens can attend to each other.         |\n",
    "| **Example Models**         | GPT, autoregressive models.                             | BERT, masked language models.                            |\n",
    "\n",
    "### Summary:\n",
    "- **Masked Attention**: Used in autoregressive models where future tokens must not be seen during training, ensuring the model predicts tokens sequentially.\n",
    "- **Bidirectional Attention**: Used in models like BERT where the entire context (both past and future tokens) is available for understanding the sequence. This is typically used for tasks like classification, QA, and other non-generative tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so only difference is actually what type of mask we use?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you're right in a way! The key difference between **masked attention** and **bidirectional attention** primarily comes down to the **masking strategy** used during the attention mechanism. Here’s how the masking strategy differentiates them:\n",
    "\n",
    "### 1. **Masked Attention**:\n",
    "   - The model uses a **mask** to block access to future tokens. \n",
    "   - This mask ensures that the model can only attend to the current token and previous tokens (i.e., tokens on the left of the current position).\n",
    "   - The mask is typically **causal** or **autoregressive**, where future tokens are prevented from being attended to, which simulates the process of generating text token-by-token.\n",
    "\n",
    "   **Example**: For the sequence \"I ate an apple,\" when predicting the token \"an,\" the model can only attend to \"I ate\" (tokens to the left) and not \"apple\" (tokens to the right).\n",
    "\n",
    "### 2. **Bidirectional Attention**:\n",
    "   - In this case, **no mask** is used to block access to any part of the sequence. The model can attend to **all tokens** in the sequence (both past and future) during training.\n",
    "   - The model has a complete understanding of the entire sequence context at once, which is useful for tasks where understanding the full context is crucial.\n",
    "\n",
    "   **Example**: In BERT, when processing the word \"apple\" in the sentence \"I ate an apple,\" the model can attend to both \"I ate\" and \"an apple,\" utilizing both the left and right context.\n",
    "\n",
    "### In Summary:\n",
    "- **Masked Attention** involves applying a **mask** that limits the attention to past tokens only, which is essential for tasks like autoregressive text generation.\n",
    "- **Bidirectional Attention** doesn't apply any mask and allows the model to attend to both past and future tokens, enabling a richer understanding of the entire sequence, which is great for tasks like sentence classification or masked language modeling.\n",
    "\n",
    "So yes, the **main difference** lies in the **masking strategy**: masked attention restricts the model’s access to future tokens, while bidirectional attention allows access to the full sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE is not deffernetiable at orgin that's why we use MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the statement that **Mean Absolute Error (MAE)** is not differentiable at the origin is **correct**, but the reasoning for why we might prefer **Mean Squared Error (MSE)** in some cases needs a bit more clarification.\n",
    "\n",
    "### Differentiability of MAE:\n",
    "The **Mean Absolute Error (MAE)** is defined as:\n",
    "\n",
    "\\[\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the true value,\n",
    "- \\( \\hat{y}_i \\) is the predicted value,\n",
    "- \\( n \\) is the number of samples.\n",
    "\n",
    "The issue with MAE arises at the point where the absolute error \\( |y_i - \\hat{y}_i| \\) is **zero** (i.e., when the prediction is exactly correct). This creates a **kink** in the function at the origin (where the error is zero), making the derivative undefined at this point.\n",
    "\n",
    "Mathematically, the derivative of \\( |y_i - \\hat{y}_i| \\) with respect to \\( \\hat{y}_i \\) is:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{d\\hat{y}_i} |y_i - \\hat{y}_i| =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } y_i - \\hat{y}_i > 0 \\\\\n",
    "-1 & \\text{if } y_i - \\hat{y}_i < 0 \\\\\n",
    "\\text{undefined} & \\text{if } y_i - \\hat{y}_i = 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "The gradient is undefined when \\( \\hat{y}_i = y_i \\), so MAE is **non-differentiable** at the origin.\n",
    "\n",
    "### Why MSE is Used Instead:\n",
    "The **Mean Squared Error (MSE)** is defined as:\n",
    "\n",
    "\\[\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "Unlike MAE, the squared error function is **differentiable everywhere**, including at the origin. The derivative of \\( (y_i - \\hat{y}_i)^2 \\) is:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{d\\hat{y}_i} (y_i - \\hat{y}_i)^2 = -2(y_i - \\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Since this is well-defined and continuous everywhere, **MSE is differentiable at all points**, including when \\( \\hat{y}_i = y_i \\).\n",
    "\n",
    "### Why MSE is Preferred in Many ML Models:\n",
    "1. **Smooth Gradient**: The differentiability of MSE at all points (including the origin) provides a smooth gradient, which makes it easier to optimize using gradient-based methods like **Gradient Descent**. This smoothness helps avoid problems during optimization, especially when using deep learning models or more complex regression techniques.\n",
    "\n",
    "2. **Convexity**: MSE is a **convex function**, meaning that it has a single global minimum. This property is useful for optimization because it ensures that gradient-based optimization algorithms can reliably converge to the optimal solution without getting stuck in local minima.\n",
    "\n",
    "3. **Smoother Adjustments**: Since MSE squares the error, it tends to penalize larger errors more heavily than MAE. This can lead to a model that fits outliers better, which may be desirable in certain contexts, especially when large errors are undesirable.\n",
    "\n",
    "### When MAE is Preferred:\n",
    "Despite MSE being differentiable and often used in practice, **MAE** has its advantages in some cases:\n",
    "- **Robustness to Outliers**: MAE is less sensitive to large outliers than MSE. Since the error is not squared, large deviations in predictions don't get penalized as heavily, which can be beneficial when outliers are not as important or should not disproportionately affect the model.\n",
    "- **Interpretability**: MAE is more interpretable because it directly measures the average absolute error in the same units as the data, while MSE gives the error in squared units, which can sometimes be harder to interpret.\n",
    "\n",
    "### Summary:\n",
    "- **MAE** is not differentiable at the origin, but it is still useful in many applications, especially when robustness to outliers is desired.\n",
    "- **MSE** is differentiable everywhere, making it easier to optimize with gradient-based methods and often preferred in many machine learning models.\n",
    "- The choice between MAE and MSE depends on the specific requirements of the problem, such as sensitivity to outliers or the need for smooth gradients for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: A Comprehensive Overview\n",
    "\n",
    "Linear regression is a fundamental algorithm in machine learning and statistics used to model the relationship between one or more independent variables (features) and a dependent variable (target). Here's an in-depth explanation of linear regression, its equations, and the mathematical flow.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Objective**\n",
    "The goal of linear regression is to find the best-fit line (or hyperplane in higher dimensions) that minimizes the difference between the predicted values (\\(\\hat{y}\\)) and the actual values (\\(y\\)).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Hypothesis Function**\n",
    "In linear regression, the hypothesis function is:\n",
    "\\[\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n",
    "\\]\n",
    "- \\(\\hat{y}\\): Predicted value.\n",
    "- \\(\\beta_0\\): Intercept (bias term).\n",
    "- \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\): Coefficients (weights) of the independent variables.\n",
    "- \\(x_1, x_2, \\dots, x_p\\): Independent variables (features).\n",
    "\n",
    "In vectorized form:\n",
    "\\[\n",
    "\\hat{y} = X \\beta\n",
    "\\]\n",
    "- \\(X\\): Matrix of input features (\\(n \\times (p+1)\\)), where \\(n\\) is the number of samples and \\(p\\) is the number of features. The first column is typically 1 to account for \\(\\beta_0\\).\n",
    "- \\(\\beta\\): Coefficient vector (\\((p+1) \\times 1\\)).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Cost Function**\n",
    "The cost function measures the error between predicted and actual values. For linear regression, the most common cost function is the **Mean Squared Error (MSE)**:\n",
    "\\[\n",
    "J(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2\n",
    "\\]\n",
    "In vectorized form:\n",
    "\\[\n",
    "J(\\beta) = \\frac{1}{2n} \\| X\\beta - y \\|^2\n",
    "\\]\n",
    "- \\(\\| X\\beta - y \\|\\): Euclidean norm of the residual vector.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Optimization: Minimizing the Cost Function**\n",
    "To find the optimal \\(\\beta\\) values, we minimize \\(J(\\beta)\\). This is done using either **Analytical Methods** or **Gradient Descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1 Analytical Solution (Normal Equation)**\n",
    "The normal equation directly computes \\(\\beta\\) that minimizes \\(J(\\beta)\\):\n",
    "\\[\n",
    "\\beta = (X^T X)^{-1} X^T y\n",
    "\\]\n",
    "- \\(X^T\\): Transpose of \\(X\\).\n",
    "- \\((X^T X)^{-1}\\): Inverse of the Gram matrix (\\(X^T X\\)).\n",
    "- \\(X^T y\\): Correlation between features and the target.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 Gradient Descent**\n",
    "Gradient descent iteratively updates \\(\\beta\\) to minimize \\(J(\\beta)\\):\n",
    "\\[\n",
    "\\beta := \\beta - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta}\n",
    "\\]\n",
    "Where:\n",
    "- \\(\\alpha\\): Learning rate.\n",
    "- \\(\\frac{\\partial J(\\beta)}{\\partial \\beta}\\): Gradient of the cost function w.r.t. \\(\\beta\\).\n",
    "\n",
    "The gradient is:\n",
    "\\[\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta} = \\frac{1}{n} X^T (X\\beta - y)\n",
    "\\]\n",
    "\n",
    "Update rule:\n",
    "\\[\n",
    "\\beta := \\beta - \\frac{\\alpha}{n} X^T (X\\beta - y)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Assumptions of Linear Regression**\n",
    "1. **Linearity**: The relationship between \\(X\\) and \\(y\\) is linear.\n",
    "2. **Independence**: Observations are independent.\n",
    "3. **Homoscedasticity**: Constant variance of residuals.\n",
    "4. **Normality**: Residuals are normally distributed.\n",
    "5. **No Multicollinearity**: Independent variables are not highly correlated.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Evaluation Metrics**\n",
    "To assess the performance of the linear regression model, we use:\n",
    "- **Mean Squared Error (MSE)**:\n",
    "  \\[\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "  \\]\n",
    "- **Mean Absolute Error (MAE)**:\n",
    "  \\[\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "  \\]\n",
    "- **\\(R^2\\) (Coefficient of Determination)**:\n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{\\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}{\\sum_{i=1}^{n} \\left( y_i - \\bar{y} \\right)^2}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Regularization**\n",
    "To prevent overfitting, regularization can be added to the cost function:\n",
    "- **Ridge Regression (L2 Regularization)**:\n",
    "  \\[\n",
    "  J(\\beta) = \\frac{1}{2n} \\| X\\beta - y \\|^2 + \\lambda \\| \\beta \\|^2\n",
    "  \\]\n",
    "- **Lasso Regression (L1 Regularization)**:\n",
    "  \\[\n",
    "  J(\\beta) = \\frac{1}{2n} \\| X\\beta - y \\|^2 + \\lambda \\| \\beta \\|_1\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Multivariate Linear Regression**\n",
    "When \\(p > 1\\) (multiple features), the process remains the same:\n",
    "- The hypothesis is extended to multiple features.\n",
    "- The cost function, gradient, and update rules are computed for all dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Flow of Linear Regression**\n",
    "1. **Define Hypothesis**: \\(\\hat{y} = X\\beta\\).\n",
    "2. **Compute Cost Function**: \\(J(\\beta) = \\frac{1}{2n} \\| X\\beta - y \\|^2\\).\n",
    "3. **Optimize**:\n",
    "   - Analytical: Solve \\(\\beta = (X^T X)^{-1} X^T y\\).\n",
    "   - Gradient Descent: Update \\(\\beta := \\beta - \\alpha \\frac{1}{n} X^T (X\\beta - y)\\).\n",
    "4. **Evaluate Model**: Use metrics like \\(R^2\\), MSE, or MAE.\n",
    "5. **Regularize (if needed)**: Apply L1/L2 regularization for robustness.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like any part explained further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) in Machine Learning: An In-Depth Overview\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It aims to find the optimal hyperplane that best separates data into different classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Core Idea of SVM**\n",
    "\n",
    "SVM finds the **hyperplane** in an \\(n\\)-dimensional space (\\(n\\) being the number of features) that separates the data points of different classes with the **maximum margin**.\n",
    "\n",
    "- **Margin**: The distance between the hyperplane and the nearest data points (support vectors) from each class.\n",
    "- **Support Vectors**: The data points closest to the hyperplane that influence its position.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Mathematical Formulation**\n",
    "\n",
    "### Binary Classification:\n",
    "Given a dataset of \\(n\\) points \\((x_i, y_i)\\), where:\n",
    "- \\(x_i \\in \\mathbb{R}^d\\) (feature vector),\n",
    "- \\(y_i \\in \\{-1, +1\\}\\) (class labels).\n",
    "\n",
    "The goal is to find a hyperplane:\n",
    "\\[\n",
    "w \\cdot x + b = 0\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(w\\) is the weight vector (normal to the hyperplane),\n",
    "- \\(b\\) is the bias (offset from the origin).\n",
    "\n",
    "### Conditions for Separation:\n",
    "For a correctly classified point:\n",
    "\\[\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Optimization Problem**\n",
    "\n",
    "SVM maximizes the margin while ensuring all points are correctly classified.\n",
    "\n",
    "### Margin:\n",
    "The margin is given by:\n",
    "\\[\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "\\]\n",
    "\n",
    "### Objective Function:\n",
    "To maximize the margin, we minimize \\(\\|w\\|^2\\), leading to the following optimization problem:\n",
    "\n",
    "\\[\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "\\]\n",
    "Subject to:\n",
    "\\[\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Soft Margin SVM**\n",
    "\n",
    "Real-world data may not be linearly separable. To handle this, SVM introduces a **slack variable** (\\(\\xi_i\\)) to allow some misclassifications.\n",
    "\n",
    "### Modified Optimization Problem:\n",
    "\\[\n",
    "\\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "\\]\n",
    "Subject to:\n",
    "\\[\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(C\\) is the regularization parameter controlling the trade-off between margin maximization and misclassification penalty.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Kernel Trick**\n",
    "\n",
    "When data is not linearly separable in the input space, SVM uses the **kernel trick** to map the data to a higher-dimensional space where it becomes linearly separable.\n",
    "\n",
    "### Kernel Function:\n",
    "\\[\n",
    "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "\\]\n",
    "\n",
    "Common kernels include:\n",
    "1. **Linear Kernel**: \\(K(x_i, x_j) = x_i \\cdot x_j\\)\n",
    "2. **Polynomial Kernel**: \\(K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d\\)\n",
    "3. **Radial Basis Function (RBF) Kernel**: \\(K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)\\)\n",
    "4. **Sigmoid Kernel**: \\(K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + r)\\)\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Dual Formulation**\n",
    "\n",
    "The optimization problem can be expressed in its dual form, which allows the use of kernel functions.\n",
    "\n",
    "### Dual Objective Function:\n",
    "\\[\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "\\]\n",
    "Subject to:\n",
    "\\[\n",
    "0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\alpha_i\\) are the Lagrange multipliers.\n",
    "\n",
    "The decision boundary is then given by:\n",
    "\\[\n",
    "f(x) = \\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **7. SVM for Multi-Class Classification**\n",
    "\n",
    "SVM is inherently a binary classifier. For multi-class problems, common strategies include:\n",
    "1. **One-vs-One (OvO)**: Train \\(k(k-1)/2\\) classifiers for \\(k\\) classes.\n",
    "2. **One-vs-Rest (OvR)**: Train \\(k\\) classifiers, one for each class against the rest.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Advantages of SVM**\n",
    "\n",
    "1. **Effective in High Dimensions**: Works well even when the number of features exceeds the number of samples.\n",
    "2. **Robust to Overfitting**: Especially with proper regularization.\n",
    "3. **Kernel Trick**: Handles non-linear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Disadvantages of SVM**\n",
    "\n",
    "1. **Computationally Intensive**: Training time scales poorly with the number of samples (\\(O(n^3)\\)).\n",
    "2. **Choice of Kernel**: Requires careful tuning.\n",
    "3. **Hard to Interpret**: Especially with non-linear kernels.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Python Implementation**\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a deeper dive into kernels, optimization, or specific applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **conditions for separation** in Support Vector Machines (SVM) are derived from the goal of finding a hyperplane that separates two classes such that all points lie on the correct side of the decision boundary with the maximum margin. Here's a detailed breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. General Form of the Hyperplane**\n",
    "The hyperplane equation is:\n",
    "\\[\n",
    "w \\cdot x + b = 0\n",
    "\\]\n",
    "Where:\n",
    "- \\(w\\) is the weight vector (normal to the hyperplane),\n",
    "- \\(x\\) is the feature vector,\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The decision rule for classification is:\n",
    "\\[\n",
    "f(x) = \\text{sign}(w \\cdot x + b)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Separation Conditions**\n",
    "To ensure that data points are correctly classified, the following conditions must hold for each data point \\((x_i, y_i)\\), where \\(y_i \\in \\{-1, +1\\}\\):\n",
    "\n",
    "1. For a point in the positive class (\\(y_i = +1\\)):\n",
    "   \\[\n",
    "   w \\cdot x_i + b \\geq +1\n",
    "   \\]\n",
    "\n",
    "2. For a point in the negative class (\\(y_i = -1\\)):\n",
    "   \\[\n",
    "   w \\cdot x_i + b \\leq -1\n",
    "   \\]\n",
    "\n",
    "These can be combined into a single condition:\n",
    "\\[\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Geometric Interpretation**\n",
    "- **Correct Classification**: A point is correctly classified if it satisfies \\(y_i (w \\cdot x_i + b) \\geq 1\\).\n",
    "- **Margin**: The margin is the distance between the hyperplane and the nearest data points. SVM maximizes this margin to ensure robust classification.\n",
    "- **Support Vectors**: The points that lie exactly on the margin boundary (\\(y_i (w \\cdot x_i + b) = 1\\)) are called **support vectors**. These points define the hyperplane.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Non-Separable Case**\n",
    "In real-world scenarios, perfect separation may not be possible. To handle this:\n",
    "- **Soft Margin SVM** introduces slack variables (\\(\\xi_i\\)) to allow some misclassifications:\n",
    "  \\[\n",
    "  y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "  \\]\n",
    "- The optimization objective includes a penalty for these violations, controlled by the regularization parameter \\(C\\).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to dive deeper into any of these concepts, such as the role of support vectors or how the margin is computed geometrically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (L1) vs. Ridge (L2) Regularization: A Detailed Comparison\n",
    "\n",
    "Regularization techniques are used in machine learning to prevent overfitting by adding a penalty term to the loss function. The penalty discourages overly complex models, helping them generalize better to unseen data. **Lasso (L1)** and **Ridge (L2)** regularization are two popular methods, and they differ in how they penalize model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Overview of Regularization**\n",
    "\n",
    "In linear regression, the cost function (mean squared error) is augmented with a penalty term to enforce simplicity:\n",
    "\n",
    "\\[\n",
    "J(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\cdot \\text{Penalty}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\lambda\\): Regularization parameter controlling the strength of the penalty.\n",
    "- Penalty: Defined differently for L1 and L2 regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Ridge Regression (L2 Regularization)**\n",
    "\n",
    "### Penalty Term:\n",
    "\\[\n",
    "\\text{Penalty} = \\| \\beta \\|^2 = \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\]\n",
    "\n",
    "### Updated Cost Function:\n",
    "\\[\n",
    "J(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\]\n",
    "\n",
    "### Key Characteristics:\n",
    "1. **Effect on Coefficients**: L2 regularization shrinks coefficients closer to zero but does not set them exactly to zero.\n",
    "2. **Smoothness**: L2 encourages small, distributed coefficients across all features.\n",
    "3. **Solution**: The optimization problem for Ridge is convex and differentiable, making it computationally efficient to solve.\n",
    "\n",
    "### Analytical Solution:\n",
    "The optimal coefficients can be computed using:\n",
    "\\[\n",
    "\\beta = \\left( X^T X + \\lambda I \\right)^{-1} X^T y\n",
    "\\]\n",
    "Where \\(I\\) is the identity matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Lasso Regression (L1 Regularization)**\n",
    "\n",
    "### Penalty Term:\n",
    "\\[\n",
    "\\text{Penalty} = \\| \\beta \\|_1 = \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\]\n",
    "\n",
    "### Updated Cost Function:\n",
    "\\[\n",
    "J(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\]\n",
    "\n",
    "### Key Characteristics:\n",
    "1. **Effect on Coefficients**: L1 regularization encourages sparsity by setting some coefficients exactly to zero, effectively performing feature selection.\n",
    "2. **Solution**: The optimization problem for Lasso is not differentiable at \\(\\beta_j = 0\\), requiring specialized solvers like coordinate descent.\n",
    "3. **Feature Selection**: L1 is particularly useful when many features are irrelevant.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Comparison**\n",
    "\n",
    "| **Aspect**               | **Ridge (L2)**                                      | **Lasso (L1)**                                    |\n",
    "|---------------------------|----------------------------------------------------|--------------------------------------------------|\n",
    "| **Penalty Term**          | \\(\\sum_{j=1}^{p} \\beta_j^2\\)                      | \\(\\sum_{j=1}^{p} |\\beta_j|\\)                     |\n",
    "| **Effect on Coefficients**| Shrinks coefficients closer to zero but keeps all | Shrinks some coefficients to exactly zero        |\n",
    "| **Feature Selection**     | Does not perform feature selection                | Performs automatic feature selection             |\n",
    "| **Complexity**            | Computationally simpler (differentiable)          | Computationally more complex (non-differentiable)|\n",
    "| **Best Use Case**         | When all features are relevant                    | When there are many irrelevant features          |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Elastic Net Regularization**\n",
    "\n",
    "Elastic Net combines L1 and L2 regularization:\n",
    "\\[\n",
    "J(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\]\n",
    "\n",
    "### Advantages:\n",
    "- Combines the strengths of Lasso (sparsity) and Ridge (stability).\n",
    "- Useful when features are highly correlated, as Lasso alone can randomly select one among correlated features.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Practical Scenarios**\n",
    "\n",
    "### When to Use Ridge (L2):\n",
    "- Most features are believed to contribute to the target variable.\n",
    "- Multicollinearity exists among features.\n",
    "- Smooth models with small coefficients are desired.\n",
    "\n",
    "### When to Use Lasso (L1):\n",
    "- There are many irrelevant or redundant features.\n",
    "- Sparse solutions (with fewer active features) are required.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Visualization**\n",
    "\n",
    "- **Ridge**: Penalizes large coefficients proportionally, creating circular contours in parameter space.\n",
    "- **Lasso**: Penalizes coefficients proportionally to their magnitude, creating diamond-shaped contours that lead to sparsity.\n",
    "\n",
    "Let me know if you'd like a code example or further details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression in Machine Learning: An In-Depth Exploration\n",
    "\n",
    "Logistic regression is a statistical method used in machine learning for binary classification tasks. It predicts the probability of an instance belonging to one of two classes. Despite its name, logistic regression is a classification algorithm, not a regression algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Why Logistic Regression?**\n",
    "Linear regression is unsuitable for classification because:\n",
    "1. Its predictions can exceed the range \\([0, 1]\\), making them unsuitable for probabilities.\n",
    "2. It is sensitive to outliers and may not handle binary outcomes well.\n",
    "\n",
    "Logistic regression solves this by transforming the output of linear regression using a **sigmoid function**, ensuring predictions lie between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. The Sigmoid Function**\n",
    "\n",
    "The sigmoid (or logistic) function maps any real-valued number to a range between 0 and 1:\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p = X \\beta\\) (linear combination of features).\n",
    "\n",
    "The sigmoid function ensures the output is interpreted as the probability of the positive class (\\(P(y=1|X)\\)).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Logistic Regression Model**\n",
    "\n",
    "For a binary classification problem, the logistic regression model predicts:\n",
    "\\[\n",
    "P(y=1|X) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "Where:\n",
    "- \\(P(y=1|X)\\) is the probability of the positive class.\n",
    "- \\(z = X \\beta\\) is the linear combination of features and coefficients.\n",
    "\n",
    "The probability of the negative class (\\(y=0\\)) is:\n",
    "\\[\n",
    "P(y=0|X) = 1 - P(y=1|X)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Decision Boundary**\n",
    "\n",
    "Logistic regression predicts \\(y=1\\) if \\(P(y=1|X) \\geq 0.5\\), and \\(y=0\\) otherwise. The threshold can be adjusted based on the problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Log-Odds and Logit Function**\n",
    "\n",
    "The relationship between the linear model (\\(z\\)) and probabilities is expressed in terms of log-odds:\n",
    "\\[\n",
    "\\text{Odds} = \\frac{P(y=1|X)}{P(y=0|X)} = e^z\n",
    "\\]\n",
    "\n",
    "Taking the logarithm:\n",
    "\\[\n",
    "\\text{Log-Odds (Logit)} = \\log\\left(\\frac{P(y=1|X)}{1 - P(y=1|X)}\\right) = z = X \\beta\n",
    "\\]\n",
    "\n",
    "Thus, logistic regression is linear in the log-odds space.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Loss Function for Logistic Regression**\n",
    "\n",
    "Instead of minimizing mean squared error (as in linear regression), logistic regression minimizes the **log loss** or **cross-entropy loss**, derived from the likelihood of the data.\n",
    "\n",
    "### Likelihood Function:\n",
    "The likelihood function is the probability of observing the given data:\n",
    "\\[\n",
    "L(\\beta) = \\prod_{i=1}^{n} P(y_i|X_i) = \\prod_{i=1}^{n} \\left[ \\sigma(z_i) \\right]^{y_i} \\left[ 1 - \\sigma(z_i) \\right]^{1-y_i}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(z_i = X_i \\beta\\).\n",
    "\n",
    "### Log-Likelihood:\n",
    "Taking the logarithm simplifies the computation:\n",
    "\\[\n",
    "\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right]\n",
    "\\]\n",
    "\n",
    "### Loss Function (Negative Log-Likelihood):\n",
    "To minimize the loss, we negate the log-likelihood:\n",
    "\\[\n",
    "J(\\beta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right]\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Gradient Descent for Optimization**\n",
    "\n",
    "Logistic regression coefficients (\\(\\beta\\)) are estimated using **gradient descent** by minimizing the log-loss function.\n",
    "\n",
    "### Gradient of the Loss Function:\n",
    "The gradient with respect to \\(\\beta\\) is:\n",
    "\\[\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\sigma(z_i) - y_i \\right) X_i\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\sigma(z_i) = P(y=1|X_i)\\).\n",
    "\n",
    "### Gradient Descent Update Rule:\n",
    "\\[\n",
    "\\beta \\gets \\beta - \\alpha \\cdot \\frac{\\partial J(\\beta)}{\\partial \\beta}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\alpha\\): Learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Regularization in Logistic Regression**\n",
    "\n",
    "To prevent overfitting, regularization terms are added to the loss function:\n",
    "\n",
    "### Ridge (L2 Regularization):\n",
    "\\[\n",
    "J(\\beta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\]\n",
    "\n",
    "### Lasso (L1 Regularization):\n",
    "\\[\n",
    "J(\\beta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right] + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Multiclass Logistic Regression**\n",
    "\n",
    "For multiclass problems (\\(K > 2\\)), logistic regression is extended using the **softmax function**:\n",
    "\\[\n",
    "P(y=k|X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(z_k = X \\beta_k\\) is the linear score for class \\(k\\).\n",
    "\n",
    "The loss function becomes the categorical cross-entropy:\n",
    "\\[\n",
    "J(\\beta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{i,k} \\log(P(y=k|X_i))\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Assumptions of Logistic Regression**\n",
    "\n",
    "1. **Linearity in Log-Odds**: The log-odds are a linear function of the input features.\n",
    "2. **Independence of Observations**: Data points are independent of each other.\n",
    "3. **No Multicollinearity**: Features are not highly correlated.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Advantages and Disadvantages**\n",
    "\n",
    "### Advantages:\n",
    "- Simple and interpretable.\n",
    "- Works well for linearly separable data.\n",
    "- Probabilistic predictions.\n",
    "\n",
    "### Disadvantages:\n",
    "- Struggles with non-linear relationships (requires feature engineering).\n",
    "- Sensitive to outliers.\n",
    "- Assumes linearity in log-odds.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a practical Python implementation or more details on specific aspects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernel in Support Vector Machines (SVM)**\n",
    "\n",
    "In SVM, the **kernel trick** is a mathematical technique that enables the algorithm to solve non-linear problems by transforming the input data into a higher-dimensional space where a linear decision boundary can be found.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Use Kernels?**\n",
    "Real-world data is often **not linearly separable** in its original feature space. Kernels allow SVM to work efficiently in these scenarios by applying a non-linear transformation \\(\\phi(x)\\) to the data, implicitly mapping it to a higher-dimensional space.\n",
    "\n",
    "The SVM optimization problem only involves the **dot product** of data points. Instead of explicitly transforming the data to a higher dimension (which can be computationally expensive), the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### Recurrent Neural Networks (RNNs) in Depth\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** are a class of neural networks designed to handle sequential data, making them ideal for tasks such as time series prediction, natural language processing (NLP), speech recognition, and more. Unlike traditional feedforward neural networks, RNNs have connections that form cycles within the network, allowing them to maintain a form of memory and use information from previous time steps to influence the output.\n",
    "\n",
    "### Key Characteristics of RNNs:\n",
    "- **Sequential Data Processing**: RNNs process data sequentially, meaning that they take inputs one at a time, maintaining a state (memory) of the previous inputs to make predictions or decisions.\n",
    "- **Shared Parameters**: The weights used in an RNN are shared across time steps. This weight sharing helps the network generalize across sequences of different lengths.\n",
    "- **Hidden State**: RNNs have a hidden state that is updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "### Basic Architecture of an RNN:\n",
    "An RNN processes a sequence of inputs \\(\\{x_1, x_2, ..., x_T\\}\\), and at each time step \\(t\\), the network updates its hidden state \\(h_t\\) and generates an output \\(y_t\\). The process can be described by the following equations:\n",
    "\n",
    "1. **Hidden State Update**:\n",
    "   \\[\n",
    "   h_t = \\text{activation}(W_h h_{t-1} + W_x x_t + b)\n",
    "   \\]\n",
    "   Where:\n",
    "   - \\(h_t\\) is the hidden state at time step \\(t\\),\n",
    "   - \\(h_{t-1}\\) is the hidden state from the previous time step,\n",
    "   - \\(x_t\\) is the input at time step \\(t\\),\n",
    "   - \\(W_h\\) and \\(W_x\\) are weight matrices, and\n",
    "   - \\(b\\) is a bias term.\n",
    "   \n",
    "2. **Output Generation**:\n",
    "   \\[\n",
    "   y_t = \\text{activation}(W_y h_t + c)\n",
    "   \\]\n",
    "   Where:\n",
    "   - \\(y_t\\) is the output at time step \\(t\\),\n",
    "   - \\(W_y\\) is the output weight matrix,\n",
    "   - \\(c\\) is a bias term.\n",
    "\n",
    "### Backpropagation Through Time (BPTT):\n",
    "To train an RNN, we use **Backpropagation Through Time (BPTT)**, which is an extension of backpropagation for sequential data. In BPTT, we unfold the RNN across all time steps, treating it as a deep network, and compute the gradients for each time step. These gradients are then used to update the weights.\n",
    "\n",
    "The process involves:\n",
    "1. **Forward Pass**: Passing the input sequence through the network to compute the output at each time step.\n",
    "2. **Loss Calculation**: Computing the loss (e.g., mean squared error or cross-entropy loss) based on the predicted outputs.\n",
    "3. **Backward Pass**: Calculating the gradients of the loss with respect to the weights at each time step and propagating them back through the sequence to update the weights.\n",
    "\n",
    "### Challenges in Training RNNs:\n",
    "\n",
    "1. **Vanishing Gradient Problem**:\n",
    "   - The vanishing gradient problem occurs when gradients become extremely small as they are propagated backward through time. This leads to slow or no learning in the earlier time steps, especially in long sequences. This is because the gradients are multiplied by the same weights at each time step, and if the weights are small, the gradients shrink exponentially.\n",
    "   \n",
    "2. **Exploding Gradient Problem**:\n",
    "   - On the other hand, gradients can also become too large and cause the weights to update in an unstable manner. This can lead to the model diverging and failing to converge.\n",
    "\n",
    "### Variants of RNNs:\n",
    "\n",
    "To address the challenges in training traditional RNNs, several advanced architectures have been developed:\n",
    "\n",
    "1. **Long Short-Term Memory (LSTM)**:\n",
    "   - **LSTMs** are a type of RNN designed to combat the vanishing gradient problem by introducing **gates** that regulate the flow of information.\n",
    "   - LSTMs have three main gates:\n",
    "     - **Forget Gate**: Decides what information from the previous hidden state should be discarded.\n",
    "     - **Input Gate**: Controls how much of the new information should be stored in the memory cell.\n",
    "     - **Output Gate**: Determines how much of the memory cell should be passed to the next layer or output.\n",
    "   - The key feature of LSTMs is the **memory cell**, which can store information over long periods and be updated selectively, helping them retain long-term dependencies in sequences.\n",
    "\n",
    "2. **Gated Recurrent Unit (GRU)**:\n",
    "   - **GRUs** are a simplified version of LSTMs. They combine the forget and input gates into a single gate, and they don't have a separate memory cell. Instead, they use the hidden state directly.\n",
    "   - GRUs have fewer parameters than LSTMs, which can make them faster to train and less prone to overfitting in some cases.\n",
    "\n",
    "3. **Bidirectional RNNs**:\n",
    "   - A **Bidirectional RNN (BiRNN)** processes the input sequence in both forward and backward directions, allowing the network to use both past and future context to make predictions.\n",
    "   - This can be particularly useful in tasks like speech recognition and machine translation, where future context can provide valuable information.\n",
    "\n",
    "4. **Attention Mechanism**:\n",
    "   - The **Attention Mechanism** is often used in conjunction with RNNs (especially LSTMs and GRUs) to improve performance on tasks that require the model to focus on different parts of the input sequence.\n",
    "   - Instead of treating all time steps equally, attention mechanisms allow the model to weigh the importance of different time steps dynamically, enabling it to focus on the most relevant parts of the input.\n",
    "\n",
    "### Applications of RNNs:\n",
    "RNNs are particularly well-suited for tasks involving sequential data. Some common applications include:\n",
    "\n",
    "1. **Natural Language Processing (NLP)**:\n",
    "   - **Language modeling**: Predicting the next word in a sequence.\n",
    "   - **Machine translation**: Translating sentences from one language to another.\n",
    "   - **Sentiment analysis**: Analyzing the sentiment of a sentence or document.\n",
    "   - **Named Entity Recognition (NER)**: Identifying entities such as names, dates, and locations in text.\n",
    "\n",
    "2. **Speech Recognition**:\n",
    "   - Converting spoken language into text by processing audio sequences.\n",
    "\n",
    "3. **Time Series Prediction**:\n",
    "   - Predicting future values based on past data (e.g., stock prices, weather forecasting).\n",
    "\n",
    "4. **Video Analysis**:\n",
    "   - Recognizing actions or objects in video sequences by processing the frames over time.\n",
    "\n",
    "5. **Music Generation**:\n",
    "   - Generating new music by learning patterns from existing musical compositions.\n",
    "\n",
    "### Summary:\n",
    "- **RNNs** are powerful for processing sequential data because they can capture temporal dependencies by maintaining a hidden state across time steps.\n",
    "- Training RNNs can be challenging due to the vanishing and exploding gradient problems, but variants like **LSTMs** and **GRUs** have been developed to mitigate these issues.\n",
    "- RNNs are widely used in NLP, speech recognition, time series forecasting, and other domains where sequential information is important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# issues with RNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a powerful tool for handling sequential data, but they come with several challenges that can hinder their performance, particularly when dealing with long sequences. Here are the main issues with RNNs:\n",
    "\n",
    "### 1. **Vanishing Gradient Problem**\n",
    "   - **Description**: During backpropagation, gradients can become exceedingly small as they are propagated backward through many time steps. This is especially problematic for long sequences, as the gradients shrink exponentially with each time step.\n",
    "   - **Impact**: The vanishing gradients cause the weights of earlier time steps to receive minimal updates, making it difficult for the network to learn long-term dependencies in the data.\n",
    "   - **Example**: If you're training an RNN on a sentence with long-term dependencies, the network may fail to connect the first word to the last one.\n",
    "\n",
    "### 2. **Exploding Gradient Problem**\n",
    "   - **Description**: In contrast to vanishing gradients, the exploding gradient problem occurs when gradients grow exponentially during backpropagation, resulting in excessively large updates to the weights.\n",
    "   - **Impact**: This leads to instability in training, causing the model to diverge and fail to converge to a solution.\n",
    "   - **Example**: If the model’s weights become too large, the model may not be able to learn in a stable manner, resulting in poor performance or complete failure.\n",
    "\n",
    "### 3. **Difficulty in Learning Long-Term Dependencies**\n",
    "   - **Description**: RNNs struggle to learn long-term dependencies because of the vanishing gradient problem. When a sequence has long-term dependencies, the network is expected to remember information from far earlier in the sequence, but the gradient-based learning process has trouble preserving this information.\n",
    "   - **Impact**: RNNs tend to focus more on recent inputs, and long-term dependencies often get \"forgotten\" over time.\n",
    "   - **Example**: In tasks like language translation or sentiment analysis, RNNs may fail to capture relationships between distant words or phrases.\n",
    "\n",
    "### 4. **Slow Training and Computationally Expensive**\n",
    "   - **Description**: RNNs process one element of the sequence at a time, which makes them inherently slower to train compared to feedforward networks, where the entire batch of data can be processed simultaneously.\n",
    "   - **Impact**: Training RNNs can be computationally expensive, especially for long sequences, as the model must maintain a hidden state for each time step and propagate gradients back through time.\n",
    "   - **Example**: Training a language model on long paragraphs or a speech recognition model on long audio sequences can take a long time, especially on large datasets.\n",
    "\n",
    "### 5. **Difficulty with Parallelization**\n",
    "   - **Description**: RNNs process sequences one time step at a time, which limits the ability to parallelize the computations across time steps. This contrasts with feedforward neural networks, where all inputs can be processed simultaneously in parallel.\n",
    "   - **Impact**: Training RNNs on large datasets with long sequences is inefficient, as the computation for each time step depends on the previous one.\n",
    "   - **Example**: If you are training a neural machine translation model, each word in the sentence must be processed sequentially, making it hard to take advantage of parallel computation.\n",
    "\n",
    "### 6. **Sensitivity to Initial Conditions**\n",
    "   - **Description**: RNNs are highly sensitive to the initial weights and biases, as small changes in these parameters can significantly impact the network's performance.\n",
    "   - **Impact**: This sensitivity can lead to instability during training or cause the model to get stuck in local minima, making it hard to train effectively.\n",
    "   - **Example**: If the model starts with poorly initialized weights, it might fail to converge or take a long time to learn meaningful patterns.\n",
    "\n",
    "### 7. **Difficulty in Handling Very Long Sequences**\n",
    "   - **Description**: As the length of the sequence increases, RNNs struggle to maintain relevant information in their hidden states, especially in cases where dependencies span a long duration.\n",
    "   - **Impact**: RNNs may fail to capture critical information from earlier in the sequence, leading to poor performance on tasks requiring long-term memory.\n",
    "   - **Example**: For tasks like video captioning, where each frame might depend on the previous few frames, an RNN might lose track of earlier frames as the sequence length increases.\n",
    "\n",
    "### 8. **Limited Memory Capacity**\n",
    "   - **Description**: RNNs have limited memory capacity due to the way they store information in the hidden state. The hidden state vector typically has a fixed size, which can limit the amount of information it can store, especially for long sequences.\n",
    "   - **Impact**: As the sequence length increases, the hidden state might no longer be able to represent all the necessary information, causing important details to be lost.\n",
    "   - **Example**: In tasks like document classification, where the context from the entire document is needed, the fixed-size hidden state may not be able to capture all the important information.\n",
    "\n",
    "### 9. **Overfitting**\n",
    "   - **Description**: Like many other deep learning models, RNNs are prone to overfitting, especially when there is insufficient training data or when the model is too complex for the problem at hand.\n",
    "   - **Impact**: Overfitting can result in poor generalization to new, unseen data.\n",
    "   - **Example**: An RNN trained on a small dataset for sentiment analysis might memorize the training examples, leading to poor performance on real-world data.\n",
    "\n",
    "### 10. **Training Difficulty with Non-stationary Data**\n",
    "   - **Description**: RNNs are designed to learn patterns from sequential data, but they can struggle when the underlying data distribution changes over time, such as in cases of non-stationary data.\n",
    "   - **Impact**: The model might fail to adapt to changes in the data distribution, leading to suboptimal performance.\n",
    "   - **Example**: In time series forecasting, if the data exhibits sudden shifts (e.g., market crashes), an RNN may have trouble adapting to the new pattern.\n",
    "\n",
    "---\n",
    "\n",
    "### Solutions to RNN Issues:\n",
    "1. **Long Short-Term Memory (LSTM)** and **Gated Recurrent Units (GRU)**: Both LSTM and GRU architectures address the vanishing gradient problem by introducing gating mechanisms that regulate the flow of information and allow the model to capture long-term dependencies more effectively.\n",
    "2. **Gradient Clipping**: To prevent exploding gradients, gradient clipping can be applied to limit the size of gradients during training.\n",
    "3. **Bidirectional RNNs**: These RNNs process data in both forward and backward directions, which can help capture context from both the past and the future.\n",
    "4. **Attention Mechanisms**: Attention mechanisms allow the model to focus on different parts of the sequence at each time step, improving performance on tasks requiring the model to capture long-range dependencies.\n",
    "5. **Transformer Models**: The Transformer model, which replaces RNNs with self-attention mechanisms, has largely addressed many of the issues with RNNs, particularly the difficulty in handling long sequences and the inefficiency of sequential processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "While RNNs are powerful for sequential data tasks, they come with several challenges such as vanishing/exploding gradients, difficulty with long-term dependencies, and computational inefficiency. Advanced architectures like LSTMs, GRUs, and attention-based models like Transformers have been developed to overcome these limitations, making them more effective for tasks like machine translation, speech recognition, and time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### **Long Short-Term Memory (LSTM)**\n",
    "\n",
    "LSTM is a type of **Recurrent Neural Network (RNN)** architecture designed to address the **vanishing gradient problem** in standard RNNs, enabling the model to learn long-term dependencies in sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why LSTM?**\n",
    "- **Vanishing Gradient Problem**: Standard RNNs struggle to maintain information over long sequences because gradients diminish during backpropagation.\n",
    "- **Long-Term Dependencies**: LSTM introduces a memory cell that explicitly retains information over long periods, making it effective for tasks like time series prediction, language modeling, and sequence classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Components of LSTM**\n",
    "An LSTM unit consists of the following components:\n",
    "\n",
    "1. **Cell State (\\(C_t\\))**:\n",
    "   - Acts as the \"memory\" of the network.\n",
    "   - Information can flow through the cell state unchanged, subject to modifications by gates.\n",
    "\n",
    "2. **Hidden State (\\(h_t\\))**:\n",
    "   - The output of the LSTM at time \\(t\\), which is passed to the next time step.\n",
    "\n",
    "3. **Gates**:\n",
    "   Gates control the flow of information, enabling the LSTM to selectively remember or forget information:\n",
    "   - **Forget Gate (\\(f_t\\))**: Decides what information to discard from the cell state.\n",
    "   - **Input Gate (\\(i_t\\))**: Decides what new information to add to the cell state.\n",
    "   - **Output Gate (\\(o_t\\))**: Decides what part of the cell state to output as the hidden state.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Equations in LSTM**\n",
    "\n",
    "At each time step \\(t\\), the LSTM performs the following computations:\n",
    "\n",
    "1. **Forget Gate**:\n",
    "   \\[\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   \\]\n",
    "   - \\(W_f\\): Weight matrix for forget gate,\n",
    "   - \\(b_f\\): Bias for forget gate,\n",
    "   - \\(\\sigma\\): Sigmoid activation function.\n",
    "\n",
    "2. **Input Gate**:\n",
    "   - Compute the candidate values:\n",
    "     \\[\n",
    "     \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "     \\]\n",
    "   - Compute the input gate:\n",
    "     \\[\n",
    "     i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "     \\]\n",
    "\n",
    "3. **Update Cell State**:\n",
    "   \\[\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "   \\]\n",
    "   - \\(\\odot\\): Element-wise multiplication.\n",
    "\n",
    "4. **Output Gate**:\n",
    "   \\[\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   \\]\n",
    "\n",
    "5. **Hidden State**:\n",
    "   \\[\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Workflow**\n",
    "1. Input \\(x_t\\) and previous hidden state \\(h_{t-1}\\) are concatenated and fed into the gates.\n",
    "2. Gates control how much of the information is passed through.\n",
    "3. Cell state \\(C_t\\) is updated based on the forget and input gates.\n",
    "4. Hidden state \\(h_t\\) is derived from the output gate and the updated cell state.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. LSTM Diagram**\n",
    "```\n",
    "          Input (x_t)\n",
    "              |\n",
    "      +-------+-------+\n",
    "      |               |\n",
    "  Forget Gate      Input Gate\n",
    "      |               |\n",
    "      +-------+-------+\n",
    "              |\n",
    "          Cell State (C_t)\n",
    "              |\n",
    "         Output Gate\n",
    "              |\n",
    "         Hidden State (h_t)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Applications**\n",
    "- **Natural Language Processing (NLP)**: Text generation, sentiment analysis, and language translation.\n",
    "- **Time Series Analysis**: Stock price prediction, weather forecasting.\n",
    "- **Speech Processing**: Speech recognition, text-to-speech systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Python Implementation**\n",
    "\n",
    "Using TensorFlow/Keras:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generate synthetic sequential data\n",
    "X = np.random.random((1000, 10, 5))  # (samples, timesteps, features)\n",
    "y = np.random.random((1000, 1))\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(10, 5), return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Advantages of LSTM**\n",
    "- Handles long-term dependencies effectively.\n",
    "- Mitigates vanishing gradient issues.\n",
    "- Flexible for a wide range of sequence-based tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Limitations**\n",
    "- Computationally expensive due to the complexity of gates.\n",
    "- Prone to overfitting for small datasets.\n",
    "- Slower training compared to simpler architectures.\n",
    "\n",
    "Would you like a comparison with GRU, or further examples and visualizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) networks were designed to address many of the issues faced by traditional RNNs, particularly the vanishing gradient problem. However, they come with their own set of challenges and limitations:\n",
    "\n",
    "### 1. **Complexity and Computational Cost**\n",
    "   - **Description**: LSTM networks are more complex than standard RNNs because they introduce additional gates (input, forget, and output gates) that control the flow of information. This increases the number of parameters and operations required during both training and inference.\n",
    "   - **Impact**: The added complexity can result in higher computational costs and slower training times, particularly when dealing with large datasets or deep networks.\n",
    "   - **Example**: In tasks such as machine translation or speech recognition, LSTMs may require significantly more computational resources than simpler models, which can be a barrier when scaling to large datasets or real-time applications.\n",
    "\n",
    "### 2. **Difficulty with Very Long Sequences**\n",
    "   - **Description**: While LSTMs mitigate the vanishing gradient problem and can capture long-term dependencies better than vanilla RNNs, they are still limited when dealing with extremely long sequences.\n",
    "   - **Impact**: In practice, LSTMs may still struggle with sequences that are long enough for the network to forget important information, especially if the dependencies span many time steps.\n",
    "   - **Example**: For tasks such as document classification or long-form text generation, where dependencies might stretch over hundreds or thousands of words, LSTMs may still fail to capture all relevant information.\n",
    "\n",
    "### 3. **Overfitting**\n",
    "   - **Description**: LSTMs, like other deep learning models, are prone to overfitting, especially when there is a small amount of training data relative to the complexity of the model.\n",
    "   - **Impact**: If the LSTM model is too complex for the data, it can memorize the training examples, leading to poor generalization on unseen data.\n",
    "   - **Example**: In applications like time series forecasting, if the dataset is too small, the LSTM might overfit to the noise in the data rather than learning meaningful patterns.\n",
    "\n",
    "### 4. **Difficulty in Parallelization**\n",
    "   - **Description**: LSTMs process one time step at a time, which makes it difficult to fully parallelize computations. Each time step depends on the previous one, so LSTMs cannot take full advantage of modern parallel computing hardware like GPUs or TPUs.\n",
    "   - **Impact**: This sequential nature limits the efficiency of training and inference, especially when dealing with long sequences.\n",
    "   - **Example**: When training an LSTM on a large dataset of time series or video frames, the sequential nature of the model means that each time step must be processed in order, leading to slower training compared to models that can be fully parallelized.\n",
    "\n",
    "### 5. **Memory and Storage Requirements**\n",
    "   - **Description**: Due to the additional gates and internal state variables (cell state and hidden state), LSTMs require more memory and storage than simpler RNNs or other architectures.\n",
    "   - **Impact**: The increased memory requirements can be a limitation, particularly when deploying LSTMs on resource-constrained devices or when training large models with millions of parameters.\n",
    "   - **Example**: For applications like real-time speech recognition or edge-based inference on mobile devices, the memory overhead of LSTMs might make them impractical.\n",
    "\n",
    "### 6. **Optimization Challenges**\n",
    "   - **Description**: Despite being designed to mitigate the vanishing gradient problem, LSTMs can still face challenges in optimization, particularly when dealing with very deep or very large networks.\n",
    "   - **Impact**: Training LSTMs on large datasets can sometimes be slow or unstable, and finding optimal hyperparameters (e.g., learning rate, batch size) can be difficult.\n",
    "   - **Example**: When training on a very large dataset, such as millions of sequences, tuning the hyperparameters to avoid slow convergence or unstable training can be a significant challenge.\n",
    "\n",
    "### 7. **Lack of Long-Term Memory for Complex Dependencies**\n",
    "   - **Description**: While LSTMs are better at remembering long-term dependencies compared to vanilla RNNs, they still face challenges in maintaining complex, multi-step dependencies across very long sequences.\n",
    "   - **Impact**: For tasks that require understanding intricate relationships over long periods of time, LSTMs may still struggle to capture these dependencies.\n",
    "   - **Example**: In tasks like multi-turn dialogue generation, LSTMs may have difficulty keeping track of context across multiple interactions, leading to incoherent responses.\n",
    "\n",
    "### 8. **Training Instability**\n",
    "   - **Description**: LSTMs are still prone to certain types of training instability, especially when dealing with high learning rates, poor weight initialization, or inappropriate activation functions.\n",
    "   - **Impact**: This instability can lead to poor convergence, slow training, or the model failing to learn effectively.\n",
    "   - **Example**: If an LSTM model is trained with a learning rate that is too high, it might oscillate and fail to converge to an optimal solution.\n",
    "\n",
    "### 9. **Limited by Recurrent Structure**\n",
    "   - **Description**: While LSTMs are designed to handle sequential data, they still rely on the recurrent structure, which limits their ability to capture non-sequential dependencies or relationships that are not time-based.\n",
    "   - **Impact**: For tasks that do not involve sequences, or when the data is better represented in a non-sequential format, LSTMs may not be the most efficient model.\n",
    "   - **Example**: In tasks like image classification or non-sequential tabular data, LSTMs may not be as effective as models like CNNs or fully connected networks.\n",
    "\n",
    "### 10. **Hyperparameter Sensitivity**\n",
    "   - **Description**: LSTMs have several hyperparameters (e.g., number of layers, hidden units, dropout rates) that can significantly affect their performance.\n",
    "   - **Impact**: Finding the optimal combination of these hyperparameters can be a time-consuming process and may require significant experimentation.\n",
    "   - **Example**: In a sentiment analysis task, tuning the number of LSTM layers and the size of the hidden units could dramatically change the model's accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Solutions to Some LSTM Issues:\n",
    "1. **GRUs (Gated Recurrent Units)**: GRUs are a simplified version of LSTMs that can sometimes perform equally well while being computationally less expensive.\n",
    "2. **Attention Mechanisms**: Attention mechanisms, which are the basis for Transformer models, allow the model to focus on relevant parts of the sequence at each time step, addressing issues related to long-term dependencies and the sequential nature of LSTMs.\n",
    "3. **Transformer Models**: Transformers, which use self-attention rather than recurrence, have largely overcome many of the limitations of LSTMs, such as difficulty with very long sequences and inefficiency in training. They have become the go-to model for many sequential tasks.\n",
    "4. **Dropout and Regularization**: Techniques like dropout and L2 regularization can help mitigate overfitting, especially in large models.\n",
    "5. **Layer Normalization**: Applying normalization techniques like layer normalization can help improve the stability of training.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "While LSTMs are an improvement over traditional RNNs and have been widely successful in tasks like machine translation, speech recognition, and time series forecasting, they still face several challenges such as computational inefficiency, overfitting, and difficulty in handling very long sequences. Newer architectures like GRUs, attention mechanisms, and Transformers have been developed to address these issues and are increasingly being used in place of LSTMs for many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN vs LSTM**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are both designed to handle sequential data. However, they have key differences in how they manage long-term dependencies, which significantly impacts their performance on tasks involving long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Recurrent Neural Networks (RNN)**\n",
    "\n",
    "#### **Basic Concept**:\n",
    "- RNNs are neural networks designed to process sequences of data by maintaining a hidden state that gets updated at each time step.\n",
    "- At each time step \\( t \\), the network takes the current input \\( x_t \\) and the previous hidden state \\( h_{t-1} \\) to produce the current hidden state \\( h_t \\).\n",
    "\n",
    "#### **RNN Equations**:\n",
    "At each time step \\( t \\), an RNN computes the following:\n",
    "1. **Hidden State Update**:\n",
    "   \\[\n",
    "   h_t = \\sigma(W_h \\cdot x_t + U_h \\cdot h_{t-1} + b_h)\n",
    "   \\]\n",
    "   - \\(W_h\\): Weight matrix for input \\(x_t\\),\n",
    "   - \\(U_h\\): Weight matrix for the previous hidden state \\(h_{t-1}\\),\n",
    "   - \\(b_h\\): Bias term,\n",
    "   - \\(\\sigma\\): Activation function (typically \\(\\tanh\\) or \\(\\text{ReLU}\\)).\n",
    "\n",
    "2. **Output**:\n",
    "   \\[\n",
    "   y_t = W_y \\cdot h_t + b_y\n",
    "   \\]\n",
    "   - \\(W_y\\): Weight matrix for output,\n",
    "   - \\(b_y\\): Bias for output.\n",
    "\n",
    "#### **Drawbacks**:\n",
    "- **Vanishing Gradient Problem**: When training deep RNNs, gradients tend to become very small as they are backpropagated through many time steps, making it difficult to learn long-term dependencies.\n",
    "- **Limited Memory**: Standard RNNs struggle to remember information from earlier time steps, especially for long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Long Short-Term Memory (LSTM)**\n",
    "\n",
    "#### **Basic Concept**:\n",
    "- LSTMs are a specialized form of RNN designed to address the vanishing gradient problem by introducing a memory cell that can store information over long periods.\n",
    "- LSTM units contain **gates** that regulate the flow of information, enabling the model to learn long-term dependencies effectively.\n",
    "\n",
    "#### **LSTM Equations**:\n",
    "At each time step \\( t \\), an LSTM computes the following:\n",
    "1. **Forget Gate**:\n",
    "   \\[\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   \\]\n",
    "   - Decides what to forget from the previous memory.\n",
    "\n",
    "2. **Input Gate**:\n",
    "   \\[\n",
    "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "   \\]\n",
    "   - Decides what new information to store in the memory.\n",
    "\n",
    "3. **Cell State Update**:\n",
    "   \\[\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "   \\]\n",
    "   - Combines the previous memory \\( C_{t-1} \\) with new information.\n",
    "\n",
    "4. **Output Gate**:\n",
    "   \\[\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   \\]\n",
    "   - Controls what part of the memory should be output.\n",
    "\n",
    "5. **Hidden State**:\n",
    "   \\[\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   \\]\n",
    "   - The output of the LSTM at time step \\( t \\).\n",
    "\n",
    "#### **Advantages**:\n",
    "- **Memory Cells**: LSTMs have a memory cell that can maintain information over long periods, allowing them to capture long-term dependencies.\n",
    "- **Gates**: The forget, input, and output gates regulate the flow of information, enabling LSTMs to selectively remember or forget data.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Key Differences Between RNN and LSTM**\n",
    "\n",
    "| **Aspect**                  | **RNN**                              | **LSTM**                               |\n",
    "|-----------------------------|--------------------------------------|----------------------------------------|\n",
    "| **Memory**                   | Limited memory, struggles with long-term dependencies. | Can maintain long-term memory using a memory cell. |\n",
    "| **Vanishing Gradient**       | Prone to vanishing gradients during backpropagation. | Designed to mitigate the vanishing gradient problem. |\n",
    "| **Gates**                    | No gates, all information is processed similarly. | Uses forget, input, and output gates to control memory flow. |\n",
    "| **Complexity**               | Simpler architecture.               | More complex with multiple gates and memory cells. |\n",
    "| **Training Time**            | Faster to train due to simpler structure. | Slower to train due to additional complexity. |\n",
    "| **Performance on Long Sequences** | Struggles with long sequences.       | Excels at learning long-term dependencies. |\n",
    "| **Use Cases**                | Suitable for simpler sequential tasks. | Suitable for complex sequential tasks, like language modeling, speech recognition, and time series prediction. |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Use Cases**\n",
    "- **RNN**: Can be used for tasks with relatively short sequences, such as:\n",
    "  - Sentiment analysis (on short text),\n",
    "  - Simple sequence prediction.\n",
    "  \n",
    "- **LSTM**: More suited for tasks requiring learning from long sequences, such as:\n",
    "  - Speech recognition,\n",
    "  - Machine translation,\n",
    "  - Video analysis,\n",
    "  - Time series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Practical Example (Comparison)**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n",
    "\n",
    "# Example: Sequence data (samples, timesteps, features)\n",
    "X = np.random.random((1000, 10, 5))\n",
    "y = np.random.random((1000, 1))\n",
    "\n",
    "# RNN Model\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(10, 5)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, input_shape=(10, 5)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and fit models\n",
    "rnn_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "rnn_model.fit(X, y, epochs=10, batch_size=32)\n",
    "lstm_model.fit(X, y, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Conclusion**\n",
    "- **RNN**: Suitable for simpler tasks where long-term dependencies are not critical.\n",
    "- **LSTM**: The preferred choice for tasks that involve long-term dependencies due to its ability to store and regulate memory over time.\n",
    "\n",
    "If you're working on complex tasks such as natural language processing or time series forecasting, LSTMs are typically more effective than RNNs. Would you like to explore GRU (Gated Recurrent Unit), which is a simpler alternative to LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM VS TRANSFOERM \n",
    "\n",
    "\n",
    "When comparing **LSTMs (Long Short-Term Memory)** networks with **Transformers**, it's clear that while LSTMs were a significant advancement in handling sequential data, Transformers have largely surpassed them in many natural language processing (NLP) and machine learning tasks. Below are the **key issues with LSTMs** when compared to Transformers:\n",
    "\n",
    "### 1. **Sequential Nature vs. Parallelization**\n",
    "   - **LSTM**: LSTMs process input sequences one step at a time, meaning the output at each time step depends on the previous time step. This inherently makes them **sequential**, which **limits parallelization** during training. This sequential processing slows down training time and makes LSTMs less efficient on modern hardware like GPUs or TPUs.\n",
    "   - **Transformer**: In contrast, Transformers use **self-attention** mechanisms, allowing them to process all tokens in a sequence simultaneously. This enables **full parallelization**, drastically improving training speed and efficiency, especially for long sequences.\n",
    "\n",
    "   **Impact**: LSTMs are much slower to train and infer, especially for long sequences, whereas Transformers scale much better with large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Difficulty with Long-Range Dependencies**\n",
    "   - **LSTM**: While LSTMs were designed to mitigate the vanishing gradient problem and capture long-term dependencies, they still struggle with **very long-range dependencies**. The memory of LSTMs can degrade over time, especially for sequences that span hundreds or thousands of time steps.\n",
    "   - **Transformer**: Transformers excel at capturing **long-range dependencies** due to the self-attention mechanism. Each token can directly attend to every other token in the sequence, regardless of the distance, which helps capture global dependencies much more effectively.\n",
    "\n",
    "   **Impact**: For tasks requiring long-term memory, like document-level text understanding or long-form generation, Transformers significantly outperform LSTMs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Memory and Computational Efficiency**\n",
    "   - **LSTM**: LSTMs have a relatively large memory footprint due to the additional gates (input, forget, output gates) that need to be stored and updated during training. This makes LSTMs **memory-intensive** and harder to scale.\n",
    "   - **Transformer**: Although Transformers also require significant memory, their architecture is more **efficient** in handling large amounts of data due to the parallel processing capability. However, Transformer models like BERT or GPT can become computationally expensive for very large datasets.\n",
    "\n",
    "   **Impact**: While both LSTMs and Transformers require significant resources, Transformers are generally more efficient at utilizing parallel computation and memory, leading to better scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Training Stability**\n",
    "   - **LSTM**: LSTMs, although better than vanilla RNNs, can still face challenges with **training instability** and difficulties in convergence, especially with very deep or complex networks. Hyperparameter tuning, like choosing the right learning rate or batch size, can be difficult.\n",
    "   - **Transformer**: Transformers, especially with attention mechanisms, tend to be **more stable** during training, partly because they don't rely on recurrent steps, which can cause instability in gradient flow. The use of techniques like **Layer Normalization** and **Positional Encoding** further helps stabilize training.\n",
    "\n",
    "   **Impact**: Transformers often provide better training stability, especially in large-scale tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Difficulty with Extremely Long Sequences**\n",
    "   - **LSTM**: While LSTMs can handle sequences of varying lengths, they still face **difficulty** when sequences are extremely long, and the model needs to capture complex dependencies over a large span of time.\n",
    "   - **Transformer**: The Transformer’s **self-attention mechanism** is more effective at handling long sequences, but the quadratic complexity (O(n²)) of self-attention can become a bottleneck for very long sequences. However, techniques like **sparse attention** or **memory-augmented attention** (used in models like Longformer) help alleviate this issue.\n",
    "\n",
    "   **Impact**: LSTMs can struggle with very long sequences, while Transformers have better scalability but may need modifications for extremely long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Overfitting and Hyperparameter Sensitivity**\n",
    "   - **LSTM**: LSTMs are prone to **overfitting** on small datasets, especially if the model is too complex. Their performance is also sensitive to hyperparameters like the number of layers, hidden units, and dropout rates.\n",
    "   - **Transformer**: While Transformers can also overfit, they generally require **less tuning** of hyperparameters compared to LSTMs. Additionally, pre-trained Transformer models (like BERT, GPT) can be fine-tuned on specific tasks with much less data, which mitigates overfitting in many cases.\n",
    "\n",
    "   **Impact**: LSTMs require more careful tuning to avoid overfitting, while Transformers can be more robust, especially when pre-trained models are available.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Handling Variable-Length Sequences**\n",
    "   - **LSTM**: LSTMs process sequences in a **step-by-step manner**, making them naturally suited for tasks where sequence length can vary, such as time series forecasting or speech recognition. However, they still require padding or truncation when dealing with varying sequence lengths, which can affect model performance.\n",
    "   - **Transformer**: Transformers require **positional encoding** to handle sequence order, and while they are also capable of handling variable-length sequences, the padding operation (to standardize sequence length) can introduce inefficiencies, especially in the attention mechanism.\n",
    "\n",
    "   **Impact**: Both LSTMs and Transformers can handle variable-length sequences, but Transformers might require more careful handling of padding and positional encodings.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Interpretability**\n",
    "   - **LSTM**: LSTMs are considered **less interpretable** due to the complexity of their internal state, including the gates and cell states. It’s difficult to understand why a specific prediction was made.\n",
    "   - **Transformer**: While still complex, the **self-attention mechanism** in Transformers provides a clearer interpretation of how the model attends to different parts of the input sequence. This is useful for understanding which tokens are most important for a given prediction.\n",
    "\n",
    "   **Impact**: Transformers offer more interpretability through attention scores, which help in understanding the model's decision-making process.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Generalization to Non-Sequential Tasks**\n",
    "   - **LSTM**: LSTMs are designed primarily for **sequential data** (time series, text, etc.), and while they can be adapted for other tasks, they may not perform as well when the task doesn’t involve sequential dependencies.\n",
    "   - **Transformer**: Transformers are more **flexible** and have been successfully applied to both sequential tasks (e.g., NLP) and non-sequential tasks (e.g., image classification, multi-modal tasks). The architecture's adaptability has made it a go-to solution for a wide variety of problems beyond sequence modeling.\n",
    "\n",
    "   **Impact**: Transformers can generalize to a broader range of tasks, while LSTMs are more specialized for sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "While **LSTMs** were groundbreaking for sequence modeling, **Transformers** have largely surpassed them in terms of performance, scalability, and flexibility. The main advantages of Transformers include their ability to **parallelize** computations, **handle long-range dependencies** effectively, and be more **scalable** for large datasets. LSTMs, despite their improvements over vanilla RNNs, still face challenges related to **sequential processing**, **long-range dependencies**, and **training inefficiency**.\n",
    "\n",
    "For many modern NLP and sequence modeling tasks, **Transformers** (like BERT, GPT, and T5) have become the go-to architecture, while LSTMs are being gradually replaced or supplemented by Transformer-based models. However, LSTMs may still be useful in certain scenarios where the computational cost of Transformers is prohibitive or when dealing with smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### **Object Classification in Deep Learning: In-Depth Overview**\n",
    "\n",
    "Object classification is the task of assigning a label to an object in an image or video. Deep learning-based object classification typically uses convolutional neural networks (CNNs), which are specifically designed to handle grid-like data, such as images. The goal is to predict the class (category) of an object in an image, where the classes could be anything from animals, vehicles, and people to more specialized objects like medical images or satellite data.\n",
    "\n",
    "In this explanation, we will cover:\n",
    "- The core concepts behind object classification in deep learning.\n",
    "- A detailed look at how CNNs work for object classification.\n",
    "- Equations and architectures involved.\n",
    "- Training and evaluation strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Object Classification with Deep Learning: Overview**\n",
    "\n",
    "Deep learning models, particularly **Convolutional Neural Networks (CNNs)**, have revolutionized object classification tasks by automatically learning spatial hierarchies of features. Unlike traditional machine learning methods, deep learning models do not require manual feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "CNNs are composed of several types of layers that extract features from images and learn hierarchical representations of those features.\n",
    "\n",
    "#### **Key Layers in CNNs:**\n",
    "1. **Convolutional Layer**: Extracts local features by applying filters (kernels).\n",
    "2. **Activation Layer (ReLU)**: Introduces non-linearity to the model.\n",
    "3. **Pooling Layer**: Reduces spatial dimensions (downsampling).\n",
    "4. **Fully Connected Layer**: Maps learned features to class scores.\n",
    "5. **Softmax Layer**: Converts raw scores into class probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mathematical Formulation of CNN Layers**\n",
    "\n",
    "#### **a) Convolutional Layer**\n",
    "\n",
    "The convolutional operation applies a filter (kernel) to the input image to extract features. For a 2D image and a 3x3 filter, the convolution operation is as follows:\n",
    "\n",
    "\\[\n",
    "y(i,j) = (x * w)(i,j) = \\sum_m \\sum_n x(i+m, j+n) \\cdot w(m,n)\n",
    "\\]\n",
    "Where:\n",
    "- \\( x \\) is the input image,\n",
    "- \\( w \\) is the filter,\n",
    "- \\( y(i,j) \\) is the output feature map.\n",
    "\n",
    "The convolution operation is applied across the entire image to produce feature maps.\n",
    "\n",
    "#### **b) Activation Function (ReLU)**\n",
    "\n",
    "The activation function introduces non-linearity. The **Rectified Linear Unit (ReLU)** is commonly used:\n",
    "\n",
    "\\[\n",
    "f(x) = \\max(0, x)\n",
    "\\]\n",
    "\n",
    "This activation function outputs zero for negative values and passes positive values unchanged.\n",
    "\n",
    "#### **c) Pooling Layer**\n",
    "\n",
    "The pooling layer reduces the spatial size of the feature maps to reduce computation and overfitting. A common type is **Max Pooling**:\n",
    "\n",
    "\\[\n",
    "y(i,j) = \\max \\{x(i,j), x(i+1,j), x(i,j+1), x(i+1,j+1)\\}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y(i,j) \\) is the output after pooling,\n",
    "- \\( x(i,j) \\) is the input image or feature map.\n",
    "\n",
    "#### **d) Fully Connected Layer**\n",
    "\n",
    "After several convolutional and pooling layers, the output feature maps are flattened into a vector and passed through one or more fully connected layers:\n",
    "\n",
    "\\[\n",
    "z = W \\cdot x + b\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( W \\) is the weight matrix,\n",
    "- \\( x \\) is the input vector (flattened features),\n",
    "- \\( b \\) is the bias vector,\n",
    "- \\( z \\) is the output of the fully connected layer.\n",
    "\n",
    "#### **e) Softmax Activation for Classification**\n",
    "\n",
    "For multi-class classification, the final layer typically uses the **Softmax** function to output class probabilities. Softmax transforms the raw scores into a probability distribution:\n",
    "\n",
    "\\[\n",
    "P(y = c_i | x) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "\\]\n",
    "Where:\n",
    "- \\( P(y = c_i | x) \\) is the probability of class \\( c_i \\),\n",
    "- \\( z_i \\) is the raw score (logits) for class \\( c_i \\),\n",
    "- \\( C \\) is the total number of classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Object Classification Process Flow**\n",
    "\n",
    "The overall process of object classification involves the following steps:\n",
    "\n",
    "1. **Input Image**: A raw image is passed into the CNN.\n",
    "2. **Convolutional Layers**: Filters are applied to the image to extract features (e.g., edges, textures, patterns).\n",
    "3. **Activation Function**: Non-linear transformations are applied using ReLU.\n",
    "4. **Pooling Layers**: Spatial dimensions are reduced, keeping the important features.\n",
    "5. **Fully Connected Layers**: Flattened feature maps are passed through fully connected layers to learn high-level features.\n",
    "6. **Output Layer (Softmax)**: The final layer computes the class probabilities for each class.\n",
    "7. **Prediction**: The class with the highest probability is selected as the predicted class.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Training a CNN for Object Classification**\n",
    "\n",
    "Training a CNN involves optimizing the model's weights through backpropagation. The key steps include:\n",
    "\n",
    "#### **a) Loss Function**\n",
    "\n",
    "For classification tasks, **Cross-Entropy Loss** is typically used:\n",
    "\n",
    "\\[\n",
    "L = - \\sum_{i=1}^{C} y_i \\log(p_i)\n",
    "\\]\n",
    "Where:\n",
    "- \\( C \\) is the number of classes,\n",
    "- \\( y_i \\) is the ground truth (one-hot encoded vector),\n",
    "- \\( p_i \\) is the predicted probability for class \\( i \\).\n",
    "\n",
    "#### **b) Backpropagation**\n",
    "\n",
    "The loss function is minimized using gradient descent. The gradients of the loss with respect to the weights are computed using the chain rule and propagated backward through the network.\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\frac{\\partial L}{\\partial W} \\) is the gradient of the loss with respect to the weights,\n",
    "- \\( z \\) is the output of the fully connected layer.\n",
    "\n",
    "#### **c) Optimization**\n",
    "\n",
    "The weights are updated using an optimization algorithm such as **Stochastic Gradient Descent (SGD)** or **Adam**:\n",
    "\n",
    "\\[\n",
    "W = W - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\eta \\) is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Evaluation Metrics for Object Classification**\n",
    "\n",
    "To evaluate the performance of an object classification model, we typically use the following metrics:\n",
    "\n",
    "1. **Accuracy**:\n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "   \\]\n",
    "   \n",
    "2. **Precision, Recall, and F1-Score**:\n",
    "   - **Precision**: Measures how many of the predicted positive instances are actually positive.\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   \\]\n",
    "   - **Recall**: Measures how many of the actual positive instances were correctly predicted.\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   \\]\n",
    "   - **F1-Score**: The harmonic mean of precision and recall.\n",
    "   \\[\n",
    "   F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "   Where:\n",
    "   - \\( TP \\) = True Positives,\n",
    "   - \\( FP \\) = False Positives,\n",
    "   - \\( FN \\) = False Negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Python Example of Object Classification with CNN (Keras)**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Conclusion**\n",
    "\n",
    "Object classification in deep learning, primarily through CNNs, has become a powerful tool for various applications. The process involves multiple stages, from feature extraction through convolution to final classification using softmax. Understanding the equations and flow of these operations is essential for improving model performance and for tackling real-world classification tasks efficiently.\n",
    "\n",
    "Let me know if you'd like to explore any specific part in more detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### **ROC Curve (Receiver Operating Characteristic Curve)**\n",
    "\n",
    "The **ROC curve** is a graphical representation of the performance of a binary classification model as its decision threshold is varied. It is used to evaluate how well a model distinguishes between two classes (positive and negative). The ROC curve plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** for different threshold values.\n",
    "\n",
    "### **Key Definitions:**\n",
    "1. **True Positive (TP)**: The number of positive samples correctly classified as positive.\n",
    "2. **True Negative (TN)**: The number of negative samples correctly classified as negative.\n",
    "3. **False Positive (FP)**: The number of negative samples incorrectly classified as positive.\n",
    "4. **False Negative (FN)**: The number of positive samples incorrectly classified as negative.\n",
    "\n",
    "### **Important Metrics:**\n",
    "- **True Positive Rate (TPR)** or **Recall**:\n",
    "  \\[\n",
    "  \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  \\]\n",
    "  This measures the proportion of actual positives correctly identified by the model.\n",
    "\n",
    "- **False Positive Rate (FPR)**:\n",
    "  \\[\n",
    "  \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}\n",
    "  \\]\n",
    "  This measures the proportion of actual negatives incorrectly identified as positive by the model.\n",
    "\n",
    "### **How the ROC Curve is Plotted:**\n",
    "- The **x-axis** of the ROC curve represents the **False Positive Rate (FPR)**.\n",
    "- The **y-axis** represents the **True Positive Rate (TPR)**.\n",
    "- Each point on the ROC curve corresponds to a different decision threshold, which determines how the model classifies a sample as positive or negative.\n",
    "\n",
    "As the threshold is varied, the model will produce different combinations of **TPR** and **FPR**. At a very high threshold, the model will classify most samples as negative, resulting in a low **TPR** and a low **FPR**. As the threshold is lowered, the **TPR** increases, but the **FPR** may also increase, leading to a trade-off between sensitivity and specificity.\n",
    "\n",
    "### **Steps to Plot the ROC Curve:**\n",
    "1. **Generate the predicted probabilities** for each sample in the dataset.\n",
    "2. **Sort the predicted probabilities** in descending order.\n",
    "3. For each threshold (from 0 to 1), calculate the **TPR** and **FPR**.\n",
    "4. Plot **FPR** vs. **TPR** on the graph.\n",
    "\n",
    "### **Area Under the Curve (AUC):**\n",
    "- **AUC (Area Under the Curve)** is a metric that summarizes the performance of a classifier. It quantifies the entire 2D area underneath the ROC curve.\n",
    "- AUC ranges from 0 to 1:\n",
    "  - **AUC = 1**: Perfect model.\n",
    "  - **AUC = 0.5**: The model has no discrimination ability (equivalent to random guessing).\n",
    "  - **AUC < 0.5**: The model is worse than random guessing.\n",
    "\n",
    "### **Interpretation of the ROC Curve:**\n",
    "- The **closer the ROC curve is to the top-left corner**, the better the model is at distinguishing between the positive and negative classes. A curve closer to the diagonal (from bottom-left to top-right) indicates a poor classifier.\n",
    "- **Diagonal line (45-degree line)**: This line represents random guessing. Any model with an ROC curve above this line is better than random guessing.\n",
    "\n",
    "### **Example of ROC Curve:**\n",
    "\n",
    "If we have a classifier that predicts probabilities for the positive class, and the thresholds are adjusted to classify samples as positive or negative, the ROC curve might look something like this:\n",
    "\n",
    "- **High threshold**: The model is conservative, predicting positive only for very confident samples. **TPR** is low, and **FPR** is low.\n",
    "- **Low threshold**: The model becomes more liberal, predicting more positives, so **TPR** increases, but **FPR** might also increase.\n",
    "\n",
    "### **ROC Curve in Practice:**\n",
    "The ROC curve is especially useful when:\n",
    "- **Class distribution is imbalanced**: In cases where one class is much more frequent than the other, traditional accuracy metrics may be misleading, but the ROC curve and AUC give a better understanding of the model’s performance.\n",
    "- **Threshold tuning**: The ROC curve helps you to choose the optimal threshold for your model depending on whether you want to optimize for **sensitivity** (high **TPR**) or **specificity** (low **FPR**).\n",
    "\n",
    "### **Summary:**\n",
    "- **ROC Curve**: A plot of **TPR** vs **FPR** as the decision threshold is varied.\n",
    "- **AUC**: A scalar value that summarizes the performance of the model, with values closer to 1 indicating better performance.\n",
    "- The **ROC curve** helps in understanding the trade-off between **sensitivity** and **specificity** across different threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### **Image Segmentation in Deep Learning: In-depth Explanation**\n",
    "\n",
    "Image segmentation in deep learning refers to the task of dividing an image into multiple segments or regions, each of which represents a different object or part of the object. This is crucial in many applications such as medical imaging, autonomous driving, and object detection. Segmentation involves classifying each pixel of the image into one of the predefined classes.\n",
    "\n",
    "Here’s an in-depth breakdown of **image segmentation**, the equations, and how it works.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Overview of Image Segmentation**\n",
    "\n",
    "The goal of **image segmentation** is to assign a label to each pixel in an image. These labels represent the different objects or regions in the image.\n",
    "\n",
    "- **Semantic Segmentation**: Classifies each pixel as belonging to a particular class, e.g., road, car, building, etc.\n",
    "- **Instance Segmentation**: Differentiates between distinct objects of the same class, e.g., distinguishing two cars from each other.\n",
    "- **Panoptic Segmentation**: Combines semantic and instance segmentation, providing a complete view of both object categories and individual object instances.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Convolutional Neural Networks (CNNs) for Segmentation**\n",
    "\n",
    "CNNs are the backbone of most modern image segmentation techniques. Specifically, **Fully Convolutional Networks (FCNs)** have become popular for segmentation tasks because they can take input images of arbitrary size and output a segmentation map of the same size.\n",
    "\n",
    "#### **FCN Architecture for Image Segmentation**:\n",
    "\n",
    "An FCN is similar to a standard CNN but with a few key differences:\n",
    "- **No Fully Connected Layers**: In a traditional CNN, fully connected layers are used at the end of the network. FCNs replace these layers with convolutional layers that allow the network to output spatial maps.\n",
    "- **Upsampling/Deconvolution**: After the CNN processes the image, the feature maps are upsampled to match the input image's size using deconvolution (also known as transposed convolution).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Loss Function for Image Segmentation**\n",
    "\n",
    "The choice of loss function depends on the segmentation task (e.g., binary or multi-class segmentation). Common loss functions include **Cross-Entropy Loss**, **Dice Coefficient**, and **Intersection over Union (IoU)**.\n",
    "\n",
    "#### **Cross-Entropy Loss**:\n",
    "\n",
    "For **binary classification**, the cross-entropy loss is given by:\n",
    "\n",
    "\\[\n",
    "\\text{Loss}_{CE} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( n \\) is the number of pixels.\n",
    "- \\( y_i \\) is the ground truth label for pixel \\( i \\) (1 for object, 0 for background).\n",
    "- \\( \\hat{y}_i \\) is the predicted probability for pixel \\( i \\) being part of the object (output of the sigmoid function).\n",
    "\n",
    "For **multi-class classification**, the cross-entropy loss becomes:\n",
    "\n",
    "\\[\n",
    "\\text{Loss}_{CE} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_i^c \\log(\\hat{y}_i^c)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( C \\) is the number of classes.\n",
    "- \\( y_i^c \\) is the ground truth for pixel \\( i \\) for class \\( c \\).\n",
    "- \\( \\hat{y}_i^c \\) is the predicted probability for pixel \\( i \\) for class \\( c \\) (output of the softmax function).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Dice Coefficient (Dice Similarity Index)**\n",
    "\n",
    "The **Dice Coefficient** is a metric for evaluating the overlap between two sets, often used in image segmentation tasks. It is especially useful for **binary segmentation** tasks.\n",
    "\n",
    "\\[\n",
    "\\text{Dice} = \\frac{2 \\times \\text{Intersection}}{\\text{Union} + \\text{Intersection}} = \\frac{2 \\sum_{i=1}^{n} y_i \\hat{y}_i}{\\sum_{i=1}^{n} y_i + \\sum_{i=1}^{n} \\hat{y}_i}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the ground truth binary label for pixel \\( i \\).\n",
    "- \\( \\hat{y}_i \\) is the predicted binary label for pixel \\( i \\).\n",
    "\n",
    "The Dice coefficient ranges from 0 (no overlap) to 1 (perfect overlap).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Intersection over Union (IoU)**\n",
    "\n",
    "The **Intersection over Union (IoU)** is another metric commonly used to evaluate the performance of segmentation models.\n",
    "\n",
    "\\[\n",
    "\\text{IoU} = \\frac{\\text{Intersection of Ground Truth and Prediction}}{\\text{Union of Ground Truth and Prediction}} = \\frac{\\sum_{i=1}^{n} y_i \\hat{y}_i}{\\sum_{i=1}^{n} (y_i + \\hat{y}_i) - \\sum_{i=1}^{n} y_i \\hat{y}_i}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the ground truth label for pixel \\( i \\).\n",
    "- \\( \\hat{y}_i \\) is the predicted label for pixel \\( i \\).\n",
    "\n",
    "IoU is commonly used to evaluate the performance of **instance segmentation** tasks and typically measures how well the predicted segmentation overlaps with the ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. U-Net Architecture for Segmentation**\n",
    "\n",
    "The **U-Net** architecture is one of the most widely used models for image segmentation, especially in medical image segmentation. It is a type of FCN that consists of two parts:\n",
    "- **Encoder**: A typical CNN that extracts features from the image.\n",
    "- **Decoder**: Upsamples the features to match the original image size, using **skip connections** to combine low-level features with high-level features for more accurate segmentation.\n",
    "\n",
    "#### **U-Net Architecture Flow:**\n",
    "1. **Encoder**: Downsampling using convolution and pooling layers to extract high-level features.\n",
    "2. **Bottleneck**: The lowest level of the network, capturing the most abstract features.\n",
    "3. **Decoder**: Upsampling using transposed convolutions (deconvolutions), and combining features from the encoder via skip connections.\n",
    "4. **Output**: The final layer uses a softmax activation to predict the class for each pixel.\n",
    "\n",
    "The U-Net architecture has been successful in a variety of segmentation tasks due to its ability to retain fine-grained spatial information through skip connections.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Training the Model**\n",
    "\n",
    "The model is typically trained using **gradient-based optimization** methods, such as **Stochastic Gradient Descent (SGD)** or **Adam optimizer**, with backpropagation to update the weights.\n",
    "\n",
    "#### **Backpropagation for Image Segmentation**:\n",
    "1. **Forward Pass**: Pass the input image through the network to get the predicted segmentation map.\n",
    "2. **Loss Calculation**: Compute the loss function (e.g., Cross-Entropy Loss, Dice Coefficient).\n",
    "3. **Backward Pass**: Calculate the gradients of the loss with respect to the model parameters.\n",
    "4. **Parameter Update**: Use an optimization algorithm to update the model parameters (e.g., weights of convolutional layers).\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Common Architectures for Image Segmentation**\n",
    "\n",
    "- **FCN (Fully Convolutional Networks)**: A CNN architecture modified to output a segmentation map.\n",
    "- **U-Net**: Popular for medical image segmentation due to its skip connections.\n",
    "- **DeepLab**: Uses **Atrous Convolution** (dilated convolutions) for better feature extraction at different scales.\n",
    "- **Mask R-CNN**: Combines **Region Proposal Networks (RPNs)** for object detection with segmentation, making it ideal for instance segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Key Equations in Image Segmentation**\n",
    "\n",
    "1. **Cross-Entropy Loss (Binary):**\n",
    "\n",
    "\\[\n",
    "\\text{Loss}_{CE} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "\\]\n",
    "\n",
    "2. **Cross-Entropy Loss (Multi-Class):**\n",
    "\n",
    "\\[\n",
    "\\text{Loss}_{CE} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_i^c \\log(\\hat{y}_i^c)\n",
    "\\]\n",
    "\n",
    "3. **Dice Coefficient:**\n",
    "\n",
    "\\[\n",
    "\\text{Dice} = \\frac{2 \\times \\text{Intersection}}{\\text{Union} + \\text{Intersection}} = \\frac{2 \\sum_{i=1}^{n} y_i \\hat{y}_i}{\\sum_{i=1}^{n} y_i + \\sum_{i=1}^{n} \\hat{y}_i}\n",
    "\\]\n",
    "\n",
    "4. **Intersection over Union (IoU):**\n",
    "\n",
    "\\[\n",
    "\\text{IoU} = \\frac{\\sum_{i=1}^{n} y_i \\hat{y}_i}{\\sum_{i=1}^{n} (y_i + \\hat{y}_i) - \\sum_{i=1}^{n} y_i \\hat{y}_i}\n",
    "\\]\n",
    "\n",
    "These equations define how image segmentation models are trained and evaluated, providing the foundation for tasks like semantic, instance, and panoptic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### **Object Tracking in Deep Learning: In-Depth Overview**\n",
    "\n",
    "Object tracking is the process of locating a moving object over time across a sequence of frames in video data. In deep learning, object tracking typically leverages convolutional neural networks (CNNs), recurrent neural networks (RNNs), or a combination of both to track objects across frames. Object tracking is a key component in many applications such as autonomous vehicles, video surveillance, and augmented reality.\n",
    "\n",
    "There are two main types of object tracking:\n",
    "1. **Single Object Tracking (SOT)**: Tracking one object across a sequence of frames.\n",
    "2. **Multiple Object Tracking (MOT)**: Tracking multiple objects in the scene simultaneously.\n",
    "\n",
    "Deep learning-based tracking methods can be broadly categorized into:\n",
    "- **Tracking-by-Detection**: Objects are detected in each frame, and the tracking algorithm links the detections across frames.\n",
    "- **End-to-End Tracking**: The model directly learns to track objects over time without explicit detection.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Object Tracking Frameworks**\n",
    "\n",
    "#### **Tracking-by-Detection Approach**\n",
    "- **Step 1**: **Object Detection** in each frame.\n",
    "  - Use pre-trained object detection models like **YOLO**, **Faster R-CNN**, or **SSD** to detect objects in each frame.\n",
    "  \n",
    "- **Step 2**: **Data Association** between frames.\n",
    "  - Link detected objects across consecutive frames using various methods like **Kalman Filter**, **Hungarian Algorithm**, or **Optical Flow**.\n",
    "\n",
    "- **Step 3**: **Tracking**: Use the associations to track objects over time.\n",
    "\n",
    "#### **End-to-End Tracking Approach**\n",
    "- A deep learning model is trained to predict object trajectories over time. This approach is more complex but can learn to track objects directly from raw video data.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Object Tracking Algorithms**\n",
    "\n",
    "#### **a) Kalman Filter-based Tracking**\n",
    "- The Kalman filter is used to predict the object's position in the next frame based on its motion model (position, velocity, and acceleration). It's often combined with object detection methods to provide accurate tracking.\n",
    "\n",
    "**Equations for Kalman Filter**:\n",
    "- **Prediction Step**:\n",
    "  \\[\n",
    "  \\hat{x}_t^- = F \\cdot \\hat{x}_{t-1} + B \\cdot u_t\n",
    "  \\]\n",
    "  \\[\n",
    "  P_t^- = F \\cdot P_{t-1} \\cdot F^T + Q\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( \\hat{x}_t^- \\) is the predicted state (position and velocity),\n",
    "  - \\( F \\) is the state transition matrix,\n",
    "  - \\( B \\) is the control input matrix,\n",
    "  - \\( u_t \\) is the control vector,\n",
    "  - \\( P_t^- \\) is the predicted covariance matrix,\n",
    "  - \\( Q \\) is the process noise covariance matrix.\n",
    "\n",
    "- **Update Step**:\n",
    "  \\[\n",
    "  K_t = P_t^- \\cdot H^T \\cdot (H \\cdot P_t^- \\cdot H^T + R)^{-1}\n",
    "  \\]\n",
    "  \\[\n",
    "  \\hat{x}_t = \\hat{x}_t^- + K_t \\cdot (z_t - H \\cdot \\hat{x}_t^-)\n",
    "  \\]\n",
    "  \\[\n",
    "  P_t = (I - K_t \\cdot H) \\cdot P_t^-\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( K_t \\) is the Kalman gain,\n",
    "  - \\( H \\) is the observation matrix,\n",
    "  - \\( R \\) is the observation noise covariance,\n",
    "  - \\( z_t \\) is the measurement (detected object in the current frame),\n",
    "  - \\( I \\) is the identity matrix.\n",
    "\n",
    "---\n",
    "\n",
    "#### **b) Optical Flow-based Tracking**\n",
    "- **Optical Flow** computes the motion of objects by comparing pixel intensities between consecutive frames. It is used for tracking objects by estimating the velocity of pixel movements.\n",
    "\n",
    "**Optical Flow Equation**:\n",
    "\\[\n",
    "I_x u + I_y v + I_t = 0\n",
    "\\]\n",
    "Where:\n",
    "- \\( I_x \\) and \\( I_y \\) are the image gradients in the x and y directions,\n",
    "- \\( u \\) and \\( v \\) are the optical flow velocities in the x and y directions,\n",
    "- \\( I_t \\) is the time derivative of the image intensity.\n",
    "\n",
    "The optical flow method can track object movement without needing explicit detection but may be less robust to occlusions and lighting changes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **c) Siamese Network-based Tracking (Deep Learning Approach)**\n",
    "- **Siamese Networks** are often used for object tracking in deep learning. These networks learn to compare image patches and predict whether they correspond to the same object.\n",
    "\n",
    "- **Siamese Network Architecture**:\n",
    "  - A **Siamese Network** consists of two identical sub-networks that share weights and take two inputs: a reference image (previous frame) and a target image (current frame).\n",
    "  - The network computes a similarity score between the two images.\n",
    "\n",
    "**Loss Function** for Siamese Network:\n",
    "\\[\n",
    "L(x_1, x_2, y) = \\frac{1}{2} \\cdot y \\cdot D(x_1, x_2)^2 + \\frac{1}{2} \\cdot (1 - y) \\cdot \\max(0, m - D(x_1, x_2))^2\n",
    "\\]\n",
    "Where:\n",
    "- \\( x_1, x_2 \\) are the feature vectors of the reference and target images,\n",
    "- \\( D(x_1, x_2) \\) is the Euclidean distance between the feature vectors,\n",
    "- \\( y \\) is the label (1 if the images are from the same object, 0 otherwise),\n",
    "- \\( m \\) is the margin (used for negative pairs).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Deep Learning-based Tracking Frameworks**\n",
    "\n",
    "#### **a) Tracking-Learning-Detection (TLD)**\n",
    "- TLD is a framework that combines **tracking**, **learning**, and **detection**:\n",
    "  - **Tracking**: Predicts the object's movement based on previous frames.\n",
    "  - **Learning**: Updates the object model to adapt to changes in appearance.\n",
    "  - **Detection**: Detects the object in each frame.\n",
    "\n",
    "#### **b) DeepSORT (Deep Learning-based SORT)**\n",
    "- **SORT** (Simple Online and Realtime Tracking) is an efficient tracking algorithm based on Kalman filtering and the Hungarian algorithm for data association.\n",
    "- **DeepSORT** extends SORT by incorporating deep learning features to improve object re-identification, making it more robust to occlusions and appearance changes.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Applications of Object Tracking**\n",
    "- **Autonomous Vehicles**: Tracking pedestrians, vehicles, and other obstacles in real-time.\n",
    "- **Surveillance**: Monitoring people and objects in security camera feeds.\n",
    "- **Sports Analytics**: Tracking players and ball movement during a game.\n",
    "- **Human-Computer Interaction**: Gesture tracking and interactive systems.\n",
    "- **Robotics**: Object tracking for robotic manipulation and navigation.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Python Example Using DeepSORT**\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "from deep_sort import DeepSort\n",
    "\n",
    "# Initialize DeepSORT and object detection model\n",
    "tracker = DeepSort()\n",
    "\n",
    "# Load video and object detection model (YOLO, SSD, etc.)\n",
    "video = cv2.VideoCapture('video.mp4')\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Step 1: Perform object detection (YOLO/SSD/Faster R-CNN)\n",
    "    detections = detect_objects(frame)\n",
    "    \n",
    "    # Step 2: Track objects using DeepSORT\n",
    "    tracker.update(detections)\n",
    "    \n",
    "    # Step 3: Draw bounding boxes and IDs\n",
    "    for track in tracker.tracks:\n",
    "        if track.is_confirmed():\n",
    "            x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, str(track.track_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Object Tracking', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Conclusion**\n",
    "Object tracking is essential for various real-world applications. While traditional methods like Kalman filtering and optical flow are effective in many scenarios, deep learning approaches such as Siamese Networks and DeepSORT offer superior performance, especially for complex tracking tasks in dynamic environments.\n",
    "\n",
    "If you're interested, I can dive deeper into any of the specific algorithms or provide more implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In machine learning, **bias** and **variance** are two fundamental sources of error that influence the performance of a model. Understanding the trade-off between them is crucial for building models that generalize well to unseen data.\n",
    "\n",
    "### **Bias**\n",
    "Bias refers to the error introduced by approximating a real-world problem with a simplified model. A model with **high bias** makes strong assumptions about the data and typically oversimplifies the underlying patterns, leading to **underfitting**. In other words, it fails to capture the complexity of the data.\n",
    "\n",
    "- **High Bias**: The model makes assumptions that are too simple, leading to systematic errors. This happens when the model is too simple (e.g., a linear model for non-linear data).\n",
    "- **Low Bias**: The model is more flexible and can capture the complexities of the data, leading to a better fit.\n",
    "\n",
    "**Impact**:\n",
    "- High bias leads to poor performance on both training and test data.\n",
    "- The model fails to learn from the data and misses important patterns.\n",
    "\n",
    "### **Variance**\n",
    "Variance refers to the model’s sensitivity to small fluctuations in the training data. A model with **high variance** is overly complex and learns not only the underlying patterns but also the noise in the training data, leading to **overfitting**. This means the model performs well on the training data but fails to generalize to unseen data.\n",
    "\n",
    "- **High Variance**: The model is too complex, capturing noise and random fluctuations in the training data. It can perform very well on the training set but poorly on the test set.\n",
    "- **Low Variance**: The model is less sensitive to fluctuations in the training data, leading to a more stable and generalizable model.\n",
    "\n",
    "**Impact**:\n",
    "- High variance leads to excellent performance on training data but poor performance on test data.\n",
    "- The model overfits, meaning it’s too tailored to the training data and fails to generalize.\n",
    "\n",
    "### **Bias-Variance Trade-off**\n",
    "There is an inherent **trade-off** between bias and variance:\n",
    "\n",
    "- **High Bias and Low Variance**: The model is too simple, and its predictions are consistent but inaccurate (underfitting).\n",
    "- **Low Bias and High Variance**: The model is too complex, and it fits the training data well but generalizes poorly to new data (overfitting).\n",
    "- **Ideal Scenario**: A model with **low bias** and **low variance** is ideal, meaning it captures the underlying patterns of the data while being stable enough to generalize to unseen data.\n",
    "\n",
    "### **Visualizing the Trade-off**\n",
    "- **Underfitting**: Occurs when the model has high bias and low variance. The model is too simple and cannot capture the complexity of the data, leading to poor performance.\n",
    "- **Overfitting**: Occurs when the model has low bias and high variance. The model fits the training data too well, capturing both the signal and the noise, resulting in poor generalization.\n",
    "\n",
    "### **Strategies to Manage Bias and Variance**\n",
    "1. **High Bias (Underfitting)**:\n",
    "   - Use a more complex model (e.g., moving from linear regression to decision trees or neural networks).\n",
    "   - Add more features to the model or use polynomial features.\n",
    "   - Decrease regularization strength if too much is applied.\n",
    "\n",
    "2. **High Variance (Overfitting)**:\n",
    "   - Use simpler models (e.g., reduce the number of parameters or layers in a neural network).\n",
    "   - Increase the amount of training data.\n",
    "   - Apply regularization techniques like **L2 regularization (Ridge)** or **L1 regularization (Lasso)**.\n",
    "   - Use **cross-validation** to evaluate model performance and prevent overfitting.\n",
    "   - Apply **early stopping** in the case of neural networks.\n",
    "\n",
    "### **Summary**\n",
    "- **Bias**: Error due to overly simplistic models (underfitting).\n",
    "- **Variance**: Error due to overly complex models (overfitting).\n",
    "- The goal is to find a balance where both bias and variance are minimized to achieve **good generalization** on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# err(x) = bias**2 + variance = irereduciable error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The equation you provided, \\( \\text{err}(x) = \\text{bias}^2 + \\text{variance} \\), is related to the **bias-variance decomposition** of the **mean squared error (MSE)** in machine learning, particularly in regression tasks. However, it is important to note that this equation doesn't directly describe the \"irreducible error,\" but rather the **total error** in a model.\n",
    "\n",
    "Let's break it down in detail:\n",
    "\n",
    "### **1. Mean Squared Error (MSE) Decomposition**\n",
    "\n",
    "In supervised learning, the total error in a model can be decomposed into three components:\n",
    "- **Bias**: The error due to overly simplistic assumptions in the model.\n",
    "- **Variance**: The error due to the model's sensitivity to fluctuations in the training data.\n",
    "- **Irreducible Error**: The inherent noise in the data that cannot be reduced by any model.\n",
    "\n",
    "#### **MSE Decomposition:**\n",
    "\\[\n",
    "\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "\\]\n",
    "\n",
    "- **Bias**: Measures how much on average the predicted values deviate from the true values. A high bias means the model is too simple and makes strong assumptions (underfitting).\n",
    "  \n",
    "- **Variance**: Measures how much the model’s predictions vary for different training sets. A high variance means the model is very sensitive to small changes in the training data (overfitting).\n",
    "  \n",
    "- **Irreducible Error**: This is the noise or random variation in the data that cannot be predicted by any model. It is the error that is outside of the model’s control, and thus cannot be reduced through training.\n",
    "\n",
    "### **2. Explanation of Each Component:**\n",
    "\n",
    "- **Bias** (\\( \\text{Bias} \\)): The difference between the expected prediction of the model and the true values. If the model is too simple (e.g., linear regression on non-linear data), the bias is high.\n",
    "  \n",
    "- **Variance** (\\( \\text{Variance} \\)): The variability of the model's predictions for different training datasets. High variance occurs when the model is too complex and overfits the data.\n",
    "\n",
    "- **Irreducible Error** (\\( \\epsilon \\)): This is the error that comes from the data itself. It includes things like noise, measurement errors, and unmodeled variables. It is the minimum error that any model can achieve for a given problem.\n",
    "\n",
    "### **3. Irreducible Error:**\n",
    "The irreducible error, denoted as \\( \\epsilon \\), is often represented as part of the error that is independent of the model. It is caused by factors that the model cannot control, such as:\n",
    "- Randomness in the data.\n",
    "- Inherent uncertainty in the process that generated the data.\n",
    "\n",
    "In practical terms, the irreducible error is often viewed as a constant, and it sets a lower bound on the performance of any model.\n",
    "\n",
    "### **4. Total Error Equation:**\n",
    "The total error can be viewed as:\n",
    "\\[\n",
    "\\text{err}(x) = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "\\]\n",
    "\n",
    "This means that even the best possible model cannot achieve perfect accuracy, as there will always be some irreducible error in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary:**\n",
    "- The equation \\( \\text{err}(x) = \\text{Bias}^2 + \\text{Variance} \\) represents the total error from bias and variance, but **irreducible error** is an additional component that is independent of the model.\n",
    "- **Bias** and **variance** can be controlled to some extent by tuning the model (e.g., regularization, complexity of the model), but the **irreducible error** cannot be reduced by improving the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The statement that **ReLU (Rectified Linear Unit)** is not differentiable is **partially correct** but requires clarification. Let's break it down:\n",
    "\n",
    "### **ReLU Function:**\n",
    "The **ReLU** activation function is defined as:\n",
    "\\[\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\]\n",
    "This means:\n",
    "- For any positive input, the output is equal to the input.\n",
    "- For any negative input, the output is zero.\n",
    "\n",
    "### **Differentiability of ReLU:**\n",
    "The ReLU function is **differentiable almost everywhere** except at **\\(x = 0\\)**.\n",
    "\n",
    "- **For \\(x > 0\\)**: The function behaves like \\( f(x) = x \\), which is differentiable, and its derivative is 1.\n",
    "- **For \\(x < 0\\)**: The function behaves like \\( f(x) = 0 \\), which is also differentiable, and its derivative is 0.\n",
    "- **At \\(x = 0\\)**: The function is **not differentiable** because there is a discontinuity in the slope. The left-hand derivative is 0, and the right-hand derivative is 1. Since the derivatives do not match at this point, ReLU is **not differentiable at \\(x = 0\\)**.\n",
    "\n",
    "### **Why is ReLU not differentiable at \\(x = 0\\)?**\n",
    "At \\(x = 0\\), the ReLU function has a sharp corner. A function is differentiable at a point if the slope of the tangent is well-defined at that point. In the case of ReLU, the function changes from a flat line (slope = 0) to a line with a slope of 1, which causes the derivative to be undefined at \\(x = 0\\).\n",
    "\n",
    "### **In Practice:**\n",
    "In deep learning and neural networks, **this non-differentiability at \\(x = 0\\)** is usually not an issue. During training, optimization algorithms like **gradient descent** do not often encounter the exact value of \\(x = 0\\), because most of the input values are not exactly zero. Additionally, the **subgradient** method is often used at points of non-differentiability.\n",
    "\n",
    "For most practical purposes:\n",
    "- The ReLU function is considered **differentiable almost everywhere** except at \\(x = 0\\).\n",
    "- In implementations, if the input is exactly zero, the derivative can be treated as **0** or **1**, or the system can rely on subgradients.\n",
    "\n",
    "### **Summary:**\n",
    "- **ReLU is differentiable almost everywhere**, except at \\(x = 0\\).\n",
    "- At \\(x = 0\\), it is not differentiable because the function has a corner, and the left-hand and right-hand derivatives do not match.\n",
    "- In practice, this is not usually a problem for training deep networks, and subgradients are often used for optimization at non-differentiable points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-differentiability of the ReLU function at \\( x = 0 \\) is typically handled in deep learning through a few practical solutions:\n",
    "\n",
    "### 1. **Subgradient Approach**:\n",
    "Since ReLU is not differentiable at \\( x = 0 \\), one way to deal with this is by using a **subgradient**. A subgradient is a generalization of the derivative concept for non-differentiable points. For ReLU, the subgradient at \\( x = 0 \\) can be any value between the left-hand derivative (0) and the right-hand derivative (1). In practice, during backpropagation:\n",
    "- The subgradient is typically set to **0** or **1** when the input is exactly zero.\n",
    "- The choice of the subgradient value at \\( x = 0 \\) doesn't significantly affect the learning process because it occurs so rarely, and in most cases, the value of \\( x \\) will not be exactly zero.\n",
    "\n",
    "### 2. **Leaky ReLU**:\n",
    "A more commonly used solution is to use **Leaky ReLU**. Leaky ReLU modifies the original ReLU function to allow a small negative slope for values less than zero, preventing a sharp corner at \\( x = 0 \\).\n",
    "\\[\n",
    "\\text{Leaky ReLU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "Here, \\( \\alpha \\) is a small constant (e.g., \\( \\alpha = 0.01 \\)). This small slope ensures that the function is differentiable everywhere, including at \\( x = 0 \\), and helps mitigate the problem of \"dying neurons\" where neurons never activate (due to ReLU's flat region for negative values).\n",
    "\n",
    "### 3. **Parametric ReLU (PReLU)**:\n",
    "An extension of Leaky ReLU is **Parametric ReLU (PReLU)**, where the slope for negative values is learned as a parameter during training.\n",
    "\\[\n",
    "\\text{PReLU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "Here, \\( \\alpha \\) is a parameter that is learned during training. This allows the model to learn the best slope for the negative side, which can lead to better performance in some cases.\n",
    "\n",
    "### 4. **Softmax/Swish Activation Functions**:\n",
    "- **Swish** is a newer activation function defined as:\n",
    "  \\[\n",
    "  \\text{Swish}(x) = \\frac{x}{1 + e^{-x}}\n",
    "  \\]\n",
    "  Swish is differentiable everywhere, including at \\( x = 0 \\), and it can perform better than ReLU in certain situations.\n",
    "- **Softmax** can also be used in some cases where you're working with multi-class classification tasks, but it's typically not a direct replacement for ReLU.\n",
    "\n",
    "### 5. **Randomized ReLU (RReLU)**:\n",
    "Another variation is **Randomized ReLU (RReLU)**, where a random slope is used for negative inputs during training and the slope is fixed during testing:\n",
    "\\[\n",
    "\\text{RReLU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "Here, \\( \\alpha \\) is sampled randomly from a uniform distribution between some bounds (e.g., \\( \\alpha \\in [0, 0.1] \\)). This introduces some randomness during training, which can help with regularization.\n",
    "\n",
    "### 6. **Other Smooth Activation Functions**:\n",
    "Some alternatives to ReLU that are differentiable everywhere include:\n",
    "- **ELU (Exponential Linear Unit)**:\n",
    "  \\[\n",
    "  \\text{ELU}(x) = \\begin{cases}\n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "  ELU has a smooth transition for negative values, which makes it differentiable everywhere and can avoid the problem of dead neurons.\n",
    "  \n",
    "- **SELU (Scaled Exponential Linear Unit)**: \n",
    "  A variant of ELU, which automatically scales its output for each layer, providing a self-normalizing property that can help with training deep networks.\n",
    "\n",
    "### **Summary of Solutions**:\n",
    "1. **Subgradients**: Use 0 or 1 at \\( x = 0 \\) during backpropagation.\n",
    "2. **Leaky ReLU**: Adds a small slope for negative inputs, ensuring differentiability.\n",
    "3. **PReLU**: Allows the negative slope to be learned as a parameter.\n",
    "4. **Swish**: A smooth activation function that avoids sharp corners.\n",
    "5. **RReLU**: Introduces randomness in the negative slope during training.\n",
    "6. **ELU/SELU**: Smooth activation functions that are differentiable everywhere.\n",
    "\n",
    "Each of these solutions can address the non-differentiability issue, and the choice depends on the specific use case and the performance characteristics you desire in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### **U-Net**: \n",
    "\n",
    "U-Net is a **convolutional neural network (CNN)** architecture that was originally designed for **semantic image segmentation** tasks, particularly for biomedical image segmentation. The architecture has proven highly effective in various domains, especially when there is a limited amount of labeled data.\n",
    "\n",
    "### **Key Features of U-Net**:\n",
    "1. **Encoder-Decoder Structure**: U-Net follows an **encoder-decoder** architecture, where the encoder extracts features, and the decoder reconstructs the image or its segmentation map. \n",
    "2. **Skip Connections**: One of the main innovations in U-Net is the use of **skip connections**, which connect the corresponding layers of the encoder and decoder to retain spatial information that may be lost during downsampling.\n",
    "3. **Symmetry**: The architecture is symmetric, with the number of feature maps increasing as we go deeper into the encoder and decreasing symmetrically in the decoder.\n",
    "4. **Pixel-wise Output**: U-Net is designed to produce pixel-wise classification, meaning each pixel in the output corresponds to a class label (e.g., foreground or background).\n",
    "\n",
    "### **Architecture Overview**:\n",
    "\n",
    "The U-Net architecture consists of two parts:\n",
    "1. **Contracting Path (Encoder)**: \n",
    "   - The encoder extracts high-level features using convolutional layers followed by max-pooling layers.\n",
    "   - Each step in the encoder typically doubles the number of feature channels and reduces the spatial dimensions of the feature map.\n",
    "   \n",
    "2. **Bottleneck**:\n",
    "   - The bottleneck is the deepest part of the network where the feature maps are most compressed. It typically contains several convolutional layers.\n",
    "\n",
    "3. **Expansive Path (Decoder)**: \n",
    "   - The decoder progressively upsamples the feature maps to the original input size.\n",
    "   - The upsampling is followed by a convolutional layer to refine the features.\n",
    "   - **Skip connections** are used here to concatenate the feature maps from the encoder with the corresponding feature maps in the decoder, helping the decoder retain fine-grained spatial information.\n",
    "\n",
    "### **Detailed U-Net Architecture**:\n",
    "\n",
    "1. **Encoder (Contracting Path)**:\n",
    "   - Consists of repeated **blocks** that each contain:\n",
    "     - A **convolutional layer** (usually 3x3) followed by a **ReLU activation**.\n",
    "     - A **max-pooling layer** (usually 2x2) to reduce spatial dimensions and increase receptive field.\n",
    "   - Each block increases the number of feature channels.\n",
    "\n",
    "2. **Bottleneck**:\n",
    "   - This part contains several convolutional layers without any pooling, making it the deepest part of the network. \n",
    "   - The bottleneck captures the most abstract and high-level features of the image.\n",
    "\n",
    "3. **Decoder (Expansive Path)**:\n",
    "   - The decoder consists of **up-sampling** (transposed convolution or interpolation) followed by **convolution**.\n",
    "   - The **skip connections** from the encoder are concatenated with the corresponding feature maps in the decoder, allowing the model to learn more detailed information from earlier layers.\n",
    "\n",
    "4. **Output Layer**:\n",
    "   - The final output layer is a **1x1 convolution** that reduces the feature maps to the number of classes (for segmentation, this is typically the number of object categories or 1 for binary segmentation).\n",
    "   - A **softmax activation** is usually applied to get pixel-wise class probabilities.\n",
    "\n",
    "### **U-Net Equations**:\n",
    "For each layer \\(i\\) in the contracting path (encoder):\n",
    "\\[\n",
    "x_i = \\text{Conv}(x_{i-1}) \\quad \\text{(Convolution Layer)}\n",
    "\\]\n",
    "\\[\n",
    "x_i = \\text{ReLU}(x_i)\n",
    "\\]\n",
    "\\[\n",
    "x_i = \\text{MaxPooling}(x_i)\n",
    "\\]\n",
    "Where:\n",
    "- \\(x_{i-1}\\) is the input to the current layer, and\n",
    "- \\(\\text{Conv}\\), \\(\\text{ReLU}\\), and \\(\\text{MaxPooling}\\) are standard operations in the encoder.\n",
    "\n",
    "In the expansive path (decoder), we have:\n",
    "\\[\n",
    "x_i = \\text{UpSampling}(x_{i-1})\n",
    "\\]\n",
    "\\[\n",
    "x_i = \\text{Conv}(x_i)\n",
    "\\]\n",
    "Where:\n",
    "- \\(x_i\\) is the feature map after upsampling and convolution, and\n",
    "- \\(\\text{UpSampling}\\) can be transposed convolution (also called deconvolution).\n",
    "\n",
    "For skip connections, we concatenate the feature maps from the encoder to the decoder at corresponding levels:\n",
    "\\[\n",
    "x_{\\text{skip}} = \\text{Concat}(x_{\\text{encoder}}, x_{\\text{decoder}})\n",
    "\\]\n",
    "This helps the decoder reconstruct finer spatial details that might be lost in the encoding process.\n",
    "\n",
    "### **Loss Function**:\n",
    "In semantic segmentation, the loss function is usually the **cross-entropy loss** (for classification tasks), or **dice coefficient loss** (for overlapping object segmentation tasks):\n",
    "- **Cross-Entropy Loss**:\n",
    "\\[\n",
    "\\text{Loss} = - \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "\\]\n",
    "Where:\n",
    "- \\(y_i\\) is the true label, and\n",
    "- \\(p_i\\) is the predicted probability for class 1 (or the object of interest).\n",
    "\n",
    "- **Dice Loss** (for overlap-sensitive tasks):\n",
    "\\[\n",
    "\\text{Dice Loss} = 1 - \\frac{2 |A \\cap B|}{|A| + |B|}\n",
    "\\]\n",
    "Where:\n",
    "- \\(A\\) is the set of predicted pixels, and\n",
    "- \\(B\\) is the set of ground truth pixels.\n",
    "\n",
    "### **U-Net Variants**:\n",
    "1. **U-Net++**: A variation of U-Net that adds additional skip pathways to improve the information flow between encoder and decoder, leading to better performance.\n",
    "2. **3D U-Net**: An extension of U-Net that handles 3D data (e.g., volumetric medical images) by replacing 2D convolutions with 3D convolutions.\n",
    "3. **Attention U-Net**: Introduces attention mechanisms to U-Net, allowing the model to focus on the relevant regions of the image, which is useful for complex segmentation tasks.\n",
    "\n",
    "### **Applications of U-Net**:\n",
    "- **Biomedical Image Segmentation**: U-Net was initially designed for **cell segmentation** and other biomedical tasks like tumor segmentation.\n",
    "- **Satellite Image Segmentation**: U-Net has been used for tasks such as identifying roads, buildings, and other objects in satellite imagery.\n",
    "- **Autonomous Driving**: U-Net can be used to segment road scenes, vehicles, and pedestrians in autonomous driving systems.\n",
    "- **General Semantic Segmentation**: U-Net is also used for general image segmentation tasks, where the goal is to classify each pixel into a predefined class.\n",
    "\n",
    "### **Advantages of U-Net**:\n",
    "1. **Effective with limited data**: U-Net can work well with relatively small datasets, especially when combined with data augmentation techniques.\n",
    "2. **High accuracy**: Due to the skip connections, U-Net can retain fine spatial details, leading to better segmentation performance.\n",
    "3. **Flexibility**: It can be applied to both binary and multi-class segmentation problems.\n",
    "\n",
    "### **Challenges**:\n",
    "- **Memory and Computational Load**: The skip connections and deep architecture can lead to high memory usage, especially with large input images or 3D data.\n",
    "- **Overfitting**: U-Net can overfit on small datasets, though this can be mitigated by using techniques like data augmentation, dropout, or using smaller architectures like U-Net++.\n",
    "\n",
    "### **Summary**:\n",
    "U-Net is a powerful and efficient architecture for image segmentation tasks, especially in medical imaging. Its key innovation—**skip connections**—helps the model retain high-resolution information during downsampling and upsampling. This enables it to achieve precise pixel-wise segmentation, making it suitable for tasks that require fine-grained classification of each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# object detection losses \n",
    "\n",
    "### **Object Detection Losses**:\n",
    "\n",
    "In **object detection**, the goal is to not only classify the object in an image but also predict the location (bounding box) of the object. Object detection typically involves a combination of two tasks:\n",
    "1. **Classification**: Predicting the class of the object (e.g., car, person, dog).\n",
    "2. **Regression**: Predicting the bounding box that encloses the object (usually represented by coordinates: \\([x_{\\text{center}}, y_{\\text{center}}, w, h]\\), where \\(x_{\\text{center}}\\) and \\(y_{\\text{center}}\\) are the coordinates of the box center, and \\(w\\) and \\(h\\) are the width and height of the box).\n",
    "\n",
    "### **Common Losses in Object Detection**:\n",
    "\n",
    "Object detection losses are generally a **combination of classification loss** and **bounding box regression loss**. Below is a breakdown of the two main components of the loss:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Classification Loss**:\n",
    "\n",
    "For the classification task, object detection models typically use **cross-entropy loss** to predict the class of each object in the image.\n",
    "\n",
    "#### **Multi-Class Cross-Entropy Loss**:\n",
    "\n",
    "If there are \\(C\\) classes, including a background class (usually denoted as class 0), the classification loss for a particular bounding box \\(i\\) with class label \\(y_i\\) and predicted probability \\(p_i\\) is:\n",
    "\n",
    "\\[\n",
    "\\text{Classification Loss}_i = - \\sum_{c=1}^{C} y_{i,c} \\log(p_{i,c})\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(y_{i,c}\\) is the one-hot encoded ground truth for bounding box \\(i\\) in class \\(c\\).\n",
    "- \\(p_{i,c}\\) is the predicted probability that bounding box \\(i\\) belongs to class \\(c\\).\n",
    "\n",
    "The total classification loss is computed by summing the losses over all the bounding boxes (both foreground and background):\n",
    "\n",
    "\\[\n",
    "\\text{Total Classification Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Classification Loss}_i\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(N\\) is the number of bounding boxes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Bounding Box Regression Loss**:\n",
    "\n",
    "For the regression task, the goal is to predict the coordinates of the bounding box as accurately as possible. The loss function for bounding box regression typically uses **smooth L1 loss** (also called **Huber loss**), which combines **L2 loss** for small errors and **L1 loss** for large errors, making it more robust to outliers.\n",
    "\n",
    "#### **Smooth L1 Loss**:\n",
    "\n",
    "The **Smooth L1 loss** for bounding box regression is given by:\n",
    "\n",
    "\\[\n",
    "\\text{Smooth L1 Loss}(x) = \\begin{cases} \n",
    "0.5 x^2 & \\text{if } |x| < 1 \\\\\n",
    "|x| - 0.5 & \\text{if } |x| \\geq 1\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(x\\) is the difference between the predicted bounding box coordinate and the ground truth coordinate.\n",
    "- This loss is applied independently to each coordinate (\\(x_{\\text{center}}, y_{\\text{center}}, w, h\\)).\n",
    "\n",
    "The total regression loss is:\n",
    "\n",
    "\\[\n",
    "\\text{Total Regression Loss} = \\sum_{i=1}^{N} \\text{Smooth L1 Loss}(x_{\\text{pred}} - x_{\\text{gt}}) + \\text{Smooth L1 Loss}(y_{\\text{pred}} - y_{\\text{gt}}) + \\text{Smooth L1 Loss}(w_{\\text{pred}} - w_{\\text{gt}}) + \\text{Smooth L1 Loss}(h_{\\text{pred}} - h_{\\text{gt}})\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(x_{\\text{pred}}, y_{\\text{pred}}, w_{\\text{pred}}, h_{\\text{pred}}\\) are the predicted coordinates of the bounding box.\n",
    "- \\(x_{\\text{gt}}, y_{\\text{gt}}, w_{\\text{gt}}, h_{\\text{gt}}\\) are the ground truth coordinates of the bounding box.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Combined Loss**:\n",
    "\n",
    "In **object detection**, the total loss is a weighted sum of the classification loss and the regression loss. The loss for the entire model is:\n",
    "\n",
    "\\[\n",
    "\\text{Total Loss} = \\lambda_{\\text{cls}} \\cdot \\text{Classification Loss} + \\lambda_{\\text{reg}} \\cdot \\text{Regression Loss}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\lambda_{\\text{cls}}\\) is a weight factor for the classification loss.\n",
    "- \\(\\lambda_{\\text{reg}}\\) is a weight factor for the regression loss.\n",
    "\n",
    "The weights \\(\\lambda_{\\text{cls}}\\) and \\(\\lambda_{\\text{reg}}\\) are chosen to balance the importance of classification and regression tasks. These values are typically chosen based on experimentation or heuristics.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Other Losses (Optional)**:\n",
    "\n",
    "In some object detection architectures, other loss functions are included, such as:\n",
    "\n",
    "#### **Objectness Loss** (for models like YOLO):\n",
    "\n",
    "The objectness loss refers to predicting whether a region contains an object or not. It is typically a binary classification problem (object vs. no-object). This loss is often combined with the classification loss in models like YOLO.\n",
    "\n",
    "#### **Focal Loss** (for imbalanced datasets):\n",
    "\n",
    "Focal loss is a modification of cross-entropy loss designed to focus on hard-to-classify examples. It down-weights easy examples and focuses the model's attention on hard examples.\n",
    "\n",
    "The focal loss is given by:\n",
    "\n",
    "\\[\n",
    "\\text{Focal Loss} = - \\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\alpha_t\\) is a weighting factor for class \\(t\\).\n",
    "- \\(\\gamma\\) is the focusing parameter, which adjusts the rate at which easy examples are down-weighted.\n",
    "- \\(p_t\\) is the predicted probability for the true class.\n",
    "\n",
    "Focal loss is particularly useful for object detection tasks with class imbalance, where there are many more background pixels than object pixels.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Object Detection Losses**:\n",
    "\n",
    "1. **Classification Loss**: Typically uses **cross-entropy loss** (binary or multi-class).\n",
    "2. **Regression Loss**: Typically uses **smooth L1 loss** for bounding box coordinates.\n",
    "3. **Total Loss**: A weighted sum of classification and regression losses.\n",
    "\n",
    "In some cases, additional losses like **focal loss** and **objectness loss** may be included, depending on the model architecture. The goal is to simultaneously minimize classification errors and improve the accuracy of the predicted bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In **image segmentation**, the goal is to classify each pixel in an image as belonging to a specific class. For training deep learning models for segmentation tasks, different loss functions are used to optimize the model’s performance. These losses can be divided into pixel-wise classification losses and boundary-based losses.\n",
    "\n",
    "### 1. **Pixel-wise Classification Loss**\n",
    "This loss measures the difference between the predicted class probabilities for each pixel and the ground truth class labels. The most commonly used pixel-wise loss functions are **Cross-Entropy Loss** and **Dice Loss**.\n",
    "\n",
    "#### **1.1 Cross-Entropy Loss (for Multi-Class Segmentation)**\n",
    "\n",
    "Cross-entropy loss is widely used in classification problems, including pixel-wise classification for segmentation. For each pixel, we predict a probability distribution over the classes, and the loss is computed as the negative log likelihood of the correct class.\n",
    "\n",
    "\\[\n",
    "\\text{CE Loss}_i = - \\sum_{c=1}^{C} y_{i,c} \\log(p_{i,c})\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( C \\) is the total number of classes (including background).\n",
    "- \\( y_{i,c} \\) is the ground truth label for pixel \\(i\\) in class \\(c\\) (1 if pixel \\(i\\) belongs to class \\(c\\), 0 otherwise).\n",
    "- \\( p_{i,c} \\) is the predicted probability for pixel \\(i\\) being in class \\(c\\).\n",
    "\n",
    "The total loss is averaged across all pixels in the image:\n",
    "\n",
    "\\[\n",
    "\\text{Total Cross-Entropy Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CE Loss}_i\n",
    "\\]\n",
    "\n",
    "Where \\(N\\) is the total number of pixels.\n",
    "\n",
    "#### **1.2 Dice Loss (for Binary Segmentation or Imbalanced Data)**\n",
    "\n",
    "Dice Loss is a popular loss function for **binary segmentation** and works well when there is class imbalance (e.g., when foreground objects are much smaller than the background). It is based on the **Dice coefficient**, which measures the overlap between the predicted and ground truth regions.\n",
    "\n",
    "The Dice coefficient is given by:\n",
    "\n",
    "\\[\n",
    "\\text{Dice Coefficient} = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}\n",
    "\\]\n",
    "\n",
    "Where \\(A\\) and \\(B\\) are the predicted and ground truth regions, respectively. The **Dice Loss** is:\n",
    "\n",
    "\\[\n",
    "\\text{Dice Loss} = 1 - \\frac{2 \\cdot \\sum_{i=1}^{N} y_i p_i}{\\sum_{i=1}^{N} y_i + \\sum_{i=1}^{N} p_i}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the ground truth label for pixel \\(i\\) (1 for foreground, 0 for background).\n",
    "- \\( p_i \\) is the predicted probability for pixel \\(i\\) being foreground (1).\n",
    "\n",
    "#### **1.3 Combined Cross-Entropy and Dice Loss**\n",
    "\n",
    "For many segmentation tasks, it is common to combine **Cross-Entropy Loss** and **Dice Loss** to leverage the benefits of both. The total loss is typically a weighted sum of the two:\n",
    "\n",
    "\\[\n",
    "\\text{Total Loss} = \\lambda_{\\text{CE}} \\cdot \\text{Cross-Entropy Loss} + \\lambda_{\\text{Dice}} \\cdot \\text{Dice Loss}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\lambda_{\\text{CE}} \\) and \\( \\lambda_{\\text{Dice}} \\) are the weights for the Cross-Entropy and Dice Loss components, respectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Boundary-based Losses**\n",
    "These losses focus on improving the prediction of boundaries between objects, which is crucial for segmentation tasks where fine boundaries are important.\n",
    "\n",
    "#### **2.1 Boundary Loss**\n",
    "\n",
    "Boundary Loss is designed to enforce smoothness around the object boundaries and improve the segmentation of thin structures or small objects. It compares the predicted and ground truth boundary pixels and minimizes the difference between them.\n",
    "\n",
    "\\[\n",
    "\\text{Boundary Loss} = \\sum_{i \\in \\text{boundary pixels}} |y_i - p_i|\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the ground truth label for pixel \\(i\\) (1 for boundary, 0 otherwise).\n",
    "- \\( p_i \\) is the predicted probability for pixel \\(i\\) being part of the boundary.\n",
    "\n",
    "#### **2.2 Hausdorff Loss**\n",
    "\n",
    "Hausdorff Distance is a metric used to measure the distance between two sets (in this case, the predicted and ground truth boundaries). The **Hausdorff Loss** is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Hausdorff Loss} = \\max(\\text{Hausdorff Distance}_{\\text{pred}}(Y, P), \\text{Hausdorff Distance}_{\\text{true}}(P, Y))\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Y \\) is the set of ground truth boundary points.\n",
    "- \\( P \\) is the set of predicted boundary points.\n",
    "\n",
    "The Hausdorff distance measures the maximum distance between a point in one set and the closest point in the other set, encouraging better alignment between predicted and ground truth boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Generalized Dice Loss**\n",
    "\n",
    "Generalized Dice Loss is a variant of Dice Loss that works well with **multi-class segmentation** tasks and **imbalanced datasets**. It computes the Dice coefficient for each class separately and averages them. It is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Generalized Dice Loss} = 1 - \\frac{2 \\sum_{c=1}^{C} \\sum_{i=1}^{N} y_{i,c} p_{i,c}}{\\sum_{c=1}^{C} \\sum_{i=1}^{N} y_{i,c} + \\sum_{c=1}^{C} \\sum_{i=1}^{N} p_{i,c}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( C \\) is the number of classes.\n",
    "- \\( y_{i,c} \\) is the ground truth for pixel \\(i\\) in class \\(c\\).\n",
    "- \\( p_{i,c} \\) is the predicted probability for pixel \\(i\\) in class \\(c\\).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Tversky Loss**\n",
    "\n",
    "The **Tversky Loss** is a generalization of the Dice and IoU losses and is particularly useful for **highly imbalanced data**. It is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Tversky Loss} = 1 - \\frac{\\sum_{i=1}^{N} y_i p_i}{\\sum_{i=1}^{N} y_i p_i + \\alpha \\sum_{i=1}^{N} (1 - y_i) p_i + \\beta \\sum_{i=1}^{N} y_i (1 - p_i)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\alpha \\) and \\( \\beta \\) control the balance between false positives and false negatives. By adjusting these, the loss can emphasize precision or recall.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Focal Loss**\n",
    "\n",
    "Focal Loss is used to address class imbalance by down-weighting the loss assigned to well-classified pixels and focusing on hard-to-classify pixels. It is an extension of **Cross-Entropy Loss** and is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Focal Loss} = - \\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( p_t \\) is the predicted probability for the true class.\n",
    "- \\( \\alpha \\) is a balancing factor for the classes.\n",
    "- \\( \\gamma \\) is the focusing parameter that adjusts the rate at which easy examples are down-weighted.\n",
    "\n",
    "Focal Loss helps improve the model's performance on hard-to-detect objects by focusing on them more.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Total Loss for Image Segmentation**\n",
    "\n",
    "For image segmentation, a common loss function is a weighted combination of the pixel-wise loss (such as Cross-Entropy or Dice Loss) and boundary-based losses. For example:\n",
    "\n",
    "\\[\n",
    "\\text{Total Loss} = \\lambda_{\\text{CE}} \\cdot \\text{Cross-Entropy Loss} + \\lambda_{\\text{Dice}} \\cdot \\text{Dice Loss} + \\lambda_{\\text{Boundary}} \\cdot \\text{Boundary Loss}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\lambda_{\\text{CE}} \\), \\( \\lambda_{\\text{Dice}} \\), and \\( \\lambda_{\\text{Boundary}} \\) are the respective weights for each loss term.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Loss Functions for Image Segmentation**\n",
    "1. **Pixel-wise Losses**:\n",
    "   - **Cross-Entropy Loss** (for multi-class segmentation).\n",
    "   - **Dice Loss** (for binary segmentation and imbalanced data).\n",
    "   - **Combined Cross-Entropy and Dice Loss**.\n",
    "   - **Generalized Dice Loss** (for multi-class and imbalanced datasets).\n",
    "   - **Focal Loss** (for handling class imbalance).\n",
    "\n",
    "2. **Boundary-based Losses**:\n",
    "   - **Boundary Loss**.\n",
    "   - **Hausdorff Loss**.\n",
    "\n",
    "These loss functions can be combined depending on the segmentation task and dataset characteristics, helping the model improve both pixel-wise classification and boundary delineation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IoU (Intersection over Union) loss** is commonly used in **object detection** tasks, particularly when training models to predict bounding boxes around objects. It measures the overlap between the predicted bounding box and the ground truth bounding box, and is often used as a loss function to optimize the accuracy of the predicted bounding boxes.\n",
    "\n",
    "### **What is IoU (Intersection over Union)?**\n",
    "IoU is a metric used to evaluate the accuracy of an object detection model's predicted bounding box. It is defined as the ratio of the area of intersection of the predicted bounding box and the ground truth bounding box to the area of their union.\n",
    "\n",
    "\\[\n",
    "\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **Intersection** is the area where the predicted bounding box and the ground truth bounding box overlap.\n",
    "- **Union** is the total area covered by both the predicted and ground truth bounding boxes.\n",
    "\n",
    "### **Why is IoU used in Object Detection?**\n",
    "In object detection, the goal is to predict the bounding box around an object as accurately as possible. IoU is a natural choice because it quantifies how well the predicted box overlaps with the true box, giving a clear metric for performance.\n",
    "\n",
    "- **IoU values** range from 0 to 1:\n",
    "  - **IoU = 1** means the predicted box perfectly overlaps with the ground truth box.\n",
    "  - **IoU = 0** means there is no overlap between the predicted and ground truth boxes.\n",
    "  \n",
    "- **Thresholding**: In many object detection tasks, a threshold for IoU (e.g., 0.5) is set to determine if a predicted bounding box is a \"true positive\" or a \"false positive\". If the IoU is above the threshold, the prediction is considered correct; otherwise, it's a false prediction.\n",
    "\n",
    "### **IoU Loss in Object Detection**\n",
    "When used as a **loss function**, IoU loss measures the discrepancy between the predicted bounding box and the ground truth bounding box. The goal is to **minimize IoU loss** during training, encouraging the model to predict bounding boxes that maximize the overlap with the ground truth boxes.\n",
    "\n",
    "- **IoU loss function** can be defined as:\n",
    "\n",
    "\\[\n",
    "\\text{IoU Loss} = 1 - \\text{IoU}\n",
    "\\]\n",
    "\n",
    "The **1 - IoU** loss is commonly used because we want to minimize the IoU, so minimizing the loss corresponds to maximizing the IoU.\n",
    "\n",
    "### **Types of IoU Loss**\n",
    "1. **Standard IoU Loss**: Directly uses the formula \\( 1 - \\text{IoU} \\) to compute the loss between predicted and ground truth bounding boxes.\n",
    "\n",
    "2. **Soft IoU Loss**: A smoother version of the IoU loss, used to avoid the issue of sparse gradients when the IoU is very low or zero. It uses continuous approximations of IoU for better gradient flow during training.\n",
    "\n",
    "3. **IoU with Focal Loss**: This combination helps address class imbalance in object detection, especially in datasets where most objects are small or less frequent. Focal loss focuses on hard-to-detect examples by down-weighting easy examples.\n",
    "\n",
    "4. **Generalized IoU (GIoU) Loss**: An extension of the IoU loss that also penalizes the predicted box for being far from the ground truth box, not just for poor overlap. This helps improve localization, especially in cases where the predicted box and ground truth box have a low IoU but are spatially far apart.\n",
    "\n",
    "### **Benefits of IoU Loss**\n",
    "- **Better alignment**: It encourages better spatial alignment between predicted and true bounding boxes.\n",
    "- **Focus on overlap**: Directly optimizes the overlap between predicted and true boxes, which is the primary objective in object detection.\n",
    "- **Interpretability**: IoU provides an intuitive measure of how well the model is performing, as it directly measures the accuracy of bounding box predictions.\n",
    "\n",
    "### **Limitations of IoU Loss**\n",
    "- **Non-differentiability**: IoU is not a smooth, differentiable function, which can make it challenging to use as a loss function in gradient-based optimization methods.\n",
    "- **Challenges in small object detection**: IoU loss can be less effective for small objects, as the overlap between predicted and ground truth boxes might be small, leading to high variance in the loss.\n",
    "- **Sensitivity to box aspect ratio**: IoU loss may struggle when the aspect ratios of the predicted and ground truth boxes differ significantly.\n",
    "\n",
    "### **Summary**\n",
    "- **IoU loss** is used in object detection tasks to optimize the accuracy of predicted bounding boxes by maximizing their overlap with ground truth boxes.\n",
    "- It directly measures the intersection-over-union between predicted and ground truth bounding boxes.\n",
    "- **IoU loss** is often used in conjunction with other loss functions, such as **classification loss** (e.g., cross-entropy) or **regression loss** (e.g., smooth L1 loss), in multi-task learning setups for object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0s at left and 0s at right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list: [0, 1, 0, 255, 300, 1, 0, 0, 1]\n",
      "Before swap:\ti=1\tright=8\t\ts[i]=1\ts[right]=1\n",
      "After swap: \ti=1\tright=8\t\ts[i]=1\ts[right]=1\n",
      "Updated list: [0, 1, 0, 255, 300, 1, 0, 0, 1]\n",
      "\n",
      "Before swap:\ti=1\tright=7\t\ts[i]=1\ts[right]=0\n",
      "After swap: \ti=1\tright=7\t\ts[i]=0\ts[right]=1\n",
      "Updated list: [0, 0, 0, 255, 300, 1, 0, 1, 1]\n",
      "\n",
      "Before swap:\ti=5\tright=6\t\ts[i]=1\ts[right]=0\n",
      "After swap: \ti=5\tright=6\t\ts[i]=0\ts[right]=1\n",
      "Updated list: [0, 0, 0, 255, 300, 0, 1, 1, 1]\n",
      "\n",
      "Final list: [0, 0, 0, 0, 300, 255, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s = [0, 1, 0, 255, 300, 1, 0, 0, 1]\n",
    "print(f\"Original list: {s}\")\n",
    "n = len(s)\n",
    "left = 0\n",
    "right = n - 1\n",
    "i = 0\n",
    "\n",
    "while i <= right:\n",
    "    if s[i] == 0:\n",
    "        s[left], s[i] = s[i], s[left]  # Swap `0` to the left\n",
    "        left += 1\n",
    "        i += 1\n",
    "    elif s[i] == 1:\n",
    "        print(f\"Before swap:\\t{i=}\\t{right=}\\t\\t{s[i]=}\\t{s[right]=}\")\n",
    "        s[right], s[i] = s[i], s[right]  # Swap `1` to the right\n",
    "        print(f\"After swap: \\t{i=}\\t{right=}\\t\\t{s[i]=}\\t{s[right]=}\")\n",
    "        right -= 1\n",
    "        print(f\"Updated list: {s}\\n\")\n",
    "    else:\n",
    "        i += 1  # Move past non-0, non-1 elements\n",
    "\n",
    "print(f\"Final list: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a fundamental optimization algorithm, but it can encounter several issues depending on the problem and setup. Here are common problems with Gradient Descent and ways to address them:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Vanishing Gradients**\n",
    "- **Problem**: In deep networks, gradients can become very small during backpropagation, especially in sigmoid or tanh activations. This slows learning for earlier layers.\n",
    "- **Causes**:\n",
    "  - Activation functions that squash inputs into small ranges (e.g., sigmoid, tanh).\n",
    "  - Poor weight initialization.\n",
    "- **Solutions**:\n",
    "  - Use activation functions like ReLU, Leaky ReLU, or variants.\n",
    "  - Use proper weight initialization (e.g., Xavier, He initialization).\n",
    "  - Consider normalization techniques like Batch Normalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Exploding Gradients**\n",
    "- **Problem**: Gradients grow uncontrollably, leading to large updates and numerical instability.\n",
    "- **Causes**:\n",
    "  - Poor weight initialization.\n",
    "  - Deep networks without proper gradient flow control.\n",
    "- **Solutions**:\n",
    "  - Gradient clipping (limit the magnitude of gradients).\n",
    "  - Use proper weight initialization.\n",
    "  - Use smaller learning rates.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Local Minima or Saddle Points**\n",
    "- **Problem**: Gradient Descent can get stuck in local minima or flat saddle points, slowing convergence or preventing improvement.\n",
    "- **Solutions**:\n",
    "  - Use optimization algorithms like Adam or RMSProp that incorporate momentum and adaptive learning rates.\n",
    "  - Introduce noise in training (e.g., dropout, stochasticity in data batches).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Slow Convergence**\n",
    "- **Problem**: Gradient Descent can take many iterations to converge, especially with poorly chosen learning rates.\n",
    "- **Causes**:\n",
    "  - Poor choice of learning rate (too small or too large).\n",
    "  - Ill-conditioned loss surfaces (sharp in some directions, flat in others).\n",
    "- **Solutions**:\n",
    "  - Use learning rate schedules (e.g., cosine annealing, exponential decay).\n",
    "  - Use adaptive optimizers like Adam, Adagrad, or RMSProp.\n",
    "  - Precondition the problem using normalization or whitening of input data.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Overfitting**\n",
    "- **Problem**: Gradient Descent can overfit the training data, especially when the model is overparameterized.\n",
    "- **Solutions**:\n",
    "  - Use regularization techniques (e.g., L2 regularization, dropout).\n",
    "  - Increase the size of the training dataset (data augmentation or synthetic data generation).\n",
    "  - Apply early stopping based on validation performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Oscillations or Divergence**\n",
    "- **Problem**: Gradient Descent oscillates around the minimum or diverges.\n",
    "- **Causes**:\n",
    "  - Learning rate is too high.\n",
    "  - Loss surface is poorly conditioned.\n",
    "- **Solutions**:\n",
    "  - Reduce the learning rate.\n",
    "  - Use momentum-based optimizers like SGD with Momentum or Nesterov Momentum.\n",
    "  - Normalize input data to make the loss surface smoother.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Sensitivity to Initialization**\n",
    "- **Problem**: Poor weight initialization can lead to slow convergence or suboptimal solutions.\n",
    "- **Solutions**:\n",
    "  - Use initialization strategies like Xavier (Glorot) or He initialization.\n",
    "  - Consider pretraining or transfer learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Computational Inefficiency**\n",
    "- **Problem**: Computing gradients for large datasets can be time-consuming.\n",
    "- **Solutions**:\n",
    "  - Use Mini-batch Gradient Descent or Stochastic Gradient Descent (SGD).\n",
    "  - Parallelize computations using GPUs or distributed training.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Lack of Robustness**\n",
    "- **Problem**: Gradient Descent can fail in the presence of noisy or non-stationary data.\n",
    "- **Solutions**:\n",
    "  - Use robust optimizers like Adam or RMSProp.\n",
    "  - Perform data cleaning and outlier detection.\n",
    "  - Normalize input data.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Non-Convex Loss Surfaces**\n",
    "- **Problem**: In non-convex problems, Gradient Descent might get stuck in suboptimal solutions.\n",
    "- **Solutions**:\n",
    "  - Use advanced optimizers like Adam, or add noise to escape local minima.\n",
    "  - Experiment with larger or smaller batch sizes to alter the optimization dynamics.\n",
    "  - Use ensembles or other model-averaging techniques.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to explore any of these issues in more detail, or do you have a specific problem you're encountering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **accuracy** directly as a loss function is not ideal in most machine learning models, and here's why:\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Accuracy Is Not Suitable as a Loss Function**\n",
    "1. **Non-Differentiability**:\n",
    "   - Accuracy is a step-wise function, meaning it evaluates whether a prediction is correct or not (e.g., \\( \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} \\)).\n",
    "   - Step functions are not differentiable, so gradient-based optimization algorithms like SGD or Adam cannot use them to update model parameters.\n",
    "\n",
    "2. **No Gradient Information**:\n",
    "   - Loss functions like cross-entropy or mean squared error provide a continuous gradient that guides the model on how to adjust its parameters to improve.\n",
    "   - Accuracy does not provide this gradient, so the model would have no signal on how to improve its predictions.\n",
    "\n",
    "3. **Insensitive to Small Changes**:\n",
    "   - Accuracy only considers whether a prediction is correct, ignoring how confident or close the prediction is. For example:\n",
    "     - A model predicting \\( 0.51 \\) for a class with a true label of \\( 1 \\) is as correct as one predicting \\( 0.99 \\), even though the second prediction is more confident.\n",
    "   - This lack of sensitivity makes it unsuitable for training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Alternatives**\n",
    "While accuracy itself cannot be used as a loss function, you can use surrogate loss functions that align with improving accuracy. Common choices include:\n",
    "1. **Cross-Entropy Loss**:\n",
    "   - For classification problems, cross-entropy encourages the model to output probabilities closer to the true class labels.\n",
    "   - Works well with softmax activations for multi-class problems or sigmoid for binary classification.\n",
    "\n",
    "2. **Hinge Loss**:\n",
    "   - Commonly used in SVMs, hinge loss encourages correct classification with a margin, indirectly improving accuracy.\n",
    "\n",
    "3. **Focal Loss**:\n",
    "   - A variant of cross-entropy that focuses on hard-to-classify examples, which can indirectly improve accuracy on imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Accuracy**\n",
    "While accuracy is not a good choice for a loss function, it is an excellent **metric** to evaluate the model's performance during training or validation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Workaround to Use Accuracy-Like Behavior**\n",
    "If you want a loss function that mimics accuracy but is differentiable, consider using **soft approximations** of accuracy. For example:\n",
    "- Use the sigmoid or softmax outputs as probabilities and design a custom differentiable loss based on how close predictions are to the correct class.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example custom loss approximating accuracy\n",
    "def differentiable_accuracy_loss(predictions, targets):\n",
    "    # Predictions are logits; targets are one-hot encoded or class indices\n",
    "    probabilities = F.softmax(predictions, dim=1)\n",
    "    max_probs, predicted_classes = probabilities.max(dim=1)\n",
    "    correct = (predicted_classes == targets).float()\n",
    "    # Encourage higher probabilities for correct predictions\n",
    "    return 1 - correct.mean()  # Maximizing accuracy means minimizing (1 - accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like help implementing or testing a specific loss function for your problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChromaDB (or similar vector databases) doesn't actually **decode embeddings back into text**. Instead, it uses embeddings to find semantically similar entries and retrieves the associated text stored alongside those embeddings. Here's a breakdown of how this works:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. How Embeddings Work**\n",
    "- An **embedding** is a fixed-size numerical representation (vector) of text, capturing its semantic meaning. For example, a sentence like *\"I love programming\"* might be converted into a 512-dimensional vector.\n",
    "- Embeddings are typically created using models like OpenAI's `text-embedding-ada-002`, Sentence Transformers, or other NLP models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Storing Embeddings in ChromaDB**\n",
    "- When you add data to ChromaDB, it stores:\n",
    "  - **Text:** The original text (e.g., \"I love programming\").\n",
    "  - **Embedding:** The vector representation of the text (e.g., `[0.12, -0.34, 0.56, ...]`).\n",
    "  - **Metadata (Optional):** Any additional information you provide (e.g., tags, IDs).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Querying ChromaDB**\n",
    "When you perform a query in ChromaDB:\n",
    "1. **Create a Query Embedding:**\n",
    "   - The query (e.g., *\"I enjoy coding\"*) is converted into an embedding using the same model used for the stored data.\n",
    "2. **Similarity Search:**\n",
    "   - ChromaDB calculates the similarity (e.g., cosine similarity) between the query embedding and all stored embeddings.\n",
    "3. **Retrieve Text:**\n",
    "   - It retrieves the text (and associated metadata) corresponding to the most similar embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. No Decoding of Embeddings**\n",
    "ChromaDB doesn’t decode the embedding back into the original text. Instead:\n",
    "- It **maps embeddings to their associated text** during the query process.\n",
    "- The retrieved text is the original data stored alongside the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Example Workflow**\n",
    "Here’s how this process looks in Python:\n",
    "\n",
    "```python\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = Client(Settings(chroma_db_impl=\"duckdb+parquet\"))\n",
    "\n",
    "# Add text and embeddings to ChromaDB\n",
    "collection = client.get_or_create_collection(\"example_collection\")\n",
    "collection.add(\n",
    "    documents=[\"I love programming\", \"I enjoy hiking\", \"I like reading books\"],\n",
    "    metadatas=[{\"id\": 1}, {\"id\": 2}, {\"id\": 3}],\n",
    "    embeddings=[\n",
    "        [0.12, -0.34, 0.56],  # Example embeddings\n",
    "        [0.23, -0.12, 0.78],\n",
    "        [0.67, -0.89, 0.45],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Query using a new embedding\n",
    "query_embedding = [0.15, -0.33, 0.58]  # Example query embedding\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=2,\n",
    ")\n",
    "\n",
    "# Results contain the most similar stored documents\n",
    "print(results[\"documents\"])  # [\"I love programming\", \"I enjoy hiking\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Can’t Embeddings Be Decoded Back?**\n",
    "Embeddings are lossy representations:\n",
    "- They don’t store the exact text but rather its semantic essence.\n",
    "- Many different texts could map to similar embeddings, making decoding impossible.\n",
    "- Embeddings are not reversible—they are designed for efficient similarity search, not reconstruction.\n",
    "\n",
    "If you need the original text, it must be stored alongside the embeddings (as ChromaDB does).\n",
    "\n",
    "Would you like to dive deeper into embedding creation or the retrieval process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values in tabular data, especially when data is scarce, requires careful consideration to avoid losing valuable information. Here’s how you can approach this:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Analyze the Missingness**\n",
    "- **Understand the nature of missing values:**\n",
    "  - Are they missing at random (MAR)?\n",
    "  - Missing completely at random (MCAR)?\n",
    "  - Missing not at random (MNAR)?\n",
    "- Check the percentage of missing values in each column.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Imputation Techniques**\n",
    "Imputation is critical when data is scarce to preserve as much information as possible.\n",
    "\n",
    "#### **Numerical Features**\n",
    "- **Mean/Median Imputation:** Replace missing values with the mean or median of the column. Median is more robust to outliers.\n",
    "- **K-Nearest Neighbors (KNN) Imputation:** Use KNN to find similar rows and impute missing values based on neighbors.\n",
    "- **Regression Imputation:** Predict the missing value using a regression model trained on other features.\n",
    "- **Iterative Imputation:** Use models like Random Forest or Bayesian Ridge to iteratively impute missing values (e.g., `IterativeImputer` in scikit-learn).\n",
    "\n",
    "#### **Categorical Features**\n",
    "- **Mode Imputation:** Replace missing values with the most frequent category.\n",
    "- **Label Propagation:** Use similar rows (e.g., clustering or KNN) to infer the missing category.\n",
    "- **Add a Missing Category:** Treat missing values as a separate category (e.g., \"Unknown\").\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Feature Engineering**\n",
    "- **Create Missing Indicators:** Add binary features to indicate whether a value is missing for each column. This can help models learn patterns associated with missingness.\n",
    "- **Group-based Imputation:** Use group-level statistics for imputation. For example:\n",
    "  - Fill missing values in `income` based on the average income of people in the same `job_title`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Leverage Domain Knowledge**\n",
    "- If domain knowledge is available, use it to guide imputation. For instance:\n",
    "  - If missing values in a medical dataset are more likely to mean \"not measured,\" treat them accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Model-Specific Handling**\n",
    "- Some machine learning models can handle missing values directly, such as:\n",
    "  - Decision Trees, Random Forests, and Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost).\n",
    "  - These models can often process missing values without explicit imputation.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Augment the Data**\n",
    "- **Synthetic Data Generation:** Use techniques like SMOTE, GANs, or VAE to generate synthetic samples, especially for underrepresented classes or patterns.\n",
    "- **Data Augmentation:** For example, perturbing existing rows slightly to create new samples.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Experimentation and Validation**\n",
    "- Test different imputation strategies using cross-validation to see which method works best for your specific dataset.\n",
    "- Avoid imputing values for the test set based on training data alone.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Workflow in Python\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Mean/Median Imputation for Numerical Features\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "data['num_feature'] = mean_imputer.fit_transform(data[['num_feature']])\n",
    "\n",
    "# Mode Imputation for Categorical Features\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data['cat_feature'] = mode_imputer.fit_transform(data[['cat_feature']])\n",
    "\n",
    "# KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data = knn_imputer.fit_transform(data)\n",
    "\n",
    "# Iterative Imputation\n",
    "iter_imputer = IterativeImputer()\n",
    "data = iter_imputer.fit_transform(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like help implementing any specific technique or guidance on how to choose the best approach for your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not allowed to use activation functions like sigmoid or softmax in the output layer of a neural network for a two-class classification problem, the raw outputs of the dense layer (also called **logits**) can still be used. Here's how:\n",
    "\n",
    "### 1. **Use Raw Logits for Decision Making**\n",
    "   - The dense layer will output 10 neurons, which are unbounded real numbers.\n",
    "   - You can interpret the logits directly to determine the class. For example:\n",
    "     - If the logits corresponding to class 1 are greater than those for class 0, assign the instance to class 1.\n",
    "     - This assumes the logits are ordered, e.g., 5 neurons for class 0 and 5 neurons for class 1, or a similar structure.\n",
    "\n",
    "### 2. **Post-Processing Logits**\n",
    "   - Even without in-layer activation, you can apply a softmax function **externally** to the logits to interpret them as probabilities for the two classes:\n",
    "     \\[\n",
    "     P(\\text{class } i) = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}\n",
    "     \\]\n",
    "   - This does not involve modifying the network but allows you to convert logits to probabilities during evaluation.\n",
    "\n",
    "### 3. **Loss Function**\n",
    "   - If you’re training the model, the **loss function** must handle raw logits. Use loss functions designed to work with logits directly:\n",
    "     - **Binary Cross-Entropy with Logits** for two-class problems (e.g., `torch.nn.BCEWithLogitsLoss` in PyTorch).\n",
    "     - **Cross-Entropy Loss** for multi-class problems with raw logits (e.g., `torch.nn.CrossEntropyLoss`).\n",
    "\n",
    "### 4. **Practical Use Case**\n",
    "   If you want to bypass activation functions:\n",
    "   - Train the network as usual using the appropriate loss function.\n",
    "   - At inference time, compare the raw outputs (logits) or apply post-processing (like softmax) outside the model to make decisions.\n",
    "\n",
    "Would you like help with an implementation or specific framework details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standardization vs. Normalization**\n",
    "\n",
    "Both are techniques for scaling data, but they serve different purposes and are suited for different scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Standardization**\n",
    "- **Definition**: Transforms data to have a mean (\\( \\mu \\)) of 0 and a standard deviation (\\( \\sigma \\)) of 1. The formula is:\n",
    "  \\[\n",
    "  z = \\frac{x - \\mu}{\\sigma}\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( x \\): Original value\n",
    "  - \\( \\mu \\): Mean of the feature\n",
    "  - \\( \\sigma \\): Standard deviation of the feature\n",
    "\n",
    "- **Effect**: Keeps the original shape of the data's distribution but rescales it to have unit variance.\n",
    "\n",
    "- **When to Use**:\n",
    "  1. **Assumes Gaussian Distribution**: When the data follows a normal (or approximately normal) distribution.\n",
    "  2. **Features with Different Units**: Ensures that features measured in different units (e.g., height in cm and weight in kg) are treated equally.\n",
    "  3. **Models Sensitive to Scale**:\n",
    "     - Logistic Regression\n",
    "     - SVMs\n",
    "     - PCA\n",
    "     - Gradient-based models (e.g., neural networks)\n",
    "  4. **When Outliers Are Present**: Less sensitive to outliers compared to normalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Normalization**\n",
    "- **Definition**: Scales data to a fixed range, typically [0, 1] or [-1, 1]. The formula is:\n",
    "  \\[\n",
    "  x' = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( \\text{min}(x) \\): Minimum value of the feature\n",
    "  - \\( \\text{max}(x) \\): Maximum value of the feature\n",
    "\n",
    "- **Effect**: Rescales the data so all features have the same range, often between 0 and 1.\n",
    "\n",
    "- **When to Use**:\n",
    "  1. **Does Not Assume Distribution**: When the data does not follow a Gaussian distribution.\n",
    "  2. **Feature Range Matters**: When features have different scales and absolute values are important (e.g., pixel intensities in images).\n",
    "  3. **Distance-Based Models**:\n",
    "     - KNN\n",
    "     - K-means\n",
    "     - Clustering algorithms\n",
    "  4. **Sensitive to Outliers**: Avoid if the dataset contains significant outliers unless preprocessing is done to handle them.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison**\n",
    "\n",
    "| **Aspect**          | **Standardization**                    | **Normalization**                  |\n",
    "|----------------------|----------------------------------------|------------------------------------|\n",
    "| **Output Range**     | Mean = 0, SD = 1                      | Typically [0, 1] or [-1, 1]       |\n",
    "| **Assumption**       | Assumes normal distribution           | No assumption about distribution  |\n",
    "| **Sensitive to Outliers** | Less sensitive                     | Highly sensitive                  |\n",
    "| **Use Cases**        | Models sensitive to variance or scale | Models relying on absolute ranges |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which?**\n",
    "\n",
    "| **Scenario**                                      | **Recommended Technique** |\n",
    "|---------------------------------------------------|---------------------------|\n",
    "| Data is approximately normally distributed        | Standardization           |\n",
    "| Features have different units or scales           | Standardization           |\n",
    "| Distance-based models (e.g., KNN, K-means)        | Normalization             |\n",
    "| Data needs to fit within a specific range (e.g., [0, 1]) | Normalization             |\n",
    "| Presence of outliers                              | Standardization (or handle outliers first) |\n",
    "| Preparing data for PCA or SVM                     | Standardization           |\n",
    "| Image pixel data                                  | Normalization             |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example in Python**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# Standardization\n",
    "scaler_std = StandardScaler()\n",
    "data_standardized = scaler_std.fit_transform(data)\n",
    "\n",
    "# Normalization\n",
    "scaler_norm = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler_norm.fit_transform(data)\n",
    "\n",
    "print(\"Standardized Data:\\n\", data_standardized)\n",
    "print(\"Normalized Data:\\n\", data_normalized)\n",
    "```\n",
    "\n",
    "Would you like to explore further with specific datasets or algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropout Layer: An In-Depth Explanation**\n",
    "\n",
    "A **dropout layer** is a regularization technique used in neural networks to prevent overfitting. It works by randomly \"dropping out\" (i.e., setting to zero) a subset of neurons during each forward pass of training, effectively creating a thinner network.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Dropout?**\n",
    "- Introduced by **Srivastava et al. in 2014**, dropout is a simple yet powerful technique to improve generalization in deep learning models.\n",
    "- It randomly sets a fraction \\( p \\) (the dropout rate) of the neurons' activations to zero during training. This prevents the network from becoming overly reliant on specific neurons, encouraging redundancy and robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Dropout Works**\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - For each forward pass, randomly select a subset of neurons in the layer to \"drop out\" (set their outputs to zero).\n",
    "   - The dropout rate \\( p \\) determines the proportion of neurons to drop (e.g., \\( p = 0.5 \\) drops 50% of neurons).\n",
    "   - The remaining neurons are scaled by \\( \\frac{1}{1-p} \\) to maintain the expected output.\n",
    "\n",
    "   Mathematically:\n",
    "   \\[\n",
    "   y_i = \\begin{cases} \n",
    "   0 & \\text{with probability } p \\\\\n",
    "   \\frac{a_i}{1-p} & \\text{with probability } 1-p\n",
    "   \\end{cases}\n",
    "   \\]\n",
    "   Where:\n",
    "   - \\( y_i \\): Output of neuron \\( i \\)\n",
    "   - \\( a_i \\): Activation of neuron \\( i \\)\n",
    "\n",
    "2. **Inference Phase**:\n",
    "   - During inference, dropout is turned off (i.e., no neurons are dropped).\n",
    "   - To maintain consistency, the activations are scaled down by \\( 1-p \\) during training. This ensures that the expected sum of activations remains the same between training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Dropout?**\n",
    "\n",
    "1. **Prevent Overfitting**:\n",
    "   - Neural networks tend to memorize training data, leading to poor generalization on unseen data. Dropout combats this by forcing the network to learn more generalized features.\n",
    "\n",
    "2. **Promote Redundancy**:\n",
    "   - By dropping out neurons, the network learns to distribute its knowledge across multiple pathways instead of relying heavily on specific neurons.\n",
    "\n",
    "3. **Ensemble Effect**:\n",
    "   - Dropout can be thought of as training an ensemble of smaller networks (with shared weights). Each forward pass effectively trains a slightly different network, improving the model's robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### **Effects of Dropout**\n",
    "\n",
    "1. **Regularization**:\n",
    "   - Acts as a form of \\( L^2 \\) regularization, but instead of penalizing weights, it prevents co-adaptation of neurons.\n",
    "\n",
    "2. **Increased Training Time**:\n",
    "   - Since dropout reduces the effective capacity of the network during training, more epochs may be required to converge.\n",
    "\n",
    "3. **Improved Generalization**:\n",
    "   - Prevents overfitting, especially in large networks or when training on small datasets.\n",
    "\n",
    "4. **Robustness to Noise**:\n",
    "   - Makes the model more robust to input noise and less sensitive to minor changes in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Parameters**\n",
    "\n",
    "- **Dropout Rate (\\( p \\))**:\n",
    "  - Common values are \\( p = 0.2 \\) to \\( p = 0.5 \\).\n",
    "  - For input layers, a smaller dropout rate (e.g., \\( p = 0.1 \\)) is often used to preserve critical input features.\n",
    "  - For hidden layers, higher rates (e.g., \\( p = 0.5 \\)) are typical.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Dropout**\n",
    "1. **Overfitting Scenarios**:\n",
    "   - When the model performs well on training data but poorly on validation/test data.\n",
    "\n",
    "2. **Large Neural Networks**:\n",
    "   - Networks with many parameters are prone to overfitting.\n",
    "\n",
    "3. **Small Datasets**:\n",
    "   - Dropout can help extract more generalizable features when data is limited.\n",
    "\n",
    "---\n",
    "\n",
    "### **When Not to Use Dropout**\n",
    "1. **Inference**:\n",
    "   - Dropout is disabled during testing or deployment.\n",
    "   \n",
    "2. **Small Models**:\n",
    "   - If the model already has limited capacity, dropout might hinder learning.\n",
    "\n",
    "3. **Batch Normalization**:\n",
    "   - Dropout is often unnecessary when using batch normalization, as both techniques address overfitting in different ways.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dropout in Practice**\n",
    "\n",
    "**Example in PyTorch**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model with dropout\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% rate\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout during training\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "```\n",
    "\n",
    "**Example in Keras**:\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define a simple model with dropout\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(128,)),\n",
    "    Dropout(0.5),  # Dropout with 50% rate\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "1. **Training Instability**:\n",
    "   - May increase variance in gradient updates, requiring careful tuning of the learning rate.\n",
    "\n",
    "2. **Ineffective for All Models**:\n",
    "   - Dropout may not significantly improve performance in models that already generalize well (e.g., transformers).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to explore more about advanced dropout variants (e.g., SpatialDropout, DropBlock)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bias vs. Skewness: An In-Depth Comparison**\n",
    "\n",
    "Both **bias** and **skewness** are statistical concepts that describe different characteristics of data or models. While they might sound similar, they serve distinct purposes and arise in different contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Bias**\n",
    "#### **Definition**:\n",
    "Bias refers to the error introduced in a model due to overly simplistic assumptions in its learning algorithm. It is a measure of how far the model's predictions are from the actual data on average.\n",
    "\n",
    "#### **Context**: Bias is commonly used in machine learning and statistics to describe model behavior.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Characteristics of Bias**:\n",
    "1. **Nature**:\n",
    "   - Bias represents systematic error.\n",
    "   - It reflects the inability of a model to capture the underlying patterns in the data (underfitting).\n",
    "\n",
    "2. **Cause**:\n",
    "   - High bias often occurs when a model is too simple (e.g., linear regression for non-linear data).\n",
    "\n",
    "3. **Impact on Predictions**:\n",
    "   - High bias results in inaccurate predictions for both training and test data.\n",
    "\n",
    "4. **Relation to the Bias-Variance Tradeoff**:\n",
    "   - Bias is one component of the **total error**:\n",
    "     \\[\n",
    "     \\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Bias**:\n",
    "1. **Model Bias**:\n",
    "   - A linear regression model applied to quadratic data:\n",
    "     \\[\n",
    "     y = x^2 + \\epsilon\n",
    "     \\]\n",
    "     The linear model assumes a straight-line relationship and hence introduces systematic error (bias).\n",
    "\n",
    "2. **Algorithmic Bias**:\n",
    "   - A decision tree with a maximum depth of 1 (stump) has high bias as it oversimplifies the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implications**:\n",
    "- High bias leads to **underfitting**, where the model is too simple to capture the complexity of the data.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Skewness**\n",
    "#### **Definition**:\n",
    "Skewness is a measure of the asymmetry of the probability distribution of a random variable. It describes how much the data deviates from a symmetric bell-shaped distribution.\n",
    "\n",
    "#### **Context**: Skewness is a property of data distributions, not models.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Characteristics of Skewness**:\n",
    "1. **Nature**:\n",
    "   - Skewness describes the shape of a data distribution.\n",
    "   - A symmetric distribution has zero skewness.\n",
    "\n",
    "2. **Types**:\n",
    "   - **Positive Skewness (Right-Skewed)**:\n",
    "     - The right tail is longer.\n",
    "     - Mean > Median > Mode.\n",
    "   - **Negative Skewness (Left-Skewed)**:\n",
    "     - The left tail is longer.\n",
    "     - Mean < Median < Mode.\n",
    "\n",
    "3. **Formula**:\n",
    "   Skewness is mathematically defined as:\n",
    "   \\[\n",
    "   \\text{Skewness} = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s} \\right)^3\n",
    "   \\]\n",
    "   Where:\n",
    "   - \\( n \\): Number of data points\n",
    "   - \\( x_i \\): Individual data point\n",
    "   - \\( \\bar{x} \\): Mean of the data\n",
    "   - \\( s \\): Standard deviation\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Skewness**:\n",
    "1. **Positive Skewness**:\n",
    "   - Income distribution in many countries (most people earn below the mean, with a few earning very high incomes).\n",
    "2. **Negative Skewness**:\n",
    "   - Exam scores where most students score high, with a few scoring very low.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implications**:\n",
    "- Skewness can indicate potential outliers or the need for transformation (e.g., log or square root transformations) to normalize data for statistical modeling.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences Between Bias and Skewness**\n",
    "\n",
    "| **Aspect**             | **Bias**                                       | **Skewness**                                  |\n",
    "|-------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Definition**          | Systematic error in model predictions.         | Asymmetry in the distribution of data.        |\n",
    "| **Context**             | Refers to models and algorithms.               | Refers to data distributions.                 |\n",
    "| **Nature**              | Affects predictive accuracy.                   | Affects statistical assumptions and summaries. |\n",
    "| **Cause**               | Over-simplified model assumptions.             | Uneven distribution of data.                  |\n",
    "| **Impact**              | Leads to underfitting.                         | Indicates potential outliers or non-normality.|\n",
    "| **Metric**              | Bias is measured as the deviation from truth.  | Skewness is quantified using the skewness formula. |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Address Bias vs. Skewness**\n",
    "1. **Bias**:\n",
    "   - Use a more complex model or increase features to reduce bias.\n",
    "   - Example: Use a polynomial regression instead of a linear regression for quadratic data.\n",
    "\n",
    "2. **Skewness**:\n",
    "   - Apply data transformations (e.g., log, square root) to reduce skewness.\n",
    "   - Example: Normalize positively skewed income data using a log transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- **Bias** is about how well a model learns, while **skewness** is about how data is distributed.\n",
    "- Addressing bias improves model performance, while addressing skewness ensures that the data meets statistical assumptions for analysis.\n",
    "\n",
    "Would you like further examples or code demonstrations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pixel Attack vs. Adversarial Attack**\n",
    "\n",
    "Both **pixel attacks** and **adversarial attacks** are strategies to fool machine learning models, particularly in the domain of computer vision. However, they differ in their scope, methods, and goals. Here’s an in-depth comparison:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Pixel Attack**\n",
    "#### **Definition**:\n",
    "A **pixel attack** is a type of adversarial attack where only a small number of individual pixels in an image are modified to deceive the model.\n",
    "\n",
    "#### **Key Characteristics**:\n",
    "1. **Granularity**:\n",
    "   - Focuses on changing a minimal number of pixels (e.g., 1, 2, or 3 pixels).\n",
    "   - The attack aims to achieve maximum model misclassification with minimal pixel changes.\n",
    "\n",
    "2. **Purpose**:\n",
    "   - To test the model's robustness to highly localized changes.\n",
    "   - Simulates scenarios where minor image alterations can drastically impact model predictions.\n",
    "\n",
    "3. **Techniques**:\n",
    "   - Optimization-based: Identify the most influential pixels for model predictions and perturb them.\n",
    "   - Random selection: Randomly choose pixels and perturb their values.\n",
    "\n",
    "4. **Examples**:\n",
    "   - Changing a few pixels in an image of a cat to make the model classify it as a dog.\n",
    "\n",
    "5. **Strengths**:\n",
    "   - Minimal distortion, making the changes almost imperceptible to humans.\n",
    "   - Efficient for analyzing model vulnerabilities.\n",
    "\n",
    "6. **Limitations**:\n",
    "   - May not always succeed, especially for robust models.\n",
    "   - Limited to specific scenarios with minor perturbations.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Adversarial Attack**\n",
    "#### **Definition**:\n",
    "An **adversarial attack** is a broader term encompassing techniques that intentionally modify input data to deceive a machine learning model. It includes perturbations across the entire image, rather than focusing on individual pixels.\n",
    "\n",
    "#### **Key Characteristics**:\n",
    "1. **Granularity**:\n",
    "   - Modifies many or all pixels in the image, often constrained by a specific norm (e.g., \\(L_2\\), \\(L_\\infty\\)).\n",
    "\n",
    "2. **Purpose**:\n",
    "   - To test or exploit vulnerabilities in a model by crafting inputs that look similar to humans but lead to incorrect predictions.\n",
    "\n",
    "3. **Techniques**:\n",
    "   - **Gradient-Based**:\n",
    "     - FGSM (Fast Gradient Sign Method): Adds noise proportional to the gradient of the loss.\n",
    "     - PGD (Projected Gradient Descent): Iterative version of FGSM.\n",
    "   - **Optimization-Based**:\n",
    "     - Uses optimization algorithms to find the smallest perturbation that fools the model.\n",
    "   - **Black-Box**:\n",
    "     - Does not rely on model gradients but uses queries to infer vulnerabilities.\n",
    "\n",
    "4. **Examples**:\n",
    "   - Adding subtle noise to a stop sign image that causes a model to misclassify it as a yield sign.\n",
    "\n",
    "5. **Strengths**:\n",
    "   - Effective against many models, including deep neural networks.\n",
    "   - Can reveal systemic vulnerabilities in models.\n",
    "\n",
    "6. **Limitations**:\n",
    "   - Requires more computation than pixel attacks.\n",
    "   - May introduce more noticeable perturbations compared to pixel attacks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences**\n",
    "\n",
    "| **Aspect**               | **Pixel Attack**                             | **Adversarial Attack**                       |\n",
    "|--------------------------|----------------------------------------------|----------------------------------------------|\n",
    "| **Scope**                | Focuses on modifying a small number of pixels.| Modifies multiple or all pixels in an image. |\n",
    "| **Perturbation Level**   | Minimal, localized changes.                  | Broader, often constrained by a norm.        |\n",
    "| **Perceptibility**       | Changes are often imperceptible.             | Changes can be subtle but more widespread.   |\n",
    "| **Techniques**           | Optimizes a few pixels for maximum impact.   | Uses gradient or optimization-based methods. |\n",
    "| **Use Case**             | Tests robustness to small, localized changes.| Evaluates overall model robustness.          |\n",
    "| **Effectiveness**        | Limited against robust models.               | Highly effective against most models.        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Which One to Use?**\n",
    "- **Pixel Attack**:\n",
    "  - Ideal for lightweight testing and understanding localized vulnerabilities.\n",
    "  - Useful when minimal modifications are required (e.g., security applications).\n",
    "\n",
    "- **Adversarial Attack**:\n",
    "  - Suitable for comprehensive robustness testing.\n",
    "  - Often used in research to evaluate defenses against adversarial inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "While **pixel attacks** are a subset of **adversarial attacks**, their localized nature makes them distinct. Pixel attacks focus on minimal changes, whereas adversarial attacks explore broader vulnerabilities. Both are crucial for testing and improving the robustness of machine learning models.\n",
    "\n",
    "Would you like examples or code for either type of attack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A/B Testing**\n",
    "\n",
    "A/B testing, also known as **split testing**, is a statistical method used to compare two versions of a variable (A and B) to determine which one performs better in achieving a desired outcome. It is widely used in various domains, such as web development, marketing, product design, and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Purpose of A/B Testing**\n",
    "The primary goal is to make data-driven decisions by comparing two variations of an experiment to understand their impact on a target metric.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Components**\n",
    "1. **Control Group (A)**:\n",
    "   - Represents the current or baseline version of the system (e.g., an existing webpage, ad campaign, or algorithm).\n",
    "\n",
    "2. **Test Group (B)**:\n",
    "   - Represents the new variation that is being tested against the control.\n",
    "\n",
    "3. **Target Metric**:\n",
    "   - The key performance indicator (KPI) being measured, such as click-through rate (CTR), conversion rate, or revenue.\n",
    "\n",
    "4. **Sample Population**:\n",
    "   - The users or subjects who are randomly divided into groups to participate in the experiment.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Process of A/B Testing**\n",
    "1. **Define the Objective**:\n",
    "   - Clearly state what you aim to achieve (e.g., increase sign-ups by 10%).\n",
    "\n",
    "2. **Identify the Variable to Test**:\n",
    "   - Choose one variable to change (e.g., button color, headline text, or algorithm parameter).\n",
    "\n",
    "3. **Divide the Population**:\n",
    "   - Randomly assign participants to the control group (A) or test group (B) to eliminate biases.\n",
    "\n",
    "4. **Run the Experiment**:\n",
    "   - Expose the groups to their respective versions and collect data over a predefined period.\n",
    "\n",
    "5. **Measure Performance**:\n",
    "   - Evaluate the target metric for each group.\n",
    "\n",
    "6. **Statistical Analysis**:\n",
    "   - Use statistical methods (e.g., t-tests, p-values, confidence intervals) to determine if the observed differences are statistically significant.\n",
    "\n",
    "7. **Make Decisions**:\n",
    "   - Implement the better-performing version or iterate on the test.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Example**\n",
    "#### Scenario:\n",
    "A company wants to test whether changing the color of a \"Sign Up\" button from blue (control) to green (test) increases the sign-up rate.\n",
    "\n",
    "1. Objective: Increase the sign-up rate.\n",
    "2. Variable: Button color.\n",
    "3. Population: Website visitors over a week.\n",
    "4. Groups:\n",
    "   - **Group A (Control)**: Sees the blue button.\n",
    "   - **Group B (Test)**: Sees the green button.\n",
    "5. Results:\n",
    "   - Group A: 4% sign-up rate.\n",
    "   - Group B: 6% sign-up rate.\n",
    "6. Analysis:\n",
    "   - Perform a statistical test to ensure the 2% difference is significant.\n",
    "7. Decision:\n",
    "   - If significant, adopt the green button.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Statistical Considerations**\n",
    "1. **Significance Level (\\(\\alpha\\))**:\n",
    "   - The threshold to reject the null hypothesis, commonly set at 0.05.\n",
    "\n",
    "2. **P-value**:\n",
    "   - Probability of observing the result (or more extreme) under the null hypothesis. A p-value < 0.05 typically indicates significance.\n",
    "\n",
    "3. **Confidence Interval**:\n",
    "   - Range of values that likely contain the true effect size.\n",
    "\n",
    "4. **Sample Size**:\n",
    "   - Ensure enough participants to detect a meaningful difference (use power analysis).\n",
    "\n",
    "5. **Avoiding Bias**:\n",
    "   - Randomize group assignments.\n",
    "   - Run tests for sufficient duration to account for temporal effects.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Benefits of A/B Testing**\n",
    "- **Data-Driven Decisions**: Reduces guesswork.\n",
    "- **Improved User Experience**: Identifies what resonates with users.\n",
    "- **Higher Conversion Rates**: Optimizes for better performance.\n",
    "- **Scalability**: Can test small changes with significant impact.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Limitations**\n",
    "- **Sample Size Dependence**: Small samples can lead to inconclusive results.\n",
    "- **Single Variable Focus**: Testing one variable at a time can be time-consuming.\n",
    "- **Context Sensitivity**: Results may not generalize to all users or scenarios.\n",
    "- **Short-Term Focus**: May miss long-term effects.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Applications**\n",
    "- **Web Design**: Testing layouts, headlines, or call-to-action buttons.\n",
    "- **Marketing**: Evaluating ad copy or campaign strategies.\n",
    "- **Machine Learning**: Comparing algorithm variations or model updates.\n",
    "- **Product Development**: Testing new features or pricing strategies.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like guidance on implementing A/B testing in a specific context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# issue with Gradient descent\n",
    "Let me explain why stochastic gradient descent (SGD) is generally better at avoiding local minima compared to batch gradient descent. This is a fascinating aspect of optimization algorithms that reveals important insights about how randomness can actually help us find better solutions.\n",
    "\n",
    "Think of the optimization landscape like a hilly terrain where we're trying to find the lowest point (global minimum). Both algorithms try to reach this point, but they take different approaches:\n",
    "\n",
    "Batch Gradient Descent:\n",
    "- Uses the entire dataset to compute the gradient at each step\n",
    "- Takes the average direction from all data points\n",
    "- Makes very precise, deterministic steps\n",
    "\n",
    "Stochastic Gradient Descent:\n",
    "- Uses one random data point at a time\n",
    "- Takes noisy, approximate steps\n",
    "- Introduces randomness into the optimization process\n",
    "\n",
    "Now, imagine you're walking down this hilly terrain:\n",
    "\n",
    "1. Batch Gradient Descent is like walking with perfect information about the slope around you. You always take steps in the exact direction of steepest descent. This seems good, but when you reach a valley (local minimum), you're stuck because all directions point upward.\n",
    "\n",
    "2. Stochastic Gradient Descent is like walking while someone occasionally gives you a little push in random directions. These random pushes might seem counterproductive, but they can actually help you escape local minima. When you're in a small valley, a random push might be just what you need to climb out and find an even deeper valley (better minimum).\n",
    "\n",
    "Let me create a visual demonstration to make this clearer:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_loss_landscape():\n",
    "    \"\"\"Create a loss landscape with multiple local minima\"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.sin(x) + 0.5 * np.sin(3*x) + 0.1 * x**2\n",
    "    return x, y\n",
    "\n",
    "def batch_gradient_descent(x_start, learning_rate, iterations):\n",
    "    \"\"\"Simulate batch gradient descent\"\"\"\n",
    "    path = [x_start]\n",
    "    x_current = x_start\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Compute gradient (derivative of our function)\n",
    "        gradient = (np.cos(x_current) + 1.5 * np.cos(3*x_current) + 0.2 * x_current)\n",
    "        # Update position\n",
    "        x_current = x_current - learning_rate * gradient\n",
    "        path.append(x_current)\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "def stochastic_gradient_descent(x_start, learning_rate, iterations):\n",
    "    \"\"\"Simulate stochastic gradient descent with noise\"\"\"\n",
    "    path = [x_start]\n",
    "    x_current = x_start\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Add random noise to the gradient\n",
    "        noise = np.random.normal(0, 0.5)\n",
    "        gradient = (np.cos(x_current) + 1.5 * np.cos(3*x_current) + 0.2 * x_current)\n",
    "        # Update position with noisy gradient\n",
    "        x_current = x_current - learning_rate * (gradient + noise)\n",
    "        path.append(x_current)\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Create and plot the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create loss landscape\n",
    "x, y = create_loss_landscape()\n",
    "plt.plot(x, y, 'b-', label='Loss Landscape', alpha=0.3)\n",
    "\n",
    "# Run both optimizers from different starting points\n",
    "start_points = [-4, -2, 0, 2, 4]\n",
    "colors = ['r', 'g', 'c', 'm', 'y']\n",
    "\n",
    "for start_point, color in zip(start_points, colors):\n",
    "    # Run batch gradient descent\n",
    "    batch_path = batch_gradient_descent(start_point, 0.1, 50)\n",
    "    plt.plot(batch_path, np.sin(batch_path) + 0.5 * np.sin(3*batch_path) + 0.1 * batch_path**2,\n",
    "             color=color, linestyle='--', marker='o', markersize=3,\n",
    "             label=f'Batch GD from {start_point}')\n",
    "    \n",
    "    # Run stochastic gradient descent\n",
    "    sgd_path = stochastic_gradient_descent(start_point, 0.1, 50)\n",
    "    plt.plot(sgd_path, np.sin(sgd_path) + 0.5 * np.sin(3*sgd_path) + 0.1 * sgd_path**2,\n",
    "             color=color, linestyle='-', marker='^', markersize=3,\n",
    "             label=f'SGD from {start_point}')\n",
    "\n",
    "plt.title('Batch vs Stochastic Gradient Descent')\n",
    "plt.xlabel('Parameter Space')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_loss_landscape():\n",
    "    \"\"\"Create a loss landscape with multiple local minima\"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.sin(x) + 0.5 * np.sin(3 * x) + 0.1 * x**2\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def batch_gradient_descent(x_start, learning_rate, iterations):\n",
    "    \"\"\"Simulate batch gradient descent\"\"\"\n",
    "    path = [x_start]\n",
    "    x_current = x_start\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Compute gradient (derivative of our function)\n",
    "        gradient = np.cos(x_current) + 1.5 * np.cos(3 * x_current) + 0.2 * x_current\n",
    "        # Update position\n",
    "        x_current = x_current - learning_rate * gradient\n",
    "        path.append(x_current)\n",
    "\n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(x_start, learning_rate, iterations):\n",
    "    \"\"\"Simulate stochastic gradient descent with noise\"\"\"\n",
    "    path = [x_start]\n",
    "    x_current = x_start\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Add random noise to the gradient\n",
    "        noise = np.random.normal(0, 0.5)\n",
    "        gradient = np.cos(x_current) + 1.5 * np.cos(3 * x_current) + 0.2 * x_current\n",
    "        # Update position with noisy gradient\n",
    "        x_current = x_current - learning_rate * (gradient + noise)\n",
    "        path.append(x_current)\n",
    "\n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# Create and plot the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create loss landscape\n",
    "x, y = create_loss_landscape()\n",
    "plt.plot(x, y, \"b-\", label=\"Loss Landscape\", alpha=0.3)\n",
    "\n",
    "# Run both optimizers from different starting points\n",
    "start_points = [-4, -2, 0, 2, 4]\n",
    "colors = [\"r\", \"g\", \"c\", \"m\", \"y\"]\n",
    "\n",
    "for start_point, color in zip(start_points, colors):\n",
    "    # Run batch gradient descent\n",
    "    batch_path = batch_gradient_descent(start_point, 0.1, 50)\n",
    "    plt.plot(\n",
    "        batch_path,\n",
    "        np.sin(batch_path) + 0.5 * np.sin(3 * batch_path) + 0.1 * batch_path**2,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        marker=\"o\",\n",
    "        markersize=3,\n",
    "        label=f\"Batch GD from {start_point}\",\n",
    "    )\n",
    "\n",
    "    # Run stochastic gradient descent\n",
    "    sgd_path = stochastic_gradient_descent(start_point, 0.1, 50)\n",
    "    plt.plot(\n",
    "        sgd_path,\n",
    "        np.sin(sgd_path) + 0.5 * np.sin(3 * sgd_path) + 0.1 * sgd_path**2,\n",
    "        color=color,\n",
    "        linestyle=\"-\",\n",
    "        marker=\"^\",\n",
    "        markersize=3,\n",
    "        label=f\"SGD from {start_point}\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Batch vs Stochastic Gradient Descent\")\n",
    "plt.xlabel(\"Parameter Space\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The visualization shows several key differences:\n",
    "\n",
    "1. **Path Behavior**:\n",
    "   - Batch GD (dashed lines) takes smooth, predictable paths and often gets trapped in the nearest local minimum\n",
    "   - SGD (solid lines) takes noisier paths but can jump out of local minima and explore more of the parameter space\n",
    "\n",
    "2. **Convergence**:\n",
    "   - Batch GD converges more precisely to the nearest minimum\n",
    "   - SGD might not converge as precisely but is more likely to find better minima\n",
    "\n",
    "3. **Exploration**:\n",
    "   - Batch GD is like a cautious explorer who never deviates from the most obvious path\n",
    "   - SGD is like an adventurous explorer who's willing to try different paths, sometimes finding better routes\n",
    "\n",
    "This noise in SGD serves several important purposes:\n",
    "- It adds energy to the system, helping it overcome small barriers\n",
    "- It allows exploration of different regions of the parameter space\n",
    "- It makes the algorithm more robust to different initial conditions\n",
    "\n",
    "In practice, this means that SGD is often better at finding good solutions in complex optimization landscapes, especially in deep learning where there are many local minima. The noise acts as a form of regularization, often leading to solutions that generalize better to new data.\n",
    "\n",
    "Would you like me to explain more about how we can control this noise level, or how modern variations like mini-batch gradient descent combine the benefits of both approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain in depth why stochastic gradient descent (SGD) is better at avoiding local extrema compared to batch gradient descent. I'll break this down systematically and build from fundamental concepts to more complex insights.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class OptimizationVisualizer:\n",
    "    \"\"\"\n",
    "    Demonstrates how batch and stochastic gradient descent behave differently\n",
    "    with respect to local minima\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Create a complex loss landscape with multiple local minima\n",
    "        self.x = np.linspace(-5, 5, 200)\n",
    "        # Function with multiple local minima\n",
    "        self.y = (np.sin(self.x) + \n",
    "                 0.5 * np.sin(3 * self.x) + \n",
    "                 0.25 * np.sin(5 * self.x) + \n",
    "                 0.1 * self.x**2)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        \"\"\"Compute the gradient at point x\"\"\"\n",
    "        return (np.cos(x) + \n",
    "                1.5 * np.cos(3 * x) + \n",
    "                1.25 * np.cos(5 * x) + \n",
    "                0.2 * x)\n",
    "    \n",
    "    def batch_descent(self, start_point, learning_rate=0.1, iterations=100):\n",
    "        \"\"\"Simulate batch gradient descent\"\"\"\n",
    "        path = [start_point]\n",
    "        current = start_point\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            # Compute exact gradient\n",
    "            grad = self.gradient(current)\n",
    "            # Update position using exact gradient\n",
    "            current = current - learning_rate * grad\n",
    "            path.append(current)\n",
    "            \n",
    "            # Check for convergence (gradient close to zero)\n",
    "            if abs(grad) < 1e-5:\n",
    "                break\n",
    "                \n",
    "        return np.array(path)\n",
    "    \n",
    "    def stochastic_descent(self, start_point, learning_rate=0.1, \n",
    "                          iterations=100, noise_scale=0.5):\n",
    "        \"\"\"Simulate stochastic gradient descent with controlled noise\"\"\"\n",
    "        path = [start_point]\n",
    "        current = start_point\n",
    "        \n",
    "        # Implement annealing schedule for noise\n",
    "        noise_schedule = np.linspace(noise_scale, noise_scale/10, iterations)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # Add random noise to gradient\n",
    "            noise = np.random.normal(0, noise_schedule[i])\n",
    "            grad = self.gradient(current)\n",
    "            # Update position using noisy gradient\n",
    "            current = current - learning_rate * (grad + noise)\n",
    "            path.append(current)\n",
    "            \n",
    "        return np.array(path)\n",
    "    \n",
    "    def visualize_comparison(self, start_points=[-4, -2, 0, 2, 4]):\n",
    "        \"\"\"Create comparative visualization of both methods\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Plot loss landscape\n",
    "        plt.plot(self.x, self.y, 'b-', alpha=0.3, label='Loss Landscape')\n",
    "        \n",
    "        # Plot optimization paths from different starting points\n",
    "        colors = ['r', 'g', 'c', 'm', 'y']\n",
    "        for start, color in zip(start_points, colors):\n",
    "            # Run and plot batch gradient descent\n",
    "            batch_path = self.batch_descent(start)\n",
    "            plt.plot(batch_path, self.y[np.searchsorted(self.x, batch_path)],\n",
    "                    f'{color}--o', markersize=4, alpha=0.7,\n",
    "                    label=f'Batch GD from {start}')\n",
    "            \n",
    "            # Run and plot stochastic gradient descent\n",
    "            sgd_path = self.stochastic_descent(start)\n",
    "            plt.plot(sgd_path, self.y[np.searchsorted(self.x, sgd_path)],\n",
    "                    f'{color}-^', markersize=4, alpha=0.7,\n",
    "                    label=f'SGD from {start}')\n",
    "        \n",
    "        plt.title('Comparison of Batch vs Stochastic Gradient Descent')\n",
    "        plt.xlabel('Parameter Space')\n",
    "        plt.ylabel('Loss Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        return plt\n",
    "\n",
    "# Create and show visualization\n",
    "visualizer = OptimizationVisualizer()\n",
    "visualizer.visualize_comparison()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "Let me explain why stochastic gradient descent is better at escaping local minima through several key concepts:\n",
    "\n",
    "1. The Nature of Optimization Landscapes\n",
    "\n",
    "Think of the optimization landscape like a mountainous terrain. In machine learning, this terrain is created by our loss function, with hills and valleys representing higher and lower loss values. The global minimum is the lowest valley in this landscape, but there are many smaller valleys (local minima) along the way.\n",
    "\n",
    "2. How Batch Gradient Descent Works\n",
    "\n",
    "Batch gradient descent is like a very careful hiker who:\n",
    "- Always looks at the entire landscape before taking a step\n",
    "- Always moves in the direction of steepest descent\n",
    "- Makes consistent, predictable movements\n",
    "- Uses perfect information about the slope at each point\n",
    "\n",
    "This careful approach has a significant drawback: when batch gradient descent reaches any valley (local minimum), it stops. This happens because:\n",
    "- The gradient (slope) becomes zero or very close to zero\n",
    "- All directions lead uphill\n",
    "- The algorithm has no mechanism to explore beyond this point\n",
    "\n",
    "3. How Stochastic Gradient Descent Works\n",
    "\n",
    "Stochastic gradient descent is like a hiker who:\n",
    "- Looks at only one random part of the landscape at a time\n",
    "- Takes steps that aren't always in the perfect direction\n",
    "- Makes noisy, unpredictable movements\n",
    "- Uses imperfect, partial information about the slope\n",
    "\n",
    "This seemingly less precise approach actually helps because:\n",
    "- The noise in gradient estimates acts like small random pushes\n",
    "- These random pushes can help escape shallow local minima\n",
    "- The algorithm continues exploring even in valleys\n",
    "\n",
    "4. The Role of Noise in SGD\n",
    "\n",
    "The noise in SGD serves several crucial purposes:\n",
    "- Adds energy to the system, helping overcome small barriers\n",
    "- Allows exploration of different regions of the parameter space\n",
    "- Prevents premature convergence to poor solutions\n",
    "- Acts as a natural form of regularization\n",
    "\n",
    "5. Mathematical Perspective\n",
    "\n",
    "From a mathematical viewpoint, SGD's ability to escape local minima comes from:\n",
    "- Variance in gradient estimates providing exploration energy\n",
    "- Stochastic approximation of the true gradient\n",
    "- Implicit momentum from the sequence of noisy updates\n",
    "- Natural annealing as the algorithm progresses\n",
    "\n",
    "6. Practical Benefits\n",
    "\n",
    "In practice, SGD's properties lead to several advantages:\n",
    "- Better generalization to new data\n",
    "- More robust solutions\n",
    "- Faster training in many cases\n",
    "- Ability to handle large datasets efficiently\n",
    "\n",
    "7. Real-world Analogy\n",
    "\n",
    "Think of it like trying to find the lowest point in a dark room by feeling the floor:\n",
    "- Batch gradient descent is like taking careful steps always downward\n",
    "- SGD is like occasionally stumbling or taking slightly wrong steps\n",
    "- Those \"stumbles\" in SGD might help you find an even lower point you would have missed with careful steps\n",
    "\n",
    "Would you like me to elaborate on any of these aspects? For instance, we could explore how to control the amount of noise in SGD, or how modern variations like mini-batch gradient descent combine the benefits of both approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class OptimizationVisualizer:\n",
    "    \"\"\"\n",
    "    Demonstrates how batch and stochastic gradient descent behave differently\n",
    "    with respect to local minima\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a complex loss landscape with multiple local minima\n",
    "        self.x = np.linspace(-5, 5, 200)\n",
    "        # Function with multiple local minima\n",
    "        self.y = (\n",
    "            np.sin(self.x)\n",
    "            + 0.5 * np.sin(3 * self.x)\n",
    "            + 0.25 * np.sin(5 * self.x)\n",
    "            + 0.1 * self.x**2\n",
    "        )\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"Compute the gradient at point x\"\"\"\n",
    "        return np.cos(x) + 1.5 * np.cos(3 * x) + 1.25 * np.cos(5 * x) + 0.2 * x\n",
    "\n",
    "    def batch_descent(self, start_point, learning_rate=0.1, iterations=100):\n",
    "        \"\"\"Simulate batch gradient descent\"\"\"\n",
    "        path = [start_point]\n",
    "        current = start_point\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # Compute exact gradient\n",
    "            grad = self.gradient(current)\n",
    "            # Update position using exact gradient\n",
    "            current = current - learning_rate * grad\n",
    "            path.append(current)\n",
    "\n",
    "            # Check for convergence (gradient close to zero)\n",
    "            if abs(grad) < 1e-5:\n",
    "                break\n",
    "\n",
    "        return np.array(path)\n",
    "\n",
    "    def stochastic_descent(\n",
    "        self, start_point, learning_rate=0.1, iterations=100, noise_scale=0.5\n",
    "    ):\n",
    "        \"\"\"Simulate stochastic gradient descent with controlled noise\"\"\"\n",
    "        path = [start_point]\n",
    "        current = start_point\n",
    "\n",
    "        # Implement annealing schedule for noise\n",
    "        noise_schedule = np.linspace(noise_scale, noise_scale / 10, iterations)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            # Add random noise to gradient\n",
    "            noise = np.random.normal(0, noise_schedule[i])\n",
    "            grad = self.gradient(current)\n",
    "            # Update position using noisy gradient\n",
    "            current = current - learning_rate * (grad + noise)\n",
    "            path.append(current)\n",
    "\n",
    "        return np.array(path)\n",
    "\n",
    "    def visualize_comparison(self, start_points=[-4, -2, 0, 2, 4]):\n",
    "        \"\"\"Create comparative visualization of both methods\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        # Plot loss landscape\n",
    "        plt.plot(self.x, self.y, \"b-\", alpha=0.3, label=\"Loss Landscape\")\n",
    "\n",
    "        # Plot optimization paths from different starting points\n",
    "        colors = [\"r\", \"g\", \"c\", \"m\", \"y\"]\n",
    "        for start, color in zip(start_points, colors):\n",
    "            # Run and plot batch gradient descent\n",
    "            batch_path = self.batch_descent(start)\n",
    "            plt.plot(\n",
    "                batch_path,\n",
    "                self.y[np.searchsorted(self.x, batch_path)],\n",
    "                f\"{color}--o\",\n",
    "                markersize=4,\n",
    "                alpha=0.7,\n",
    "                label=f\"Batch GD from {start}\",\n",
    "            )\n",
    "\n",
    "            # Run and plot stochastic gradient descent\n",
    "            sgd_path = self.stochastic_descent(start)\n",
    "            plt.plot(\n",
    "                sgd_path,\n",
    "                self.y[np.searchsorted(self.x, sgd_path)],\n",
    "                f\"{color}-^\",\n",
    "                markersize=4,\n",
    "                alpha=0.7,\n",
    "                label=f\"SGD from {start}\",\n",
    "            )\n",
    "\n",
    "        plt.title(\"Comparison of Batch vs Stochastic Gradient Descent\")\n",
    "        plt.xlabel(\"Parameter Space\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        return plt\n",
    "\n",
    "\n",
    "# Create and show visualization\n",
    "visualizer = OptimizationVisualizer()\n",
    "visualizer.visualize_comparison()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) and Batch Gradient Descent (BGD) behave differently when optimizing a loss function because of how they compute gradients. The key difference lies in the **deterministic** nature of BGD versus the **randomized** nature of SGD, which affects their ability to escape **local extrema** (minima or saddle points). Let's dive deeper into this, covering all aspects:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Key Concepts: Local Extrema and Saddle Points**\n",
    "- **Local Extrema**: A point on the loss surface where the gradient is zero, and the loss is smaller (local minima) or larger (local maxima) compared to its immediate neighbors.\n",
    "- **Saddle Points**: A flat point where the gradient is zero, but the loss curves upward in some directions and downward in others. It's not a true minimum or maximum.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Batch Gradient Descent (BGD)**\n",
    "#### How It Works:\n",
    "- Computes the gradient of the loss function using the **entire dataset**:\n",
    "  \\[\n",
    "  \\nabla L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla L_i(\\theta)\n",
    "  \\]\n",
    "  where \\( N \\) is the total number of data points, \\( \\theta \\) are the model parameters, and \\( L_i(\\theta) \\) is the loss for the \\( i \\)-th data point.\n",
    "- Updates parameters in the direction of the **average gradient**:\n",
    "  \\[\n",
    "  \\theta = \\theta - \\eta \\nabla L(\\theta)\n",
    "  \\]\n",
    "  where \\( \\eta \\) is the learning rate.\n",
    "\n",
    "#### Behavior at Local Extrema:\n",
    "- At a **local minima** or **saddle point**, the average gradient \\( \\nabla L(\\theta) \\) becomes zero (or very close to zero).\n",
    "- Since BGD relies on the average gradient over the dataset, it has no way to escape these points once the gradient is zero.\n",
    "- BGD is **deterministic**, meaning the gradient points exactly in the direction of the steepest descent based on the entire dataset, which makes it unable to \"explore\" other directions.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Stochastic Gradient Descent (SGD)**\n",
    "#### How It Works:\n",
    "- Computes the gradient using a **single randomly selected data point** (or a small subset, in mini-batch SGD):\n",
    "  \\[\n",
    "  \\nabla L(\\theta) = \\nabla L_i(\\theta)\n",
    "  \\]\n",
    "  where \\( i \\) is chosen randomly at each step.\n",
    "- Updates parameters using this noisy gradient:\n",
    "  \\[\n",
    "  \\theta = \\theta - \\eta \\nabla L_i(\\theta)\n",
    "  \\]\n",
    "\n",
    "#### Behavior at Local Extrema:\n",
    "- **Randomness in Gradients**:\n",
    "  - Even at a local minima or saddle point for the full dataset, the gradient computed for individual data points (\\( \\nabla L_i(\\theta) \\)) is not necessarily zero because the loss function for individual data points may still vary.\n",
    "  - This randomness allows SGD to \"jitter\" around and potentially escape local extrema.\n",
    "\n",
    "- **Escape from Saddle Points**:\n",
    "  - Saddle points are often flat in some directions and steep in others. The random gradient from a single data point can nudge the parameters away from the flat region, leading to exploration of new areas on the loss surface.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Comparison: Why SGD Avoids Getting Stuck**\n",
    "\n",
    "| Aspect                 | Batch Gradient Descent (BGD)                | Stochastic Gradient Descent (SGD)           |\n",
    "|------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| **Gradient Calculation** | Uses the full dataset, deterministic.       | Uses a single/random subset, noisy.         |\n",
    "| **Behavior at Local Minima** | Gradient becomes zero; cannot escape.      | Noisy gradients allow movement.             |\n",
    "| **Behavior at Saddle Points** | Gradient is zero; remains stuck.          | Random gradients push parameters away.      |\n",
    "| **Exploration**        | Deterministic; limited exploration.          | Randomness enables exploration.             |\n",
    "| **Efficiency**         | Slower for large datasets.                  | Faster due to smaller gradient computations.|\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Mathematical Intuition**\n",
    "\n",
    "- **BGD at Local Minima**:\n",
    "  At a local minima, the gradient:\n",
    "  \\[\n",
    "  \\nabla L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla L_i(\\theta) = 0\n",
    "  \\]\n",
    "  Since the average gradient is zero, BGD stops updating the parameters.\n",
    "\n",
    "- **SGD at Local Minima**:\n",
    "  For individual data points, the gradient:\n",
    "  \\[\n",
    "  \\nabla L_i(\\theta) \\neq 0\n",
    "  \\]\n",
    "  Even if the average gradient is zero, the gradients of individual data points vary due to randomness, allowing updates to continue.\n",
    "\n",
    "- **Saddle Points**:\n",
    "  - At a saddle point, the loss surface is flat in some directions and steep in others.\n",
    "  - BGD computes the average gradient, which is zero in all directions at the saddle point.\n",
    "  - SGD, however, computes gradients based on individual data points. The random gradients may have a non-zero component in a direction that escapes the saddle point.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Geometric Intuition**\n",
    "- **BGD**:\n",
    "  Imagine a ball rolling down a smooth valley. If the valley flattens (local minima or saddle point), the ball stops moving because there's no slope to guide it.\n",
    "\n",
    "- **SGD**:\n",
    "  The ball is shaken randomly as it rolls. Even if the valley flattens, the shaking causes the ball to explore other areas, potentially finding a steeper slope to descend.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Practical Implications**\n",
    "1. **Global Minima vs Local Minima**:\n",
    "   - SGD's randomness increases the likelihood of escaping local minima, helping the optimizer move toward a global minima.\n",
    "\n",
    "2. **Learning Rate**:\n",
    "   - A larger learning rate in SGD increases the exploration but may overshoot minima. A smaller learning rate improves stability but reduces the ability to escape local extrema.\n",
    "\n",
    "3. **Mini-Batch SGD**:\n",
    "   - Combines the benefits of both BGD and SGD by computing gradients over small batches of data. This reduces noise compared to SGD while retaining some randomness for exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Real-Life Example**\n",
    "- **BGD**: \n",
    "  Imagine following a map to the shortest route (deterministic). If a road is blocked (local minima), you stop and can’t proceed.\n",
    "\n",
    "- **SGD**: \n",
    "  Imagine taking a random walk through the city. If you encounter a blocked road, you randomly choose another path, eventually finding an alternate route to your destination.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Key Takeaways**\n",
    "- **BGD** computes gradients over the entire dataset, making it deterministic and prone to getting stuck at local extrema or saddle points.\n",
    "- **SGD** uses noisy gradients from individual data points, introducing randomness that allows it to escape local minima and saddle points.\n",
    "- This behavior makes SGD more robust for non-convex optimization problems, such as training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
